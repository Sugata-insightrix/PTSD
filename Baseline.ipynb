{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "1023c4a7-a378-4ac2-a88e-fecc9a59d765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current directory: D:\\Work\\PTSD\\Linear\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "path = os.getcwd()\n",
    "print(\"Current directory:\", path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d11789-0791-4756-b0f1-6142b2653f27",
   "metadata": {},
   "source": [
    "# DML:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "e6c90a93-4b16-4ed0-b2e7-985448b2fdb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Changed to: D:\\Work\\PTSD\\DML\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")  # optional\n",
    "\n",
    "# Option 1 (recommended)\n",
    "new_path = r\"D:\\Work\\PTSD\\DML\"\n",
    "\n",
    "os.chdir(new_path)  # Change working directory\n",
    "print(\"Changed to:\", os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "2b2ef094-c824-4279-b898-719c211cc4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(r\"data_baseline.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "7383c16b-bf42-43cd-9e4b-3db434a0a41b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of CAT_ columns: 23\n",
      "Number of SUBCAT_ columns: 42\n",
      "Number of SubSubCat_ columns: 243\n",
      "✅ Exported to 'category_column_sums.xlsx'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 1: Identify columns by prefix\n",
    "cat_cols = [col for col in df.columns if col.startswith('CAT_')]\n",
    "subcat_cols = [col for col in df.columns if col.startswith('SUBCAT_')]\n",
    "subsubcat_cols = [col for col in df.columns if col.startswith('SubSubCat_')]\n",
    "\n",
    "# Step 2: Print counts\n",
    "print(f\"Number of CAT_ columns: {len(cat_cols)}\")\n",
    "print(f\"Number of SUBCAT_ columns: {len(subcat_cols)}\")\n",
    "print(f\"Number of SubSubCat_ columns: {len(subsubcat_cols)}\")\n",
    "\n",
    "# Step 3: Column-wise sums\n",
    "cat_sums = df[cat_cols].sum().sort_values(ascending=False)\n",
    "subcat_sums = df[subcat_cols].sum().sort_values(ascending=False)\n",
    "subsubcat_sums = df[subsubcat_cols].sum().sort_values(ascending=False)\n",
    "\n",
    "# Step 4: Convert to DataFrames\n",
    "cat_df = cat_sums.reset_index()\n",
    "cat_df.columns = ['Column', 'Sum']\n",
    "\n",
    "subcat_df = subcat_sums.reset_index()\n",
    "subcat_df.columns = ['Column', 'Sum']\n",
    "\n",
    "subsubcat_df = subsubcat_sums.reset_index()\n",
    "subsubcat_df.columns = ['Column', 'Sum']\n",
    "\n",
    "# Step 5: Write to Excel\n",
    "with pd.ExcelWriter(\"category_column_sums.xlsx\") as writer:\n",
    "    cat_df.to_excel(writer, sheet_name=\"CAT_Sums\", index=False)\n",
    "    subcat_df.to_excel(writer, sheet_name=\"SUBCAT_Sums\", index=False)\n",
    "    subsubcat_df.to_excel(writer, sheet_name=\"SubSubCat_Sums\", index=False)\n",
    "\n",
    "print(\"✅ Exported to 'category_column_sums.xlsx'\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "860bb20c-6e42-4336-b4d7-cfbd5e31cd56",
   "metadata": {},
   "source": [
    "These below CAT, SUBCAT, and SubSubCat are do not have more than 30 treatment groups.\n",
    "Hence, we will not consider these onces.\n",
    "\n",
    "CAT_Anticonceptiva\n",
    "CAT_Immunomodulerende_middelen\n",
    "CAT_Migrainemiddelen\n",
    "CAT_Stemmingsstabilisatoren\n",
    "CAT_Salicylaat\n",
    "CAT_Alcoholverslaving\n",
    "CAT_Spierrelaxantia\n",
    "CAT_Parkinson\n",
    "\n",
    "\n",
    "SUBCAT_Anticonceptiva_klassiek\n",
    "SUBCAT_Anti_epileptica_GABA_analogon\n",
    "SUBCAT_Antipsychotica_Klassiek\n",
    "SUBCAT_calciumantagonisten\n",
    "SUBCAT_Paracetamol_overig_combinatie\n",
    "SUBCAT_stemm_Lithiumzouten\n",
    "SUBCAT_ACE_remmer\n",
    "SUBCAT_Paracetamol_salycilaat_combinatiepreparaat\n",
    "SUBCAT_Corticosteroiden\n",
    "SUBCAT_Anti_epileptica_Benzodiazepine\n",
    "SUBCAT_psychostimulans_overige\n",
    "SUBCAT_Anti_epileptica_overig\n",
    "SUBCAT_Immunomodulerend_Coxibs\n",
    "SUBCAT_Interleukine_remmers\n",
    "SUBCAT_Clonidine\n",
    "SUBCAT_TNF_alpha_blockers\n",
    "SUBCAT_Antihypertensiva_ARBs\n",
    "SUBCAT_Aminosalicylaten\n",
    "SUBCAT_Selectieve_immunosuppresiva\n",
    "SUBCAT_MAO_remmers\n",
    "SUBCAT_stemm_benzamide\n",
    "SUBCAT_Antihypertensiva_centraal_aangrijpend\n",
    "SUBCAT_DMARDs\n",
    "SUBCAT_calcineurineremmers\n",
    "SUBCAT_ADHD_klassiek\n",
    "SUBCAT_Combinatiepreparaten\n",
    "\n",
    "\n",
    "\n",
    "SubSubCat_Nortriptyline\n",
    "SubSubCat_pil\n",
    "SubSubCat_Aripiprazol\n",
    "SubSubCat_Dexamfetamine\n",
    "SubSubCat_Pregabaline\n",
    "SubSubCat_Trazodon\n",
    "SubSubCat_Oxycodon\n",
    "SubSubCat_Diclofenac\n",
    "SubSubCat_Risperidon\n",
    "SubSubCat_Metoprolol\n",
    "SubSubCat_Naproxen\n",
    "SubSubCat_Antidepressiva_overige\n",
    "SubSubCat_Ibuprofen\n",
    "SubSubCat_Morfine\n",
    "SubSubCat_Desloratadine\n",
    "SubSubCat_Lithium\n",
    "SubSubCat_Paracetamol_ascorbinezuur\n",
    "SubSubCat_Duloxetine\n",
    "SubSubCat_Amlodipine\n",
    "SubSubCat_Fluvoxamine\n",
    "SubSubCat_Acrivastine\n",
    "SubSubCat_Propranolol\n",
    "SubSubCat_Haloperidol\n",
    "SubSubCat_Valproïnezuur\n",
    "SubSubCat_Lamotrigine\n",
    "SubSubCat_Pipamperon\n",
    "SubSubCat_Clomipramine\n",
    "SubSubCat_Lormetazepam\n",
    "SubSubCat_Midazolam\n",
    "SubSubCat_Clonazepam\n",
    "SubSubCat_Fexofenadine\n",
    "SubSubCat_Levocetirizine\n",
    "SubSubCat_Nicardipine\n",
    "SubSubCat_Acetylsalicylzuur_paracetamol_coffeïne\n",
    "SubSubCat_Pethidine\n",
    "SubSubCat_Fentanyl\n",
    "SubSubCat_Atomoxetine\n",
    "SubSubCat_Prazepam\n",
    "SubSubCat_Doxepine\n",
    "SubSubCat_Enalapril\n",
    "SubSubCat_Gabapentine\n",
    "SubSubCat_Loratadine\n",
    "SubSubCat_Bisoprolol\n",
    "SubSubCat_Nitrazepam\n",
    "SubSubCat_Cetirizine\n",
    "SubSubCat_Levetiracetam\n",
    "SubSubCat_clonidine\n",
    "SubSubCat_celecoxib\n",
    "SubSubCat_Etoricoxib\n",
    "SubSubCat_Buspiron\n",
    "SubSubCat_Lisinopril\n",
    "SubSubCat_Prednison\n",
    "SubSubCat_Ustekinumab\n",
    "SubSubCat_Meclozine\n",
    "SubSubCat_Perindopril\n",
    "SubSubCat_Maprotiline\n",
    "SubSubCat_Flurazepam\n",
    "SubSubCat_chloorprotixeen\n",
    "SubSubCat_Imipramine\n",
    "SubSubCat_Vedolizumab\n",
    "SubSubCat_Ebastine\n",
    "SubSubCat_Clorazepinezuur\n",
    "SubSubCat_Clozapine\n",
    "SubSubCat_Sertindol\n",
    "SubSubCat_Carbamazepine\n",
    "SubSubCat_Prednisolon\n",
    "SubSubCat_Antihypertensiva_ARBs\n",
    "SubSubCat_Rupatadine\n",
    "SubSubCat_Mesalazine\n",
    "SubSubCat_Nabumeton\n",
    "SubSubCat_Nebivolol\n",
    "SubSubCat_Vigabatrine\n",
    "SubSubCat_Eculizumab\n",
    "SubSubCat_Certolizumab_pegol\n",
    "SubSubCat_Adalimumab\n",
    "SubSubCat_Tocilizumab\n",
    "SubSubCat_safinamide\n",
    "SubSubCat_Hydrocortison\n",
    "SubSubCat_Ramipril\n",
    "SubSubCat_Methylprednisolon\n",
    "SubSubCat_Satralizumab\n",
    "SubSubCat_Sulpiride\n",
    "SubSubCat_Moclobemide\n",
    "SubSubCat_Clevidipine\n",
    "SubSubCat_Atenolol\n",
    "SubSubCat_Dupilumab\n",
    "SubSubCat_Clemastine\n",
    "SubSubCat_Brivaracetam\n",
    "SubSubCat_Lercanidipine\n",
    "SubSubCat_Etanercept\n",
    "SubSubCat_Meloxicam\n",
    "SubSubCat_Nifedipine\n",
    "SubSubCat_Fosinopril\n",
    "SubSubCat_Infliximab\n",
    "SubSubCat_Barnidipine\n",
    "SubSubCat_Tofacitinib\n",
    "SubSubCat_Mycofenolaatmofetil\n",
    "SubSubCat_Mycofenolzuur\n",
    "SubSubCat_Ravulizumab\n",
    "SubSubCat_Sirolimus\n",
    "SubSubCat_Thymocytenglobuline\n",
    "SubSubCat_Anakinra\n",
    "SubSubCat_Upadacitinib\n",
    "SubSubCat_Basiliximab\n",
    "SubSubCat_Bimekizumab\n",
    "SubSubCat_Leflunomide\n",
    "SubSubCat_Canakinumab\n",
    "SubSubCat_Guselkumab\n",
    "SubSubCat_Brodalumab\n",
    "SubSubCat_Apremilast\n",
    "SubSubCat_Filgotinib\n",
    "SubSubCat_Everolimus\n",
    "SubSubCat_Belimumab\n",
    "SubSubCat_Belatacept\n",
    "SubSubCat_Baricitinib\n",
    "SubSubCat_Abatacept\n",
    "SubSubCat_Golimumab\n",
    "SubSubCat_Olsalazine\n",
    "SubSubCat_Sulfasalazine\n",
    "SubSubCat_Thalidomide\n",
    "SubSubCat_Pomalidomide\n",
    "SubSubCat_Pirfenidon\n",
    "SubSubCat_Risankizumab\n",
    "SubSubCat_Lenalidomide\n",
    "SubSubCat_Ixekizumab\n",
    "SubSubCat_Aceclofenac\n",
    "SubSubCat_Sarilumab\n",
    "SubSubCat_Secukinumab\n",
    "SubSubCat_Methotrexaat\n",
    "SubSubCat_Corticosteroiden\n",
    "SubSubCat_hypnotica_Benzodiazepine\n",
    "SubSubCat_Paracetamol_propyfenazon_coffeïne\n",
    "SubSubCat_Paracetamol_coffeïne_ascorbinezuur\n",
    "SubSubCat_Paracetamol_coffeïne\n",
    "SubSubCat_Tiaprofeenzuur\n",
    "SubSubCat_Piroxicam\n",
    "SubSubCat_Metamizol\n",
    "SubSubCat_Indometacine\n",
    "SubSubCat_Flurbiprofen\n",
    "SubSubCat_Fenylbutazon\n",
    "SubSubCat_Dexketoprofen\n",
    "SubSubCat_Tapentadol\n",
    "SubSubCat_Sufentanil\n",
    "SubSubCat_Remifentanil\n",
    "SubSubCat_Piritramide\n",
    "SubSubCat_Nalbufine\n",
    "SubSubCat_Hydromorfon\n",
    "SubSubCat_Buprenorfine\n",
    "SubSubCat_Alfentanil\n",
    "SubSubCat_Tacrolimus\n",
    "SubSubCat_Pimecrolimus\n",
    "SubSubCat_Ciclosporine\n",
    "SubSubCat_Tralokinumab\n",
    "SubSubCat_Tildrakizumab\n",
    "SubSubCat_Siltuximab\n",
    "SubSubCat_Azathioprine\n",
    "SubSubCat_fluspirileen\n",
    "SubSubCat_Parecoxib\n",
    "SubSubCat_Bromazepam\n",
    "SubSubCat_Primidon\n",
    "SubSubCat_Perampanel\n",
    "SubSubCat_Oxcarbazepine\n",
    "SubSubCat_Lacosamide\n",
    "SubSubCat_Fenobarbital\n",
    "SubSubCat_Felbamaat\n",
    "SubSubCat_Ethosuximide\n",
    "SubSubCat_Chloralhydraat\n",
    "SubSubCat_Fenytoïne\n",
    "SubSubCat_Remimazolam\n",
    "SubSubCat_Flunitrazepam\n",
    "SubSubCat_Clobazam\n",
    "SubSubCat_Brotizolam\n",
    "SubSubCat_Hydroxyzine\n",
    "SubSubCat_Triamcinolonhexacetonide\n",
    "SubSubCat_Selegiline\n",
    "SubSubCat_Rasagiline\n",
    "SubSubCat_Dosulepine\n",
    "SubSubCat_Tiapride\n",
    "SubSubCat_Paliperidon\n",
    "SubSubCat_Lurasidon\n",
    "SubSubCat_Cariprazine\n",
    "SubSubCat_Brexpiprazol\n",
    "SubSubCat_Amisulpride\n",
    "SubSubCat_periciazine\n",
    "SubSubCat_pimozide\n",
    "SubSubCat_Broomperidol\n",
    "SubSubCat_Zuclopentixol\n",
    "SubSubCat_Rufinamide\n",
    "SubSubCat_Stiripentol\n",
    "SubSubCat_Zonisamide\n",
    "SubSubCat_Chloorcyclizine\n",
    "SubSubCat_Triamcinolonacetonide\n",
    "SubSubCat_Fludrocortison\n",
    "SubSubCat_Dexamethason\n",
    "SubSubCat_Cortison\n",
    "SubSubCat_Moxonidine\n",
    "SubSubCat_Methyldopa\n",
    "SubSubCat_Guanfacine\n",
    "SubSubCat_Clonidine\n",
    "SubSubCat_Nimodipine\n",
    "SubSubCat_Lacidipine\n",
    "SubSubCat_Felodipine\n",
    "SubSubCat_Zofenopril\n",
    "SubSubCat_Captopril\n",
    "SubSubCat_Benazepril\n",
    "SubSubCat_Sotalol\n",
    "SubSubCat_Landiolol\n",
    "SubSubCat_Labetalol\n",
    "SubSubCat_Esmolol\n",
    "SubSubCat_Celiprolol\n",
    "SubSubCat_Carvedilol\n",
    "SubSubCat_Acebutolol\n",
    "SubSubCat_Oxomemazine\n",
    "SubSubCat_Mizolastine\n",
    "SubSubCat_Ketotifen\n",
    "SubSubCat_Doxylamine\n",
    "SubSubCat_Dimetindeen\n",
    "SubSubCat_Cyclizine\n",
    "SubSubCat_ADHD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71cfb8f5-7ba0-4bde-96b6-42e631f07681",
   "metadata": {},
   "source": [
    "## CAT analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "5b9a3959-6c9a-436a-8be2-dac4f16454f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all necessary packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "# For visualization and future steps\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.experimental import enable_iterative_imputer  # Needed to enable the experimental feature\n",
    "from sklearn.impute import IterativeImputer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "7c4a4a00-1c01-4e2b-a5e6-6c123e0b6bc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of dataset: (6125, 465)\n",
      "\n",
      "Sample columns: ['CIN5', 'StartDatum', 'BEH_MOD', 'BEHDAGEN_GEPLAND', 'AANTAL_PCL', 'TOESTWO', 'BEH_AFG', 'TK', 'MM_CAPS_IN', 'MM_CAPS_TK']\n",
      "\n",
      "Missing values:\n",
      " instrument_SDV_IN    6125\n",
      "Eaantal_TK           6125\n",
      "Dcriterium_FU        6125\n",
      "Cernst_FU            6125\n",
      "Caantal_FU           6125\n",
      "Ccriterium_FU        6125\n",
      "Bernst_FU            6125\n",
      "Baantal_FU           6125\n",
      "Bcriterium_FU        6125\n",
      "Eernst_TK            6125\n",
      "dtype: int64\n",
      "Shape after removing duplicates: (6125, 465)\n"
     ]
    }
   ],
   "source": [
    "# Check basic structure\n",
    "print(\"Shape of dataset:\", df.shape)\n",
    "print(\"\\nSample columns:\", df.columns.tolist()[:10])\n",
    "print(\"\\nMissing values:\\n\", df.isnull().sum().sort_values(ascending=False).head(10))\n",
    "\n",
    "# Remove duplicate rows\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "# Confirm shape after removing duplicates\n",
    "print(\"Shape after removing duplicates:\", df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "dd7bb726-0877-4969-81b3-998a6663feba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SDV_SEXE  gender_label\n",
      "2.0       Female          4602\n",
      "1.0       Male            1475\n",
      "3.0       Other             48\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pyreadstat\n",
    "\n",
    "# Load gender info\n",
    "gender_df, meta = pyreadstat.read_sav(\"SDV_IN_Gender_2019_2024.sav\")\n",
    "\n",
    "# Just extract SDV_SEXE column and append to df\n",
    "df[\"SDV_SEXE\"] = gender_df[\"SDV_SEXE\"].reset_index(drop=True)\n",
    "\n",
    "# Optional: map to labels\n",
    "gender_map = {1.0: \"Male\", 2.0: \"Female\", 3.0: \"Other\"}\n",
    "df[\"gender_label\"] = df[\"SDV_SEXE\"].map(gender_map)\n",
    "\n",
    "# Done! Check a sample\n",
    "print(df[[\"SDV_SEXE\", \"gender_label\"]].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "e8e32268-4eb2-4ab8-9323-586bd306dd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'gender' and 'SDV_SEXE' columns are in df\n",
    "\n",
    "# Gender dummy variables\n",
    "df['gender_1'] = (df['gender'] == 1).astype(int)\n",
    "df['gender_2'] = (df['gender'] == 2).astype(int)\n",
    "\n",
    "# SDV_SEXE dummy variables\n",
    "df['SDV_SEXE_1'] = (df['SDV_SEXE'] == 1).astype(int)\n",
    "df['SDV_SEXE_2'] = (df['SDV_SEXE'] == 2).astype(int)\n",
    "df['SDV_SEXE_3'] = (df['SDV_SEXE'] == 3).astype(int)\n",
    "\n",
    "# Create binary columns\n",
    "df['ethnicity_Dutch'] = np.where(df['ethnicity'] == 1, 1, 0)\n",
    "df['ethnicity_other'] = np.where(df['ethnicity'] != 1, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "2a30c922-0ef4-4623-9a37-b49ce14d1995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining columns: 458\n"
     ]
    }
   ],
   "source": [
    "# Columns manually identified for removal (example set from the R script)\n",
    "cols_to_drop = [\n",
    "    'gender', 'ethnicity', 'CIN5', 'SDV_SEXE', 'StartDatum', 'STARTDATUM', 'DROPOUT_EARLYCOMPLETER', 'TOEST_WO',\n",
    "    'depressie_IN', 'TERUGKOMER', 'VROEGK_ST', 'gender_label',\n",
    "    'depr_m_psychose_huid', 'depr_z_psychose_huid', 'depr_z_psychose_verl',\n",
    "    'depr_m_psychose_verl', 'CAPS5score_followup', 'CAPS5_DAT_IN'\n",
    "]\n",
    "\n",
    "df = df.drop(columns=[col for col in cols_to_drop if col in df.columns])\n",
    "print(\"Remaining columns:\", df.shape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "2928476d-1477-4f94-aef1-2ba57cc90110",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if 'BEH_DAGEN' in df.columns:\n",
    "    df.rename(columns={'BEH_DAGEN': 'treatmentdurationdays'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "44379251-fbb1-422d-af00-1695e5bf3d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean and standardize column names\n",
    "df.columns = (\n",
    "    df.columns\n",
    "    .str.replace(r\"\\.+\", \"_\", regex=True)\n",
    "    .str.replace(r\"[^a-zA-Z0-9_]\", \"\", regex=True)\n",
    "    .str.replace(\" \", \"_\")\n",
    "    .str.strip()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "dc51f23a-e5ac-4736-a4e1-815cffd05783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CAPS5score_baseline: 0 missing\n",
      "CAPS5Score_TK: 0 missing\n"
     ]
    }
   ],
   "source": [
    "# Preview key outcome variables\n",
    "outcome_vars = ['CAPS5score_baseline', 'CAPS5Score_TK']\n",
    "for col in outcome_vars:\n",
    "    if col in df.columns:\n",
    "        print(f\"{col}: {df[col].isnull().sum()} missing\")\n",
    "\n",
    "# Calculate change score\n",
    "if 'CAPS5score_baseline' in df.columns and 'CAPS5Score_TK' in df.columns:\n",
    "    df['caps5_change_baseline'] = df['CAPS5Score_TK'] - df['CAPS5score_baseline'] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "16955012-06c6-42d3-b91d-e103338584f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define exceptions to keep\n",
    "protected_cols = [\n",
    "    \"DIAGNOSIS_ANXIETY_OCD\",\n",
    "    \"DIAGNOSIS_PSYCHOTIC\",\n",
    "    \"DIAGNOSIS_EATING_DISORDER\",\n",
    "    \"DIAGNOSIS_SUBSTANCE_DISORDER\", \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"EB_NON_TF_THERAPY\", \"OTHER_TREATM_APPROACH\", \"Bipolar_and_Mood_disorder\", 'SUBCAT_Selectieve_immunosuppresiva', 'treatmentdurationdays',\n",
    "'SUBCAT_Corticosteroiden',\n",
    "'SUBCAT_Immunomodulerend_Coxibs',\n",
    "'SUBCAT_Aminosalicylaten',\n",
    "'SUBCAT_calcineurineremmers',\n",
    "'SUBCAT_Anti_epileptica_Benzodiazepine',\n",
    "'SUBCAT_Paracetamol_overig_combinatie', 'SUBCAT_MAO_remmers', 'SUBCAT_psychostimulans_overige', 'SUBCAT_Interleukine_remmers'\n",
    "\n",
    "]\n",
    "\n",
    "\n",
    "# ----------------------------------------\n",
    "# 1. Drop columns with >95% missing values (except protected)\n",
    "thresh_missing = int(0.95 * len(df))\n",
    "missing_cols = [col for col in df.columns if df[col].isnull().sum() > (len(df) - thresh_missing)]\n",
    "missing_cols_to_drop = [col for col in missing_cols if col not in protected_cols]\n",
    "df = df.drop(columns=missing_cols_to_drop)\n",
    "\n",
    "# ----------------------------------------\n",
    "# 2. Drop near-zero variance columns (except protected)\n",
    "low_variance_cols = [col for col in df.columns if df[col].nunique(dropna=True) <= 1 and col not in protected_cols]\n",
    "df = df.drop(columns=low_variance_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "fab9270e-9e84-4999-9cf9-6d3d6781c224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Step 1 Complete: Cleaned dataset saved.\n"
     ]
    }
   ],
   "source": [
    "df.to_csv(\"cleaned_data_baseline.csv\", index=False)\n",
    "print(\" Step 1 Complete: Cleaned dataset saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "e3195492-0a77-4727-9370-4b8616218575",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target variables:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2af56f32-3a6c-48c6-90bc-baf17a47caee",
   "metadata": {},
   "source": [
    "CAPS5score_baseline:    PTSD severity before treatment\n",
    "CAPS5Score_TK:      \tPTSD severity after treatment (post test)\n",
    "Derived:                caps5_change_baseline\t(pre - post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "4f3559c4-6bf8-402a-8739-050a016c5cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install scikit-learn\n",
    "# !pip install fancyimpute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "0cf58d99-d747-4ac5-b4b1-c481572459c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from fancyimpute import IterativeImputer\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "291e0886-8be9-4f10-b65a-8c8d8771cf9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6125, 204)\n",
      "DIAGNOSIS_ANXIETY_OCD           float64\n",
      "DIAGNOSIS_SMOKING               float64\n",
      "DIAGNOSIS_EATING_DISORDER       float64\n",
      "DIAGNOSIS_SUBSTANCE_DISORDER    float64\n",
      "DIAGNOSIS_PSYCHOTIC             float64\n",
      "DIAGNOSIS_SUICIDALITY           float64\n",
      "DIAGNOSIS_SEXUAL_TRAUMA         float64\n",
      "DIAGNOSIS_CHILDHOOD_TRAUMA        int64\n",
      "DIAGNOSIS_CPTSD                 float64\n",
      "treatmentdurationdays           float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Load the cleaned dataset from Step 1\n",
    "df = pd.read_csv(\"cleaned_data_baseline.csv\")\n",
    "\n",
    "# Quick check\n",
    "print(df.shape)\n",
    "print(df.dtypes.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "a7040b25-e2fc-4cff-a371-36f3cee7839b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate Numerical and Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "5dd82f05-926b-4e42-a049-7d46e4e47dbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numerical Columns: 204\n",
      "Categorical Columns: 0\n"
     ]
    }
   ],
   "source": [
    "# Identify numerical and categorical columns\n",
    "numerical_cols = df.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "print(f\"Numerical Columns: {len(numerical_cols)}\")\n",
    "print(f\"Categorical Columns: {len(categorical_cols)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "77aec19c-15a1-4dd9-902f-f7ee0de6f6a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6125 entries, 0 to 6124\n",
      "Columns: 204 entries, DIAGNOSIS_ANXIETY_OCD to ethnicity_other\n",
      "dtypes: float64(10), int64(194)\n",
      "memory usage: 9.5 MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "e23db7c4-d773-4df1-9681-382a86a4d940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Step 2 Complete: Final prepared dataset saved as 'final_prepared_data.csv'.\n"
     ]
    }
   ],
   "source": [
    "# Save the fully prepared data\n",
    "df.to_csv(\"final_prepared_data.csv\", index=False)\n",
    "print(\" Step 2 Complete: Final prepared dataset saved as 'final_prepared_data.csv'.\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "31c98559-4385-4a03-9883-006b35a54d72",
   "metadata": {},
   "source": [
    "Define Features, Treatment, and Outcome\n",
    "\n",
    "\n",
    " X = Covariates\n",
    "\n",
    " T = Treatment Variable (e.g., use of antidepressants CAT_Antidepressiva)\n",
    "\n",
    " Y = Target Outcome (caps5_change_baseline)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0db325e1-30db-48ea-bd24-177192b3429d",
   "metadata": {},
   "source": [
    "MICE IMputation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "0cc519e7-8480-41c8-9a15-956044ac1b04",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "STEP 1: MICE IMPUTATION\n",
      "==================================================\n",
      "\n",
      "=== Running MICE Imputation: Dataset 1 ===\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[233], line 36\u001b[0m\n\u001b[0;32m     28\u001b[0m mice_imputer \u001b[38;5;241m=\u001b[39m IterativeImputer(\n\u001b[0;32m     29\u001b[0m     max_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, \n\u001b[0;32m     30\u001b[0m     random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m\u001b[38;5;241m+\u001b[39mi,  \u001b[38;5;66;03m# Different base to avoid low numbers\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     33\u001b[0m     initial_strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     34\u001b[0m )\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# Fit-transform on numeric columns\u001b[39;00m\n\u001b[1;32m---> 36\u001b[0m imputed_array \u001b[38;5;241m=\u001b[39m mice_imputer\u001b[38;5;241m.\u001b[39mfit_transform(df[numeric_cols])\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# Replace numeric columns in a copy of the original df\u001b[39;00m\n\u001b[0;32m     38\u001b[0m df_imputed \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mcopy()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\utils\\_set_output.py:316\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    314\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    315\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 316\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    317\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    318\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    319\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    320\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    321\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    322\u001b[0m         )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\impute\\_iterative.py:789\u001b[0m, in \u001b[0;36mIterativeImputer.fit_transform\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m    785\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m feat_idx \u001b[38;5;129;01min\u001b[39;00m ordered_idx:\n\u001b[0;32m    786\u001b[0m     neighbor_feat_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_neighbor_feat_idx(\n\u001b[0;32m    787\u001b[0m         n_features, feat_idx, abs_corr_mat\n\u001b[0;32m    788\u001b[0m     )\n\u001b[1;32m--> 789\u001b[0m     Xt, estimator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_impute_one_feature(\n\u001b[0;32m    790\u001b[0m         Xt,\n\u001b[0;32m    791\u001b[0m         mask_missing_values,\n\u001b[0;32m    792\u001b[0m         feat_idx,\n\u001b[0;32m    793\u001b[0m         neighbor_feat_idx,\n\u001b[0;32m    794\u001b[0m         estimator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    795\u001b[0m         fit_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    796\u001b[0m         params\u001b[38;5;241m=\u001b[39mrouted_params\u001b[38;5;241m.\u001b[39mestimator\u001b[38;5;241m.\u001b[39mfit,\n\u001b[0;32m    797\u001b[0m     )\n\u001b[0;32m    798\u001b[0m     estimator_triplet \u001b[38;5;241m=\u001b[39m _ImputerTriplet(\n\u001b[0;32m    799\u001b[0m         feat_idx, neighbor_feat_idx, estimator\n\u001b[0;32m    800\u001b[0m     )\n\u001b[0;32m    801\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimputation_sequence_\u001b[38;5;241m.\u001b[39mappend(estimator_triplet)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\impute\\_iterative.py:418\u001b[0m, in \u001b[0;36mIterativeImputer._impute_one_feature\u001b[1;34m(self, X_filled, mask_missing_values, feat_idx, neighbor_feat_idx, estimator, fit_mode, params)\u001b[0m\n\u001b[0;32m    408\u001b[0m     X_train \u001b[38;5;241m=\u001b[39m _safe_indexing(\n\u001b[0;32m    409\u001b[0m         _safe_indexing(X_filled, neighbor_feat_idx, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m),\n\u001b[0;32m    410\u001b[0m         \u001b[38;5;241m~\u001b[39mmissing_row_mask,\n\u001b[0;32m    411\u001b[0m         axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m    412\u001b[0m     )\n\u001b[0;32m    413\u001b[0m     y_train \u001b[38;5;241m=\u001b[39m _safe_indexing(\n\u001b[0;32m    414\u001b[0m         _safe_indexing(X_filled, feat_idx, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m),\n\u001b[0;32m    415\u001b[0m         \u001b[38;5;241m~\u001b[39mmissing_row_mask,\n\u001b[0;32m    416\u001b[0m         axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m    417\u001b[0m     )\n\u001b[1;32m--> 418\u001b[0m     estimator\u001b[38;5;241m.\u001b[39mfit(X_train, y_train, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m    420\u001b[0m \u001b[38;5;66;03m# if no missing values, don't predict\u001b[39;00m\n\u001b[0;32m    421\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39msum(missing_row_mask) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\linear_model\\_bayes.py:291\u001b[0m, in \u001b[0;36mBayesianRidge.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    288\u001b[0m eigen_vals_ \u001b[38;5;241m=\u001b[39m S\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[0;32m    290\u001b[0m \u001b[38;5;66;03m# Convergence loop of the bayesian ridge regression\u001b[39;00m\n\u001b[1;32m--> 291\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m iter_ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_iter):\n\u001b[0;32m    292\u001b[0m     \u001b[38;5;66;03m# update posterior mean coef_ based on alpha_ and lambda_ and\u001b[39;00m\n\u001b[0;32m    293\u001b[0m     \u001b[38;5;66;03m# compute corresponding rmse\u001b[39;00m\n\u001b[0;32m    294\u001b[0m     coef_, rmse_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_coef_(\n\u001b[0;32m    295\u001b[0m         X, y, n_samples, n_features, XT_y, U, Vh, eigen_vals_, alpha_, lambda_\n\u001b[0;32m    296\u001b[0m     )\n\u001b[0;32m    297\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_score:\n\u001b[0;32m    298\u001b[0m         \u001b[38;5;66;03m# compute the log marginal likelihood\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.experimental import enable_iterative_imputer  # noqa\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "# ========== CONFIG ==========\n",
    "save_folder = \"imputed_data\"\n",
    "os.makedirs(save_folder, exist_ok=True)\n",
    "n_imputations = 5\n",
    "\n",
    "# ========== LOAD ==========\n",
    "# Ensure df is already defined\n",
    "assert 'df' in globals(), \"Please load the original DataFrame as `df` before running this script.\"\n",
    "\n",
    "# ========== IDENTIFY NUMERIC COLUMNS ==========\n",
    "numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "\n",
    "# ========== STEP 1: MICE IMPUTATION ==========\n",
    "print(\"=\" * 50)\n",
    "print(\"STEP 1: MICE IMPUTATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "imputed_dfs = []\n",
    "for i in range(1, n_imputations + 1):\n",
    "    print(f\"\\n=== Running MICE Imputation: Dataset {i} ===\")\n",
    "    #  NEW instance with different seed AND sample_posterior=True for randomness\n",
    "    mice_imputer = IterativeImputer(\n",
    "        max_iter=10, \n",
    "        random_state=42+i,  # Different base to avoid low numbers\n",
    "        sample_posterior=True,  #  KEY: This adds randomness!\n",
    "        n_nearest_features=None,\n",
    "        initial_strategy='mean'\n",
    "    )\n",
    "    # Fit-transform on numeric columns\n",
    "    imputed_array = mice_imputer.fit_transform(df[numeric_cols])\n",
    "    # Replace numeric columns in a copy of the original df\n",
    "    df_imputed = df.copy()\n",
    "    df_imputed[numeric_cols] = pd.DataFrame(imputed_array, columns=numeric_cols, index=df.index)\n",
    "    # Append to list\n",
    "    imputed_dfs.append(df_imputed)\n",
    "    print(f\" Completed imputation {i}\")\n",
    "\n",
    "# ========== STEP 2: ROUNDING ==========\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"STEP 2: ROUNDING NUMERIC COLUMNS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "def round_all_numeric_columns_all_imputations(imputed_dfs, decimals=0, verbose=True):\n",
    "    rounded_dfs = []\n",
    "    for i, df in enumerate(imputed_dfs):\n",
    "        df_copy = df.copy()\n",
    "        numeric_cols = df_copy.select_dtypes(include=[np.number]).columns\n",
    "        df_copy[numeric_cols] = df_copy[numeric_cols].round(decimals)\n",
    "        rounded_dfs.append(df_copy)\n",
    "        if verbose:\n",
    "            print(f\" Imputation {i+1}: Rounded {len(numeric_cols)} numeric columns to {decimals} decimal place(s).\")\n",
    "    return rounded_dfs\n",
    "\n",
    "# Apply rounding to all imputed datasets\n",
    "imputed_dfs = round_all_numeric_columns_all_imputations(imputed_dfs)\n",
    "\n",
    "# ========== STEP 3: SAVE FINAL DATASETS ==========\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"STEP 3: SAVING FINAL DATASETS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for i, df_imputed in enumerate(imputed_dfs, 1):\n",
    "    # Save outputs\n",
    "    pkl_path = f\"{save_folder}/df_imputed_final_imp{i}.pkl\"\n",
    "    csv_path = f\"{save_folder}/df_imputed_final_imp{i}.csv\"\n",
    "    excel_path = f\"{save_folder}/df_imputed_final_imp{i}.xlsx\"\n",
    "    \n",
    "    df_imputed.to_pickle(pkl_path)\n",
    "    df_imputed.to_csv(csv_path, index=False)\n",
    "    df_imputed.to_excel(excel_path, index=False)\n",
    "    \n",
    "    print(f\" Saved files for imputation {i}:\")\n",
    "    print(f\"   → {pkl_path}\")\n",
    "    print(f\"   → {csv_path}\")\n",
    "    print(f\"   → {excel_path}\")\n",
    "\n",
    "# ========== STEP 4: VERIFY DATASETS ARE DIFFERENT ==========\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"STEP 4: VERIFYING DATASET DIFFERENCES\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "def check_imputation_differences(imputed_dfs, verbose=True):\n",
    "    \"\"\"Check if imputed datasets are actually different from each other\"\"\"\n",
    "    if len(imputed_dfs) < 2:\n",
    "        print(\"  Only one dataset - cannot check differences\")\n",
    "        return\n",
    "    \n",
    "    # Get numeric columns that had missing values originally\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    missing_cols = [col for col in numeric_cols if df[col].isnull().any()]\n",
    "    \n",
    "    if not missing_cols:\n",
    "        print(\"  No missing values found in original data\")\n",
    "        return\n",
    "    \n",
    "    print(f\" Checking differences in {len(missing_cols)} columns that had missing values...\")\n",
    "    \n",
    "    differences_found = False\n",
    "    \n",
    "    for col in missing_cols[:3]:  # Check first 3 columns with missing values\n",
    "        # Compare first two datasets for this column\n",
    "        values_1 = imputed_dfs[0][col].values\n",
    "        values_2 = imputed_dfs[1][col].values\n",
    "        \n",
    "        if not np.array_equal(values_1, values_2):\n",
    "            differences_found = True\n",
    "            # Count how many values are different\n",
    "            diff_count = np.sum(values_1 != values_2)\n",
    "            print(f\" Column '{col}': {diff_count} different values between datasets 1 & 2\")\n",
    "        else:\n",
    "            print(f\" Column '{col}': IDENTICAL values between datasets 1 & 2\")\n",
    "    \n",
    "    if differences_found:\n",
    "        print(f\"\\n SUCCESS: Datasets show proper variability!\")\n",
    "    else:\n",
    "        print(f\"\\n  WARNING: Datasets appear identical - check random_state implementation\")\n",
    "    \n",
    "    return differences_found\n",
    "\n",
    "# Run the check\n",
    "check_imputation_differences(imputed_dfs)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\" MICE IMPUTATION COMPLETE!\")\n",
    "print(\"=\" * 50)\n",
    "print(f\" Created {n_imputations} imputed datasets\")\n",
    "print(f\" Applied rounding to all numeric columns\")\n",
    "print(f\" Saved files in: {save_folder}/\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8be134f2-0f23-4614-900c-f0e3d2b5bc3c",
   "metadata": {},
   "source": [
    "Imputation check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b4bd5c-a5ad-4fb8-9f74-620e4d2d14cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# ========== METHOD 1: QUICK CHECK - Compare first 2 datasets ==========\n",
    "def quick_difference_check(imputed_dfs):\n",
    "    \"\"\"Quick check to see if first two datasets are different\"\"\"\n",
    "    if len(imputed_dfs) < 2:\n",
    "        print(\"Need at least 2 datasets to compare\")\n",
    "        return\n",
    "    \n",
    "    df1 = imputed_dfs[0]\n",
    "    df2 = imputed_dfs[1]\n",
    "    \n",
    "    # Check if dataframes are identical\n",
    "    are_identical = df1.equals(df2)\n",
    "    print(f\"Dataset 1 vs Dataset 2: {'IDENTICAL ❌' if are_identical else 'DIFFERENT ✅'}\")\n",
    "    \n",
    "    if not are_identical:\n",
    "        # Count different values\n",
    "        numeric_cols = df1.select_dtypes(include=[np.number]).columns\n",
    "        total_diff = 0\n",
    "        for col in numeric_cols:\n",
    "            diff_count = np.sum(df1[col] != df2[col])\n",
    "            if diff_count > 0:\n",
    "                total_diff += diff_count\n",
    "                print(f\"  '{col}': {diff_count} different values\")\n",
    "        print(f\"  Total different values: {total_diff}\")\n",
    "\n",
    "# ========== METHOD 2: DETAILED CHECK - All pairwise comparisons ==========\n",
    "def detailed_difference_check(imputed_dfs):\n",
    "    \"\"\"Check differences between all pairs of datasets\"\"\"\n",
    "    n_datasets = len(imputed_dfs)\n",
    "    print(f\"\\n=== Checking all {n_datasets} datasets ===\")\n",
    "    \n",
    "    numeric_cols = imputed_dfs[0].select_dtypes(include=[np.number]).columns\n",
    "    \n",
    "    for i in range(n_datasets):\n",
    "        for j in range(i+1, n_datasets):\n",
    "            are_identical = imputed_dfs[i].equals(imputed_dfs[j])\n",
    "            print(f\"Dataset {i+1} vs Dataset {j+1}: {'IDENTICAL ❌' if are_identical else 'DIFFERENT ✅'}\")\n",
    "\n",
    "# ========== METHOD 3: FOCUS ON ORIGINALLY MISSING VALUES ==========\n",
    "def check_missing_value_differences(original_df, imputed_dfs):\n",
    "    \"\"\"Check differences only in originally missing positions\"\"\"\n",
    "    print(f\"\\n=== Checking differences in originally missing positions ===\")\n",
    "    \n",
    "    numeric_cols = original_df.select_dtypes(include=[np.number]).columns\n",
    "    \n",
    "    differences_found = False\n",
    "    for col in numeric_cols:\n",
    "        if original_df[col].isnull().any():\n",
    "            missing_mask = original_df[col].isnull()\n",
    "            print(f\"\\nColumn '{col}' ({missing_mask.sum()} missing values):\")\n",
    "            \n",
    "            # Compare imputed values at missing positions\n",
    "            for i in range(len(imputed_dfs)-1):\n",
    "                imp1_values = imputed_dfs[i].loc[missing_mask, col]\n",
    "                imp2_values = imputed_dfs[i+1].loc[missing_mask, col]\n",
    "                \n",
    "                are_same = np.array_equal(imp1_values.values, imp2_values.values)\n",
    "                if not are_same:\n",
    "                    differences_found = True\n",
    "                    diff_count = np.sum(imp1_values.values != imp2_values.values)\n",
    "                    print(f\"  Dataset {i+1} vs {i+2}: {diff_count}/{len(imp1_values)} different imputed values ✅\")\n",
    "                else:\n",
    "                    print(f\"  Dataset {i+1} vs {i+2}: IDENTICAL imputed values ❌\")\n",
    "    \n",
    "    return differences_found\n",
    "\n",
    "# ========== METHOD 4: SAMPLE VALUES FROM EACH DATASET ==========\n",
    "def show_sample_imputed_values(original_df, imputed_dfs, n_samples=5):\n",
    "    \"\"\"Show sample imputed values from each dataset\"\"\"\n",
    "    print(f\"\\n=== Sample imputed values (first {n_samples} missing positions) ===\")\n",
    "    \n",
    "    numeric_cols = original_df.select_dtypes(include=[np.number]).columns\n",
    "    \n",
    "    for col in numeric_cols:\n",
    "        if original_df[col].isnull().any():\n",
    "            missing_positions = original_df[original_df[col].isnull()].index[:n_samples]\n",
    "            \n",
    "            print(f\"\\nColumn '{col}' at positions {list(missing_positions)}:\")\n",
    "            for i, df_imp in enumerate(imputed_dfs):\n",
    "                values = df_imp.loc[missing_positions, col].values\n",
    "                print(f\"  Dataset {i+1}: {values}\")\n",
    "\n",
    "# ========== RUN ALL CHECKS ==========\n",
    "print(\"=\" * 60)\n",
    "print(\"CHECKING IMPUTATION DIFFERENCES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Method 1: Quick check\n",
    "quick_difference_check(imputed_dfs)\n",
    "\n",
    "# Method 2: All pairwise comparisons  \n",
    "detailed_difference_check(imputed_dfs)\n",
    "\n",
    "# Method 3: Focus on originally missing values (assumes 'df' is your original dataframe)\n",
    "if 'df' in globals():\n",
    "    differences_found = check_missing_value_differences(df, imputed_dfs)\n",
    "    if differences_found:\n",
    "        print(f\"\\n🎉 SUCCESS: Found differences in imputed values!\")\n",
    "    else:\n",
    "        print(f\"\\n⚠️ WARNING: No differences found in imputed values!\")\n",
    "\n",
    "# Method 4: Show sample values\n",
    "if 'df' in globals():\n",
    "    show_sample_imputed_values(df, imputed_dfs, n_samples=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0b31cd-485d-43af-aa6b-2c74bfabed76",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "imputed_folder = \"imputed_data\"\n",
    "n_imputations = 5\n",
    "\n",
    "# Lists to hold DataFrames and Y vectors\n",
    "imputed_dfs = []\n",
    "Y_list = []\n",
    "\n",
    "for i in range(1, n_imputations + 1):\n",
    "    file_path = f\"{imputed_folder}/df_imputed_final_imp{i}.pkl\"\n",
    "    \n",
    "    # Load imputed DataFrame\n",
    "    df_imp = pd.read_pickle(file_path)\n",
    "    imputed_dfs.append(df_imp)\n",
    "\n",
    "    # Define Y for this imputation\n",
    "    Y = df_imp[\"caps5_change_baseline\"]\n",
    "    Y_list.append(Y)\n",
    "\n",
    "    print(f\"Y for imputation {i} defined. Sample values:\")\n",
    "    print(Y.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d30451e-4ae5-433b-94a0-946273af517f",
   "metadata": {},
   "outputs": [],
   "source": [
    "covariates_CAT_ADHD = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline', 'CAT_Anti_epileptica', 'CAT_Antidepressiva', 'CAT_Antipsychotica',\n",
    "    'CAT_Benzodiazepine', 'CAT_Opioden',\n",
    "    'CAT_Z_drugs', 'DIAGNOSIS_CHILDHOOD_TRAUMA', 'DIAGNOSIS_CPTSD',\n",
    "    'DIAGNOSIS_SEXUAL_TRAUMA', 'DIAGNOSIS_SMOKING', 'DIAGNOSIS_SUICIDALITY',\n",
    "    'age',\n",
    "    'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"EB_NON_TF_THERAPY\", \"OTHER_TREATM_APPROACH\", \"Bipolar_and_Mood_disorder\", \"ethnicity_Dutch\", \"ethnicity_other\"\n",
    "]\n",
    "\n",
    "\n",
    "covariates_CAT_Aceetanilidederivaten = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline', 'CAT_Anti_epileptica', 'CAT_Antidepressiva', 'CAT_Antipsychotica',\n",
    "    'CAT_Benzodiazepine', 'CAT_Opioden', 'CAT_Z_drugs',\n",
    "    'DIAGNOSIS_CHILDHOOD_TRAUMA', 'DIAGNOSIS_CPTSD', 'DIAGNOSIS_SEXUAL_TRAUMA',\n",
    "    'DIAGNOSIS_SMOKING', 'DIAGNOSIS_SUICIDALITY', 'age',\n",
    "    'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"EB_NON_TF_THERAPY\", \"OTHER_TREATM_APPROACH\", \"Bipolar_and_Mood_disorder\", \"ethnicity_Dutch\", \"ethnicity_other\"\n",
    "]\n",
    "\n",
    "\n",
    "covariates_CAT_Z_drugs = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline', 'CAT_Anti_epileptica', 'CAT_Antidepressiva', 'CAT_Antipsychotica',\n",
    "    'CAT_Benzodiazepine', 'CAT_Opioden',\n",
    "    'DIAGNOSIS_CHILDHOOD_TRAUMA', 'DIAGNOSIS_CPTSD', 'DIAGNOSIS_SEXUAL_TRAUMA',\n",
    "    'DIAGNOSIS_SMOKING', 'DIAGNOSIS_SUICIDALITY', 'age',\n",
    "    'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"EB_NON_TF_THERAPY\", \"OTHER_TREATM_APPROACH\", \"Bipolar_and_Mood_disorder\", \"ethnicity_Dutch\", \"ethnicity_other\"\n",
    "]\n",
    "\n",
    "\n",
    "covariates_CAT_Opioden = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline', 'CAT_Anti_epileptica',\n",
    "    'CAT_Antidepressiva',\n",
    "    'CAT_Antipsychotica', 'CAT_Benzodiazepine', 'CAT_Z_drugs',\n",
    "    'DIAGNOSIS_CHILDHOOD_TRAUMA', 'DIAGNOSIS_CPTSD', 'DIAGNOSIS_SEXUAL_TRAUMA',\n",
    "    'DIAGNOSIS_SMOKING', 'DIAGNOSIS_SUICIDALITY', 'age',\n",
    "    'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"EB_NON_TF_THERAPY\", \"OTHER_TREATM_APPROACH\", \"Bipolar_and_Mood_disorder\", \"ethnicity_Dutch\", \"ethnicity_other\"\n",
    "]\n",
    "\n",
    "\n",
    "covariates_CAT_NSAIDs = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline', 'CAT_Anti_epileptica',\n",
    "    'CAT_Antidepressiva',\n",
    "    'CAT_Antipsychotica', 'CAT_Benzodiazepine', 'CAT_Opioden', 'CAT_Z_drugs',\n",
    "    'DIAGNOSIS_CHILDHOOD_TRAUMA', 'DIAGNOSIS_CPTSD', 'DIAGNOSIS_SEXUAL_TRAUMA',\n",
    "    'DIAGNOSIS_SMOKING', 'DIAGNOSIS_SUICIDALITY', 'age',\n",
    "    'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"EB_NON_TF_THERAPY\", \"OTHER_TREATM_APPROACH\", \"Bipolar_and_Mood_disorder\", \"ethnicity_Dutch\", \"ethnicity_other\"\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "covariates_CAT_Benzodiazepine = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline', 'CAT_Anti_epileptica',\n",
    "    'CAT_Antidepressiva',\n",
    "    'CAT_Antipsychotica', 'CAT_Opioden', 'CAT_Z_drugs',\n",
    "    'DIAGNOSIS_CHILDHOOD_TRAUMA', 'DIAGNOSIS_CPTSD', 'DIAGNOSIS_SEXUAL_TRAUMA',\n",
    "    'DIAGNOSIS_SMOKING', 'DIAGNOSIS_SUICIDALITY', 'age',\n",
    "    'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"EB_NON_TF_THERAPY\", \"OTHER_TREATM_APPROACH\", \"Bipolar_and_Mood_disorder\", \"ethnicity_Dutch\", \"ethnicity_other\"\n",
    "]\n",
    "\n",
    "\n",
    "covariates_CAT_Antihypertensiva = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline', 'CAT_Anti_epileptica',\n",
    "    'CAT_Antidepressiva', 'CAT_Antipsychotica',\n",
    "    'CAT_Benzodiazepine', 'CAT_Opioden', 'CAT_Z_drugs',\n",
    "    'DIAGNOSIS_CHILDHOOD_TRAUMA', 'DIAGNOSIS_CPTSD', 'DIAGNOSIS_SEXUAL_TRAUMA',\n",
    "    'DIAGNOSIS_SMOKING', 'DIAGNOSIS_SUICIDALITY', 'age',\n",
    "    'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"EB_NON_TF_THERAPY\", \"OTHER_TREATM_APPROACH\", \"Bipolar_and_Mood_disorder\", \"ethnicity_Dutch\", \"ethnicity_other\"\n",
    "]\n",
    "\n",
    "\n",
    "covariates_CAT_Antihistaminica = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline', 'CAT_Anti_epileptica',\n",
    "    'CAT_Antidepressiva', 'CAT_Antipsychotica',\n",
    "    'CAT_Benzodiazepine', 'CAT_Opioden', 'CAT_Z_drugs',\n",
    "    'DIAGNOSIS_CHILDHOOD_TRAUMA', 'DIAGNOSIS_CPTSD', 'DIAGNOSIS_SEXUAL_TRAUMA',\n",
    "    'DIAGNOSIS_SMOKING', 'DIAGNOSIS_SUICIDALITY', 'age',\n",
    "    'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"EB_NON_TF_THERAPY\", \"OTHER_TREATM_APPROACH\", \"Bipolar_and_Mood_disorder\"\n",
    "]\n",
    "\n",
    "\n",
    "covariates_CAT_Anti_epileptica = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline', 'CAT_Antidepressiva', 'CAT_Antipsychotica',\n",
    "    'CAT_Benzodiazepine', 'CAT_Opioden', 'CAT_Z_drugs',\n",
    "    'DIAGNOSIS_CHILDHOOD_TRAUMA', 'DIAGNOSIS_CPTSD', 'DIAGNOSIS_SEXUAL_TRAUMA',\n",
    "    'DIAGNOSIS_SMOKING', 'DIAGNOSIS_SUICIDALITY', 'age',\n",
    "    'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"EB_NON_TF_THERAPY\", \"OTHER_TREATM_APPROACH\", \"Bipolar_and_Mood_disorder\", \"ethnicity_Dutch\", \"ethnicity_other\"\n",
    "]\n",
    "\n",
    "\n",
    "covariates_CAT_Antidepressiva = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline', 'CAT_Anti_epileptica', 'CAT_Antipsychotica',\n",
    "    'CAT_Benzodiazepine', 'CAT_Opioden', 'CAT_Z_drugs',\n",
    "    'DIAGNOSIS_CHILDHOOD_TRAUMA', 'DIAGNOSIS_CPTSD', 'DIAGNOSIS_SEXUAL_TRAUMA',\n",
    "    'DIAGNOSIS_SMOKING', 'DIAGNOSIS_SUICIDALITY', 'age',\n",
    "    'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"EB_NON_TF_THERAPY\", \"OTHER_TREATM_APPROACH\", \"Bipolar_and_Mood_disorder\", \"ethnicity_Dutch\", \"ethnicity_other\"\n",
    "]\n",
    "\n",
    "\n",
    "covariates_CAT_Antipsychotica = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline', 'CAT_Anti_epileptica',\n",
    "    'CAT_Antidepressiva',\n",
    "    'CAT_Benzodiazepine', 'CAT_Opioden', 'CAT_Z_drugs',\n",
    "    'DIAGNOSIS_CHILDHOOD_TRAUMA', 'DIAGNOSIS_CPTSD', 'DIAGNOSIS_SEXUAL_TRAUMA',\n",
    "    'DIAGNOSIS_SMOKING', 'DIAGNOSIS_SUICIDALITY', 'age',\n",
    "    'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"EB_NON_TF_THERAPY\", \"OTHER_TREATM_APPROACH\", \"Bipolar_and_Mood_disorder\", \"ethnicity_Dutch\", \"ethnicity_other\"\n",
    "]\n",
    "\n",
    "covariates_CAT_ALL_PSYCHOTROPICS_EXCL_BENZO = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline',\n",
    "    'CAT_Benzodiazepine', 'CAT_Anticonceptiva',\n",
    "    'DIAGNOSIS_CHILDHOOD_TRAUMA', 'DIAGNOSIS_CPTSD', 'DIAGNOSIS_SEXUAL_TRAUMA',\n",
    "    'DIAGNOSIS_SMOKING', 'DIAGNOSIS_SUICIDALITY', 'age',\n",
    "    'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"EB_NON_TF_THERAPY\", \"OTHER_TREATM_APPROACH\", \"Bipolar_and_Mood_disorder\", \"ethnicity_Dutch\", \"ethnicity_other\"\n",
    "]\n",
    "\n",
    "covariates_CAT_ALL_PSYCHOTROPICS_EXCL_SEDATIVES_HYPNOTICS = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline',\n",
    "    'CAT_Benzodiazepine', 'CAT_Z_drugs', 'CAT_Anticonceptiva',\n",
    "    'DIAGNOSIS_CHILDHOOD_TRAUMA', 'DIAGNOSIS_CPTSD', 'DIAGNOSIS_SEXUAL_TRAUMA',\n",
    "    'DIAGNOSIS_SMOKING', 'DIAGNOSIS_SUICIDALITY', 'age',\n",
    "    'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"EB_NON_TF_THERAPY\", \"OTHER_TREATM_APPROACH\", \"Bipolar_and_Mood_disorder\", \"ethnicity_Dutch\", \"ethnicity_other\"\n",
    "]\n",
    "\n",
    "\n",
    "covariates_CAT_ALL_PSYCHOTROPICS = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline',\n",
    "    'DIAGNOSIS_CHILDHOOD_TRAUMA', 'DIAGNOSIS_CPTSD', 'DIAGNOSIS_SEXUAL_TRAUMA',\n",
    "    'DIAGNOSIS_SMOKING', 'DIAGNOSIS_SUICIDALITY', 'age',\n",
    "    'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"EB_NON_TF_THERAPY\", \"OTHER_TREATM_APPROACH\", \"Bipolar_and_Mood_disorder\", \"ethnicity_Dutch\", \"ethnicity_other\"\n",
    "]\n",
    "\n",
    "\n",
    "covariates_CAT_ALL = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline',\n",
    "    'DIAGNOSIS_CHILDHOOD_TRAUMA',\n",
    "    'DIAGNOSIS_CPTSD', 'DIAGNOSIS_SEXUAL_TRAUMA',\n",
    "    'DIAGNOSIS_SMOKING', 'DIAGNOSIS_SUICIDALITY',\n",
    "    'age', 'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"EB_NON_TF_THERAPY\", \"OTHER_TREATM_APPROACH\", \"Bipolar_and_Mood_disorder\", \"ethnicity_Dutch\", \"ethnicity_other\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7887112f-e4d1-4beb-b93b-1d96a34cf5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# This finds all variables that start with covariates_CAT_ or covariates_cat_\n",
    "final_covariates_map = defaultdict(list)\n",
    "final_covariates_map.update({\n",
    "    var.replace(\"covariates_\", \"\"): val\n",
    "    for var, val in globals().items()\n",
    "    if var.lower().startswith(\"covariates_cat_\") and isinstance(val, list)\n",
    "})\n",
    "\n",
    "# Show detected group names\n",
    "print(\"Groups found:\", list(final_covariates_map.keys()))\n",
    "medication_groups = list(final_covariates_map.keys())\n",
    "print(medication_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0586b8-17ca-4e92-acc8-b137bd4e98a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "def run_all_CAT_group_models(imputed_dfs):\n",
    "    \"\"\"\n",
    "    Runs downstream analysis for each CAT medication group using imputed datasets.\n",
    "\n",
    "    Parameters:\n",
    "    - imputed_dfs: list of 5 imputed DataFrames (from df_imputed_final_imp1.pkl ... imp5.pkl)\n",
    "    \n",
    "    Notes:\n",
    "    - Covariate lists must be defined as global variables: covariates_cat_<group>\n",
    "    - Outputs are saved in: outputs/CAT_<GROUP>/\n",
    "    \"\"\"\n",
    "\n",
    "    print(\" Starting analysis for all CAT groups\")\n",
    "\n",
    "    for var_name in globals():\n",
    "        if var_name.lower().startswith(\"covariates_cat_\") and isinstance(globals()[var_name], list):\n",
    "            group_name = var_name.replace(\"covariates_\", \"\")\n",
    "            group_name = group_name.replace(\"_\", \" \").title().replace(\" \", \"_\")  # e.g., cat_z_drugs → Cat_Z_Drugs\n",
    "            group_name = group_name.replace(\"Cat_\", \"CAT_\")  # force prefix to uppercase\n",
    "\n",
    "            covariates = globals()[var_name]\n",
    "            output_dir = f\"outputs/{group_name}\"\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "            print(f\"\\n Processing {group_name}...\")\n",
    "\n",
    "            for k, df_imp in enumerate(imputed_dfs):\n",
    "                print(f\"  → Using imputation {k+1}\")\n",
    "\n",
    "                # Define X and Y\n",
    "                X = df_imp[covariates]\n",
    "                Y = df_imp[\"caps5_change_baseline\"]\n",
    "\n",
    "                # === Save X and Y as placeholder (replace with modeling later)\n",
    "                X.to_csv(f\"{output_dir}/X_imp{k+1}.csv\", index=False)\n",
    "                Y.to_frame(name=\"Y\").to_csv(f\"{output_dir}/Y_imp{k+1}.csv\", index=False)\n",
    "\n",
    "            print(f\" Done: {group_name}\")\n",
    "\n",
    "    print(\"\\n All CAT group analyses complete.\")\n",
    "\n",
    "# ========= STEP 4: Execute ========= #\n",
    "run_all_CAT_group_models(imputed_dfs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964aaa15-0261-4194-b3ea-3d42c6e093d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for treatment_var in medication_groups:\n",
    "    print(f\"\\n {treatment_var}\")\n",
    "    \n",
    "    for i, df in enumerate(imputed_dfs):\n",
    "        if treatment_var not in df.columns:\n",
    "            print(f\"  Imp {i+1}:  Not found in columns.\")\n",
    "            continue\n",
    "\n",
    "        treated = (df[treatment_var] == 1).sum()\n",
    "        control = (df[treatment_var] == 0).sum()\n",
    "        missing = df[treatment_var].isna().sum()\n",
    "\n",
    "        print(f\"  Imp {i+1}: Treated = {treated}, Control = {control}, Missing = {missing}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556179f5-ba78-4e8d-ac27-5632bc5aaf4e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from collections import defaultdict\n",
    "\n",
    "# ✅ VIF computation function\n",
    "def compute_vif(X):\n",
    "    X = sm.add_constant(X, has_constant='add')\n",
    "    vif_df = pd.DataFrame()\n",
    "    vif_df[\"variable\"] = X.columns\n",
    "    vif_df[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "    return vif_df\n",
    "\n",
    "# ✅ Process each group\n",
    "for group in medication_groups:\n",
    "    print(f\"\\n🔍 Processing VIF for {group}\")\n",
    "\n",
    "    if group not in final_covariates_map:\n",
    "        print(f\" ⚠️ No covariates found for {group}. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    covariates = final_covariates_map[group]\n",
    "    vif_list = []\n",
    "\n",
    "    for i, df_imp in enumerate(imputed_dfs):\n",
    "        try:\n",
    "            X = df_imp[covariates].copy()\n",
    "            vif_df = compute_vif(X)\n",
    "            vif_df[\"imputation\"] = i + 1\n",
    "            vif_list.append(vif_df)\n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ Failed on imputation {i+1} for {group}: {e}\")\n",
    "\n",
    "    if vif_list:\n",
    "        all_vif = pd.concat(vif_list)\n",
    "        pooled_vif = all_vif.groupby(\"variable\")[\"VIF\"].mean().reset_index()\n",
    "        pooled_vif = pooled_vif.sort_values(by=\"VIF\", ascending=False)\n",
    "\n",
    "        output_folder = os.path.join(\"outputs\", group)\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "        pooled_vif.to_csv(os.path.join(output_folder, \"pooled_vif.csv\"), index=False)\n",
    "\n",
    "        print(f\" ✅ Saved: {output_folder}/pooled_vif.csv\")\n",
    "    else:\n",
    "        print(f\" ⚠️ Skipped {group}: No valid imputations.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8f0127-0657-483c-9469-c4d67267d7cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ---------- PS Estimation Function ----------\n",
    "def run_xgboost_ps_modeling(imputed_dfs, medication_groups, final_covariates_map):\n",
    "    for group in medication_groups:\n",
    "        print(f\" Running PS estimation for {group}\")\n",
    "\n",
    "        if group not in final_covariates_map:\n",
    "            print(f\" No covariates found for {group}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        output_folder = os.path.join(\"outputs\", group)\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "        covariates = final_covariates_map[group]\n",
    "        ps_matrix = pd.DataFrame()\n",
    "        auc_list = []\n",
    "\n",
    "        for i, df_imp in enumerate(imputed_dfs):\n",
    "            if group not in df_imp.columns:\n",
    "                print(f\" {group} not found in imputed dataset {i+1}. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            X = df_imp[covariates].copy()\n",
    "            T = df_imp[group]\n",
    "\n",
    "            # Drop missing treatment rows\n",
    "            valid_idx = T.dropna().index\n",
    "            X = X.loc[valid_idx]\n",
    "            T = T.loc[valid_idx]\n",
    "\n",
    "            # Train-test split for ROC\n",
    "            X_train, X_test, T_train, T_test = train_test_split(\n",
    "                X, T, stratify=T, test_size=0.3, random_state=42\n",
    "            )\n",
    "\n",
    "            try:\n",
    "                model = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "                model.fit(X_train, T_train)\n",
    "\n",
    "                ps_scores = model.predict_proba(X)[:, 1]\n",
    "                ps_matrix[f\"ps_imp{i+1}\"] = pd.Series(ps_scores, index=valid_idx)\n",
    "\n",
    "                # ROC & AUC\n",
    "                auc = roc_auc_score(T_test, model.predict_proba(X_test)[:, 1])\n",
    "                auc_list.append(auc)\n",
    "\n",
    "                fpr, tpr, _ = roc_curve(T_test, model.predict_proba(X_test)[:, 1])\n",
    "                plt.figure()\n",
    "                plt.plot(fpr, tpr, label=f\"AUC = {auc:.3f}\")\n",
    "                plt.plot([0, 1], [0, 1], 'k--')\n",
    "                plt.xlabel(\"False Positive Rate\")\n",
    "                plt.ylabel(\"True Positive Rate\")\n",
    "                plt.title(f\"ROC Curve - {group} (Imp {i+1})\")\n",
    "                plt.legend()\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(os.path.join(output_folder, f\"roc_curve_imp{i+1}.png\"))\n",
    "                plt.close()\n",
    "                print(f\"   Imp {i+1}: AUC = {auc:.3f}, ROC saved.\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"   Error in {group} (imp {i+1}): {e}\")\n",
    "\n",
    "        # Save AUCs and Composite PS\n",
    "        if not ps_matrix.empty:\n",
    "            # Fill NaN rows (from dropped subjects in some imputations) with mean\n",
    "            ps_matrix[\"composite_ps\"] = ps_matrix.mean(axis=1)\n",
    "            ps_matrix.to_excel(os.path.join(output_folder, \"propensity_scores.xlsx\"))\n",
    "\n",
    "            auc_df = pd.DataFrame({\n",
    "                \"imputation\": [f\"imp{i+1}\" for i in range(len(auc_list))],\n",
    "                \"AUC\": auc_list\n",
    "            })\n",
    "            auc_df.loc[len(auc_df.index)] = [\"mean\", np.mean(auc_list) if auc_list else np.nan]\n",
    "            auc_df.to_excel(os.path.join(output_folder, \"auc_scores.xlsx\"), index=False)\n",
    "\n",
    "            print(f\" Composite PS + AUC saved for {group}\")\n",
    "        else:\n",
    "            print(f\" No valid PS scores generated for {group}\")\n",
    "\n",
    "# ---------- Run ----------\n",
    "run_xgboost_ps_modeling(imputed_dfs, medication_groups, final_covariates_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7ac931-822f-4b4a-b79e-62211e9b032c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "def compute_feature_importance(imputed_dfs, medication_groups, final_covariates_map):\n",
    "    for group in medication_groups:\n",
    "        print(f\"\\n Computing feature importance for {group}\")\n",
    "\n",
    "        if group not in final_covariates_map:\n",
    "            print(f\" No covariates found for {group}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        covariates = final_covariates_map[group]\n",
    "        output_folder = os.path.join(\"outputs\", group)\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "        importance_df_list = []\n",
    "\n",
    "        for i, df_imp in enumerate(imputed_dfs):\n",
    "            if group not in df_imp.columns:\n",
    "                print(f\" {group} not in imputed dataset {i+1}. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            X = df_imp[covariates].copy()\n",
    "            T = df_imp[group]\n",
    "\n",
    "            # Drop NaNs in treatment\n",
    "            valid_idx = T.dropna().index\n",
    "            X = X.loc[valid_idx]\n",
    "            T = T.loc[valid_idx]\n",
    "\n",
    "            try:\n",
    "                model = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "                model.fit(X, T)\n",
    "\n",
    "                # Get feature importance\n",
    "                importances = model.get_booster().get_score(importance_type='gain')\n",
    "                df_feat = pd.DataFrame.from_dict(importances, orient='index', columns=[f\"imp{i+1}\"])\n",
    "                df_feat.index.name = 'feature'\n",
    "                importance_df_list.append(df_feat)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"   Error during modeling: {e}\")\n",
    "\n",
    "        if importance_df_list:\n",
    "            # Combine and average\n",
    "            all_feat = pd.concat(importance_df_list, axis=1).fillna(0)\n",
    "            all_feat[\"mean_importance\"] = all_feat.mean(axis=1)\n",
    "\n",
    "            # Filter top 30 non-zero\n",
    "            non_zero = all_feat[all_feat[\"mean_importance\"] > 0]\n",
    "            top30 = non_zero.sort_values(by=\"mean_importance\", ascending=False).head(30)\n",
    "\n",
    "            # Save to CSV\n",
    "            top30.to_csv(os.path.join(output_folder, \"feature_importance.csv\"))\n",
    "\n",
    "            # Plot\n",
    "            plt.figure(figsize=(10, 8))\n",
    "            plt.barh(top30.index[::-1], top30[\"mean_importance\"][::-1])  # plot top → bottom\n",
    "            plt.xlabel(\"Mean Gain Importance\")\n",
    "            plt.title(f\"Top 30 Feature Importance - {group}\")\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(output_folder, \"feature_importance_top30.png\"))\n",
    "            plt.close()\n",
    "\n",
    "            print(f\" Saved feature importance plot and CSV for {group}\")\n",
    "        else:\n",
    "            print(f\" No valid models for {group}\")\n",
    "\n",
    "#  Run\n",
    "compute_feature_importance(imputed_dfs, medication_groups, final_covariates_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151ba168-9ae5-400a-a2cd-0485a8a7c5a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def compute_trimmed_clipped_iptw(ps_df, treatment, lower=0.05, upper=0.95, clip_max=10):\n",
    "    weights = []\n",
    "    keep_mask = (ps_df > lower) & (ps_df < upper)\n",
    "\n",
    "    for i in range(ps_df.shape[1]):\n",
    "        ps = ps_df.iloc[:, i].clip(lower=1e-6, upper=1 - 1e-6)  # avoid div by zero\n",
    "        mask = keep_mask.iloc[:, i]\n",
    "        w = pd.Series(np.nan, index=ps.index)\n",
    "\n",
    "        w[mask & (treatment == 1)] = 1 / ps[mask & (treatment == 1)]\n",
    "        w[mask & (treatment == 0)] = 1 / (1 - ps[mask & (treatment == 0)])\n",
    "        w = w.clip(upper=clip_max)\n",
    "        weights.append(w)\n",
    "\n",
    "    return pd.concat(weights, axis=1)\n",
    "\n",
    "\n",
    "def apply_rubins_rule_to_iptw(iptw_matrix):\n",
    "    \"\"\"\n",
    "    Given an IPTW matrix (n rows × M imputations), return Rubin’s rule pooled mean, SD, SE.\n",
    "    \"\"\"\n",
    "    M = iptw_matrix.shape[1]\n",
    "    q_bar = iptw_matrix.mean(axis=1)\n",
    "    u_bar = iptw_matrix.var(axis=1, ddof=1)\n",
    "    B = iptw_matrix.apply(lambda x: x.mean(), axis=1).var(ddof=1)\n",
    "    total_var = u_bar + (1 + 1/M) * B\n",
    "    total_se = np.sqrt(total_var)\n",
    "    return q_bar, u_bar.pow(0.5), total_se\n",
    "\n",
    "\n",
    "def run_trim_clip_save_all(imputed_dfs, medication_groups, final_covariates_map):\n",
    "    for group in medication_groups:\n",
    "        print(f\"\\n🔍 Processing IPTW + trimming + clipping for {group}\")\n",
    "        output_folder = os.path.join(\"outputs\", group)\n",
    "        ps_path = os.path.join(output_folder, \"propensity_scores.xlsx\")\n",
    "\n",
    "        if not os.path.exists(ps_path):\n",
    "            print(f\"⚠️ Missing PS file: {ps_path}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            ps_all = pd.read_excel(ps_path, index_col=0)\n",
    "            ps_cols = [col for col in ps_all.columns if col.startswith(\"ps_imp\")]\n",
    "            composite_index = ps_all.index\n",
    "\n",
    "            # Get treatment from one imputed dataset\n",
    "            T_full = None\n",
    "            for df in imputed_dfs:\n",
    "                if group in df.columns:\n",
    "                    T_full = df.loc[composite_index, group]\n",
    "                    break\n",
    "\n",
    "            if T_full is None:\n",
    "                print(f\"❌ Treatment column {group} not found in any imputed dataset.\")\n",
    "                continue\n",
    "\n",
    "            # Compute IPTW matrix (shape: n × M)\n",
    "            iptw_matrix = compute_trimmed_clipped_iptw(ps_all[ps_cols], T_full)\n",
    "            iptw_matrix.columns = [f\"iptw_imp{i+1}\" for i in range(iptw_matrix.shape[1])]\n",
    "\n",
    "            # Apply Rubin’s Rule for mean, SD, SE\n",
    "            iptw_matrix[\"iptw_mean\"], iptw_matrix[\"iptw_sd\"], iptw_matrix[\"iptw_se\"] = apply_rubins_rule_to_iptw(\n",
    "                iptw_matrix[[f\"iptw_imp{i+1}\" for i in range(iptw_matrix.shape[1])]]\n",
    "            )\n",
    "\n",
    "            # Save IPTW matrix separately\n",
    "            iptw_matrix.to_excel(os.path.join(output_folder, \"iptw_weights.xlsx\"))\n",
    "            print(f\"✅ Saved IPTW weights for {group}\")\n",
    "\n",
    "            # Save trimmed & clipped imputed datasets with IPTW\n",
    "            for i in range(5):\n",
    "                df = imputed_dfs[i].copy()\n",
    "                if group not in df.columns:\n",
    "                    continue\n",
    "\n",
    "                trimmed_idx = iptw_matrix.index.intersection(df.index)\n",
    "                needed_cols = final_covariates_map[group] + [group, \"caps5_change_baseline\"]\n",
    "\n",
    "                # Select only necessary columns\n",
    "                df_trimmed = df.loc[trimmed_idx, needed_cols].copy()\n",
    "                df_trimmed[\"iptw\"] = iptw_matrix[f\"iptw_imp{i+1}\"].loc[trimmed_idx]\n",
    "\n",
    "                # ✅ DROP rows with missing IPTW values\n",
    "                before = len(df_trimmed)\n",
    "                df_trimmed = df_trimmed.dropna(subset=[\"iptw\"])\n",
    "                after = len(df_trimmed)\n",
    "                print(f\"    ℹ️ Retained {after}/{before} rows after IPTW NaN drop.\")\n",
    "\n",
    "                # Save to .pkl\n",
    "                df_trimmed.to_pickle(os.path.join(output_folder, f\"trimmed_data_imp{i+1}.pkl\"))\n",
    "                print(f\"  💾 Saved trimmed dataset: {output_folder}/trimmed_data_imp{i+1}.*\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error in {group}: {e}\")\n",
    "\n",
    "\n",
    "run_trim_clip_save_all(imputed_dfs, medication_groups, final_covariates_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ee9f71-87cb-49de-93a7-6ec93070579f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_ps_overlap_all_groups(medication_groups):\n",
    "    for group in medication_groups:\n",
    "        print(f\"\\n📊 Plotting PS overlap for {group}\")\n",
    "\n",
    "        folder = os.path.join(\"outputs\", group)\n",
    "        ps_file = os.path.join(folder, \"propensity_scores.xlsx\")\n",
    "        iptw_file = os.path.join(folder, \"iptw_weights.xlsx\")\n",
    "        trimmed_file = os.path.join(folder, \"trimmed_data_imp1.pkl\")\n",
    "\n",
    "        if not all(os.path.exists(f) for f in [ps_file, iptw_file, trimmed_file]):\n",
    "            print(f\"⚠️ Missing required files for {group}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            ps_df = pd.read_excel(ps_file, index_col=0)\n",
    "            iptw_df = pd.read_excel(iptw_file, index_col=0)\n",
    "            trimmed_df = pd.read_pickle(trimmed_file)\n",
    "\n",
    "            # Extract\n",
    "            ps = ps_df[\"composite_ps\"].reindex(trimmed_df.index)\n",
    "            w = iptw_df[\"iptw_mean\"].reindex(trimmed_df.index)\n",
    "            T = trimmed_df[group]\n",
    "\n",
    "            # Masks to remove NaNs\n",
    "            treated_mask = (T == 1) & ps.notna() & w.notna()\n",
    "            control_mask = (T == 0) & ps.notna() & w.notna()\n",
    "\n",
    "            treated = ps[treated_mask]\n",
    "            treated_w = w[treated_mask]\n",
    "\n",
    "            control = ps[control_mask]\n",
    "            control_w = w[control_mask]\n",
    "\n",
    "            # === Unweighted Plot ===\n",
    "            plt.figure(figsize=(8, 5))\n",
    "            plt.hist([treated, control], bins=25, label=[\"Treated\", \"Control\"], alpha=0.6)\n",
    "            plt.title(f\"Unweighted PS Overlap - {group}\")\n",
    "            plt.xlabel(\"Composite Propensity Score\")\n",
    "            plt.ylabel(\"Count\")\n",
    "            plt.legend()\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(folder, \"ps_overlap_unweighted.png\"))\n",
    "            plt.close()\n",
    "\n",
    "            # === Weighted Plot ===\n",
    "            plt.figure(figsize=(8, 5))\n",
    "            plt.hist([treated, control], bins=25, weights=[treated_w, control_w], label=[\"Treated\", \"Control\"], alpha=0.6)\n",
    "            plt.title(f\"Weighted PS Overlap - {group}\")\n",
    "            plt.xlabel(\"Composite Propensity Score\")\n",
    "            plt.ylabel(\"Weighted Count\")\n",
    "            plt.legend()\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(folder, \"ps_overlap_weighted.png\"))\n",
    "            plt.close()\n",
    "\n",
    "            print(f\"✅ Saved unweighted and weighted PS plots for {group}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error processing {group}: {e}\")\n",
    "\n",
    "# 🔁 Run\n",
    "plot_ps_overlap_all_groups(medication_groups)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de65729a-8951-4439-a512-fdbd8bc123fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set up base output folder\n",
    "output_base = \"outputs\"\n",
    "ps_file = \"propensity_scores.xlsx\"\n",
    "iptw_file = \"iptw_weights.xlsx\"\n",
    "trimmed_data_file = \"trimmed_data_imp1.pkl\"\n",
    "\n",
    "# Collect all treatment group folders\n",
    "groups = [g for g in medication_groups if os.path.isdir(os.path.join(output_base, g))]\n",
    "\n",
    "# Generate 4-panel overlap plots\n",
    "for group in groups:\n",
    "    group_path = os.path.join(output_base, group)\n",
    "    try:\n",
    "        # Load trimmed treatment info\n",
    "        trimmed_df = pd.read_pickle(os.path.join(group_path, trimmed_data_file))\n",
    "        index = trimmed_df.index\n",
    "\n",
    "        # Fix: case-insensitive match for treatment variable\n",
    "        possible_cols = [col for col in trimmed_df.columns if col.upper() == group.upper()]\n",
    "        if not possible_cols:\n",
    "            print(f\" Treatment variable {group} not found in {group}, skipping.\")\n",
    "            continue\n",
    "        treatment_var = possible_cols[0]\n",
    "        T = trimmed_df[treatment_var]\n",
    "\n",
    "        # Load composite PS (aligned to trimmed_df index)\n",
    "        ps_df = pd.read_excel(os.path.join(group_path, ps_file), index_col=0)\n",
    "        if 'composite_ps' not in ps_df.columns:\n",
    "            print(f\" Composite column missing in {ps_file}, skipping {group}.\")\n",
    "            continue\n",
    "        ps = ps_df.loc[index, 'composite_ps']\n",
    "\n",
    "        # Load IPTW weights (aligned to trimmed_df index)\n",
    "        weights_df = pd.read_excel(os.path.join(group_path, iptw_file), index_col=0)\n",
    "        if 'iptw_mean' not in weights_df.columns:\n",
    "            print(f\" IPTW weight column missing in {iptw_file}, skipping {group}.\")\n",
    "            continue\n",
    "        weights = weights_df.loc[index, 'iptw_mean']\n",
    "\n",
    "        # Prepare 4 datasets\n",
    "        raw_treated = ps[T == 1]\n",
    "        raw_control = ps[T == 0]\n",
    "        weighted_treated = (ps[T == 1], weights[T == 1])\n",
    "        weighted_control = (ps[T == 0], weights[T == 0])\n",
    "\n",
    "        # Create plot\n",
    "        fig, axs = plt.subplots(2, 2, figsize=(12, 8))\n",
    "        fig.suptitle(f\"Propensity Score Distribution - {group}\", fontsize=14)\n",
    "\n",
    "        axs[0, 0].hist(raw_treated, bins=20, alpha=0.7, color='blue')\n",
    "        axs[0, 0].set_title(\"Raw Treated\")\n",
    "\n",
    "        axs[0, 1].hist(raw_control, bins=20, alpha=0.7, color='green')\n",
    "        axs[0, 1].set_title(\"Raw Control\")\n",
    "\n",
    "        axs[1, 0].hist(weighted_treated[0], bins=20, weights=weighted_treated[1], alpha=0.7, color='blue')\n",
    "        axs[1, 0].set_title(\"Weighted Treated\")\n",
    "\n",
    "        axs[1, 1].hist(weighted_control[0], bins=20, weights=weighted_control[1], alpha=0.7, color='green')\n",
    "        axs[1, 1].set_title(\"Weighted Control\")\n",
    "\n",
    "        for ax in axs.flat:\n",
    "            ax.set_xlim(0, 1)\n",
    "            ax.set_xlabel(\"Propensity Score\")\n",
    "            ax.set_ylabel(\"Count\")\n",
    "\n",
    "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "\n",
    "        # Save figure\n",
    "        plot_path = os.path.join(group_path, f\"four_panel_overlap_{group}.png\")\n",
    "        plt.savefig(plot_path)\n",
    "        plt.close()\n",
    "        print(f\" ✅ Saved: {plot_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\" ❌ Error in {group}: {e}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4cafc79c-dfbd-4e76-9d53-0351551e895b",
   "metadata": {},
   "source": [
    "# ATT calculation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f41948-593f-43dd-b58e-821d6545c844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weighted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2314df2-9945-4767-82df-24455dedfe58",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import KFold\n",
    "from econml.dml import LinearDML\n",
    "from scipy.stats import t, probplot\n",
    "import xgboost as xgb\n",
    "\n",
    "# -----------------------------\n",
    "# Configuration\n",
    "# -----------------------------\n",
    "n_repeats = 4\n",
    "seeds = list(range(1, 11))\n",
    "imputations = 5\n",
    "output_folder = \"outputs\"\n",
    "\n",
    "# -----------------------------\n",
    "# Diagnostic Plotting Function\n",
    "# -----------------------------\n",
    "def create_diagnostic_plots(residuals_data, fitted_data, group_name):\n",
    "    \"\"\"Create 4 diagnostic plots for model validation\"\"\"\n",
    "    plots_dir = os.path.join(output_folder, \"plots\")\n",
    "    os.makedirs(plots_dir, exist_ok=True)\n",
    "    \n",
    "    # Flatten the collected data\n",
    "    all_residuals = np.concatenate(residuals_data)\n",
    "    all_fitted = np.concatenate(fitted_data)\n",
    "    \n",
    "    # Create figure with 2x2 subplots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    fig.suptitle(f'Diagnostic Plots - {group_name}', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Residuals vs Fitted\n",
    "    axes[0,0].scatter(all_fitted, all_residuals, alpha=0.6, s=20)\n",
    "    axes[0,0].axhline(y=0, color='red', linestyle='--', alpha=0.8)\n",
    "    axes[0,0].set_xlabel('Fitted Values')\n",
    "    axes[0,0].set_ylabel('Residuals')\n",
    "    axes[0,0].set_title('Residuals vs Fitted')\n",
    "    axes[0,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. QQ Plot\n",
    "    probplot(all_residuals, dist=\"norm\", plot=axes[0,1])\n",
    "    axes[0,1].set_title('Q-Q Plot (Normal)')\n",
    "    axes[0,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Residual Histogram\n",
    "    axes[1,0].hist(all_residuals, bins=30, density=True, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    axes[1,0].set_xlabel('Residuals')\n",
    "    axes[1,0].set_ylabel('Density')\n",
    "    axes[1,0].set_title('Residual Distribution')\n",
    "    axes[1,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Scale-Location Plot\n",
    "    sqrt_abs_residuals = np.sqrt(np.abs(all_residuals))\n",
    "    axes[1,1].scatter(all_fitted, sqrt_abs_residuals, alpha=0.6, s=20)\n",
    "    axes[1,1].set_xlabel('Fitted Values')\n",
    "    axes[1,1].set_ylabel('√|Residuals|')\n",
    "    axes[1,1].set_title('Scale-Location Plot')\n",
    "    axes[1,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save plot\n",
    "    plot_filename = os.path.join(plots_dir, f'{group_name}.png')\n",
    "    plt.savefig(plot_filename, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"📊 Diagnostic plots saved for {group_name}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Rubin's Rule\n",
    "# -----------------------------\n",
    "def rubins_pool(estimates, ses):\n",
    "    m = len(estimates)\n",
    "    q_bar = np.mean(estimates)\n",
    "    u_bar = np.mean(np.square(ses))\n",
    "    b_m = np.var(estimates, ddof=1)\n",
    "    total_var = u_bar + ((1 + 1/m) * b_m)\n",
    "    total_se = np.sqrt(total_var)\n",
    "    ci_lower = q_bar - 1.96 * total_se\n",
    "    ci_upper = q_bar + 1.96 * total_se\n",
    "    p_value = 2 * (1 - t.cdf(np.abs(q_bar / total_se), df=m-1))\n",
    "    rounded_p = round(p_value, 5)\n",
    "    formatted_p = \"< 0.00001\" if rounded_p <= 0.00001 else f\"{rounded_p:.5f}\"\n",
    "    return q_bar, total_se, ci_lower, ci_upper, formatted_p\n",
    "\n",
    "# -----------------------------\n",
    "# SMD + Variance Ratio\n",
    "# -----------------------------\n",
    "def calculate_smd_vr(X, T, weights):\n",
    "    treated = X[T == 1]\n",
    "    control = X[T == 0]\n",
    "    w_treated = weights[T == 1]\n",
    "    w_control = weights[T == 0]\n",
    "    smd, vr = [], []\n",
    "    for col in X.columns:\n",
    "        try:\n",
    "            m1, m0 = np.average(treated[col], weights=w_treated), np.average(control[col], weights=w_control)\n",
    "            s1 = np.sqrt(np.average((treated[col] - m1) ** 2, weights=w_treated))\n",
    "            s0 = np.sqrt(np.average((control[col] - m0) ** 2, weights=w_control))\n",
    "            pooled_sd = np.sqrt((s1 ** 2 + s0 ** 2) / 2)\n",
    "            smd.append((m1 - m0) / pooled_sd if pooled_sd > 0 else 0)\n",
    "            vr.append(s1**2 / s0**2 if s0**2 > 0 else 0)\n",
    "        except Exception:\n",
    "            smd.append(np.nan)\n",
    "            vr.append(np.nan)\n",
    "    return np.nanmean(smd), np.nanmean(vr)\n",
    "\n",
    "# -----------------------------\n",
    "# DML Main Loop\n",
    "# -----------------------------\n",
    "def run_dml_with_trimmed_data(final_covariates_map):\n",
    "    att_results = []\n",
    "    balance_results = []\n",
    "\n",
    "    for group, covariates in final_covariates_map.items():\n",
    "        print(f\"\\n🚀 Running DML for {group}\")\n",
    "        group_dir = os.path.join(output_folder, group)\n",
    "        os.makedirs(group_dir, exist_ok=True)\n",
    "\n",
    "        best_result = None\n",
    "        best_se = float(\"inf\")\n",
    "        \n",
    "        # Initialize lists to collect residuals and fitted values for diagnostic plots\n",
    "        group_residuals = []\n",
    "        group_fitted = []\n",
    "\n",
    "        for seed in seeds:\n",
    "            att_list, se_list, r2_list, rmse_list, smd_list, vr_list = [], [], [], [], [], []\n",
    "\n",
    "            for imp in range(1, imputations + 1):\n",
    "                file_path = os.path.join(group_dir, f\"trimmed_data_imp{imp}.pkl\")\n",
    "                if not os.path.exists(file_path):\n",
    "                    continue\n",
    "\n",
    "                df = pd.read_pickle(file_path)\n",
    "                if group not in df.columns or \"iptw\" not in df.columns:\n",
    "                    continue\n",
    "\n",
    "                X = df[covariates].copy()\n",
    "                T = df[group]\n",
    "                Y = df[\"caps5_change_baseline\"]\n",
    "                W = df[\"iptw\"]\n",
    "\n",
    "                for repeat in range(n_repeats):\n",
    "                    kf = KFold(n_splits=5, shuffle=True, random_state=seed + repeat)\n",
    "                    for train_idx, test_idx in kf.split(X):\n",
    "                        try:\n",
    "                            X_train, T_train, Y_train, W_train = (\n",
    "                                X.iloc[train_idx],\n",
    "                                T.iloc[train_idx],\n",
    "                                Y.iloc[train_idx],\n",
    "                                W.iloc[train_idx],\n",
    "                            )\n",
    "\n",
    "                            model_y = xgb.XGBRegressor(n_estimators=100, max_depth=3, learning_rate=0.1, n_jobs=1, random_state=seed)\n",
    "                            model_t = xgb.XGBClassifier(n_estimators=100, max_depth=3, learning_rate=0.1, n_jobs=1,\n",
    "                                                        use_label_encoder=False, eval_metric=\"logloss\", random_state=seed)\n",
    "\n",
    "                            dml = LinearDML(model_y=model_y, model_t=model_t, discrete_treatment=True,\n",
    "                                            cv=KFold(n_splits=3, shuffle=True, random_state=seed), random_state=seed)\n",
    "                            dml.fit(Y_train, T_train, X=X_train, sample_weight=W_train)\n",
    "\n",
    "                            tau = dml.effect(X_train)\n",
    "                            att = np.mean(tau)\n",
    "                            influence = tau - att\n",
    "                            se = np.sqrt(np.mean(influence ** 2) / len(tau))\n",
    "\n",
    "                            att_list.append(att)\n",
    "                            se_list.append(se)\n",
    "\n",
    "                            Y_pred = model_y.fit(X_train, Y_train, sample_weight=W_train).predict(X_train)\n",
    "                            residuals = Y_train - Y_pred\n",
    "                            rmse = mean_squared_error(Y_train, Y_pred, squared=False)\n",
    "                            r2 = r2_score(Y_train, Y_pred)\n",
    "                            r2_list.append(r2)\n",
    "                            rmse_list.append(rmse)\n",
    "                            \n",
    "                            # Collect residuals and fitted values for diagnostic plots\n",
    "                            group_residuals.append(residuals.values)\n",
    "                            group_fitted.append(Y_pred)\n",
    "\n",
    "                            smd, vr = calculate_smd_vr(X_train, T_train, W_train)\n",
    "                            smd_list.append(smd)\n",
    "                            vr_list.append(vr)\n",
    "                        except Exception as e:\n",
    "                            print(f\"⚠️ Error in {group}, seed {seed}, imp {imp}, rep {repeat}: {e}\")\n",
    "\n",
    "            if att_list:\n",
    "                att, se, ci_l, ci_u, p_val = rubins_pool(att_list, se_list)\n",
    "                avg_r2 = np.mean(r2_list)\n",
    "                avg_rmse = np.mean(rmse_list)\n",
    "                avg_smd = np.mean(smd_list)\n",
    "                avg_vr = np.mean(vr_list)\n",
    "\n",
    "                balance_results.append({\n",
    "                    \"group\": group, \"seed\": seed, \"smd\": avg_smd, \"vr\": avg_vr\n",
    "                })\n",
    "\n",
    "                if se < best_se:\n",
    "                    best_se = se\n",
    "                    best_result = {\n",
    "                        \"group\": group, \"seed\": seed, \"att\": att, \"se\": se,\n",
    "                        \"ci_lower\": ci_l, \"ci_upper\": ci_u, \"p_value\": p_val,\n",
    "                        \"r2\": avg_r2, \"rmse\": avg_rmse\n",
    "                    }\n",
    "\n",
    "                print(f\"✅ {group} | Seed {seed}: ATT = {att:.4f}, SE = {se:.4f}, p = {p_val}\")\n",
    "            else:\n",
    "                print(f\"⚠️ No valid results for {group} | Seed {seed}\")\n",
    "\n",
    "        # Create diagnostic plots for this group\n",
    "        if group_residuals and group_fitted:\n",
    "            create_diagnostic_plots(group_residuals, group_fitted, group)\n",
    "\n",
    "        if best_result:\n",
    "            att_results.append(best_result)\n",
    "            print(f\"🏆 Best result for {group} → Seed {best_result['seed']} | SE = {best_result['se']:.4f}\")\n",
    "\n",
    "    # Save final output\n",
    "    pd.DataFrame(att_results).to_excel(\"dml_rubin_summary_cats.xlsx\", index=False)\n",
    "    pd.DataFrame(balance_results).to_excel(\"smd_vr_summary_cats.xlsx\", index=False)\n",
    "    print(\"\\n🎯 All summary files saved.\")\n",
    "\n",
    "run_dml_with_trimmed_data(final_covariates_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18c032f-2250-4dc4-90ec-85ac7faa6f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unweighted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a2f7f2-1ca7-4fe4-a039-2c0ce9ba9a4a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import KFold\n",
    "from econml.dml import LinearDML\n",
    "from scipy.stats import t, probplot\n",
    "import xgboost as xgb\n",
    "\n",
    "# -----------------------------\n",
    "# Configuration\n",
    "# -----------------------------\n",
    "n_repeats = 4\n",
    "seeds = list(range(1, 11))\n",
    "imputations = 5\n",
    "output_folder = \"outputs\"\n",
    "\n",
    "# -----------------------------\n",
    "# Plotting Functions\n",
    "# -----------------------------\n",
    "def create_diagnostic_plots(residuals_data, group_name, output_folder):\n",
    "    \"\"\"Create diagnostic plots for each group\"\"\"\n",
    "    plots_dir = os.path.join(output_folder, \"plots\")\n",
    "    os.makedirs(plots_dir, exist_ok=True)\n",
    "    \n",
    "    all_residuals = residuals_data['residuals']\n",
    "    all_fitted = residuals_data['fitted']\n",
    "    \n",
    "    if len(all_residuals) == 0:\n",
    "        return\n",
    "    \n",
    "    # Create figure with subplots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    fig.suptitle(f'Diagnostic Plots - {group_name}', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # 1. Residuals vs Fitted\n",
    "    axes[0, 0].scatter(all_fitted, all_residuals, alpha=0.5, s=1)\n",
    "    axes[0, 0].axhline(y=0, color='red', linestyle='--')\n",
    "    axes[0, 0].set_xlabel('Fitted Values')\n",
    "    axes[0, 0].set_ylabel('Residuals')\n",
    "    axes[0, 0].set_title('Residuals vs Fitted')\n",
    "    \n",
    "    # 2. QQ Plot\n",
    "    probplot(all_residuals, dist=\"norm\", plot=axes[0, 1])\n",
    "    axes[0, 1].set_title('QQ Plot (Normal)')\n",
    "    \n",
    "    # 3. Histogram of Residuals\n",
    "    axes[1, 0].hist(all_residuals, bins=50, alpha=0.7, edgecolor='black')\n",
    "    axes[1, 0].axvline(x=0, color='red', linestyle='--')\n",
    "    axes[1, 0].set_xlabel('Residuals')\n",
    "    axes[1, 0].set_ylabel('Frequency')\n",
    "    axes[1, 0].set_title('Residual Distribution')\n",
    "    \n",
    "    # 4. Scale-Location Plot\n",
    "    sqrt_abs_resid = np.sqrt(np.abs(all_residuals))\n",
    "    axes[1, 1].scatter(all_fitted, sqrt_abs_resid, alpha=0.5, s=1)\n",
    "    axes[1, 1].set_xlabel('Fitted Values')\n",
    "    axes[1, 1].set_ylabel('√|Residuals|')\n",
    "    axes[1, 1].set_title('Scale-Location Plot')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(plots_dir, f'{group_name}_unweighted.png'), \n",
    "                dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "# -----------------------------\n",
    "# Rubin's Rule\n",
    "# -----------------------------\n",
    "def rubins_pool(estimates, ses):\n",
    "    m = len(estimates)\n",
    "    q_bar = np.mean(estimates)\n",
    "    u_bar = np.mean(np.square(ses))\n",
    "    b_m = np.var(estimates, ddof=1)\n",
    "    total_var = u_bar + ((1 + 1/m) * b_m)\n",
    "    total_se = np.sqrt(total_var)\n",
    "    ci_lower = q_bar - 1.96 * total_se\n",
    "    ci_upper = q_bar + 1.96 * total_se\n",
    "    p_value = 2 * (1 - t.cdf(np.abs(q_bar / total_se), df=m-1))\n",
    "    rounded_p = round(p_value, 5)\n",
    "    formatted_p = \"< 0.00001\" if rounded_p <= 0.00001 else f\"{rounded_p:.5f}\"\n",
    "    return q_bar, total_se, ci_lower, ci_upper, formatted_p\n",
    "\n",
    "# -----------------------------\n",
    "# SMD + Variance Ratio\n",
    "# -----------------------------\n",
    "def calculate_smd_vr(X, T):\n",
    "    treated = X[T == 1]\n",
    "    control = X[T == 0]\n",
    "    smd, vr = [], []\n",
    "    for col in X.columns:\n",
    "        try:\n",
    "            m1, m0 = treated[col].mean(), control[col].mean()\n",
    "            s1, s0 = treated[col].std(), control[col].std()\n",
    "            pooled_sd = np.sqrt((s1 ** 2 + s0 ** 2) / 2)\n",
    "            smd.append((m1 - m0) / pooled_sd if pooled_sd > 0 else 0)\n",
    "            vr.append(s1**2 / s0**2 if s0**2 > 0 else 0)\n",
    "        except Exception:\n",
    "            smd.append(np.nan)\n",
    "            vr.append(np.nan)\n",
    "    return np.nanmean(smd), np.nanmean(vr)\n",
    "\n",
    "# -----------------------------\n",
    "# DML Main Loop\n",
    "# -----------------------------\n",
    "def run_dml_with_trimmed_data(final_covariates_map):\n",
    "    att_results = []\n",
    "    balance_results = []\n",
    "\n",
    "    for group, covariates in final_covariates_map.items():\n",
    "        print(f\"\\n🚀 Running DML for {group}\")\n",
    "        group_dir = os.path.join(output_folder, group)\n",
    "        os.makedirs(group_dir, exist_ok=True)\n",
    "\n",
    "        best_result = None\n",
    "        best_se = float(\"inf\")\n",
    "        \n",
    "        # Initialize residuals collection for this group\n",
    "        group_residuals_data = {'residuals': [], 'fitted': []}\n",
    "\n",
    "        for seed in seeds:\n",
    "            att_list, se_list, r2_list, rmse_list, smd_list, vr_list = [], [], [], [], [], []\n",
    "\n",
    "            for imp in range(1, imputations + 1):\n",
    "                file_path = os.path.join(group_dir, f\"trimmed_data_imp{imp}.pkl\")\n",
    "                if not os.path.exists(file_path):\n",
    "                    continue\n",
    "\n",
    "                df = pd.read_pickle(file_path)\n",
    "                if group not in df.columns:\n",
    "                    continue\n",
    "\n",
    "                X = df[covariates].copy()\n",
    "                T = df[group]\n",
    "                Y = df[\"caps5_change_baseline\"]\n",
    "\n",
    "                for repeat in range(n_repeats):\n",
    "                    kf = KFold(n_splits=5, shuffle=True, random_state=seed + repeat)\n",
    "                    for train_idx, test_idx in kf.split(X):\n",
    "                        try:\n",
    "                            X_train, T_train, Y_train = (\n",
    "                                X.iloc[train_idx],\n",
    "                                T.iloc[train_idx],\n",
    "                                Y.iloc[train_idx],\n",
    "                            )\n",
    "\n",
    "                            model_y = xgb.XGBRegressor(n_estimators=100, max_depth=3, learning_rate=0.1, n_jobs=1, random_state=seed)\n",
    "                            model_t = xgb.XGBClassifier(n_estimators=100, max_depth=3, learning_rate=0.1, n_jobs=1,\n",
    "                                                        use_label_encoder=False, eval_metric=\"logloss\", random_state=seed)\n",
    "\n",
    "                            dml = LinearDML(model_y=model_y, model_t=model_t, discrete_treatment=True,\n",
    "                                            cv=KFold(n_splits=3, shuffle=True, random_state=seed), random_state=seed)\n",
    "                            dml.fit(Y_train, T_train, X=X_train)\n",
    "\n",
    "                            tau = dml.effect(X_train)\n",
    "                            att = np.mean(tau)\n",
    "                            influence = tau - att\n",
    "                            se = np.sqrt(np.mean(influence ** 2) / len(tau))\n",
    "\n",
    "                            att_list.append(att)\n",
    "                            se_list.append(se)\n",
    "\n",
    "                            Y_pred = model_y.fit(X_train, Y_train).predict(X_train)\n",
    "                            residuals = Y_train - Y_pred\n",
    "                            \n",
    "                            # Collect residuals and fitted values for plotting\n",
    "                            group_residuals_data['residuals'].extend(residuals.tolist())\n",
    "                            group_residuals_data['fitted'].extend(Y_pred.tolist())\n",
    "                            \n",
    "                            rmse = mean_squared_error(Y_train, Y_pred, squared=False)\n",
    "                            r2 = r2_score(Y_train, Y_pred)\n",
    "                            r2_list.append(r2)\n",
    "                            rmse_list.append(rmse)\n",
    "\n",
    "                            smd, vr = calculate_smd_vr(X_train, T_train)\n",
    "                            smd_list.append(smd)\n",
    "                            vr_list.append(vr)\n",
    "                        except Exception as e:\n",
    "                            print(f\"⚠️ Error in {group}, seed {seed}, imp {imp}, rep {repeat}: {e}\")\n",
    "\n",
    "            if att_list:\n",
    "                att, se, ci_l, ci_u, p_val = rubins_pool(att_list, se_list)\n",
    "                avg_r2 = np.mean(r2_list)\n",
    "                avg_rmse = np.mean(rmse_list)\n",
    "                avg_smd = np.mean(smd_list)\n",
    "                avg_vr = np.mean(vr_list)\n",
    "\n",
    "                balance_results.append({\n",
    "                    \"group\": group, \"seed\": seed, \"smd\": avg_smd, \"vr\": avg_vr\n",
    "                })\n",
    "\n",
    "                if se < best_se:\n",
    "                    best_se = se\n",
    "                    best_result = {\n",
    "                        \"group\": group, \"seed\": seed, \"att\": att, \"se\": se,\n",
    "                        \"ci_lower\": ci_l, \"ci_upper\": ci_u, \"p_value\": p_val,\n",
    "                        \"r2\": avg_r2, \"rmse\": avg_rmse\n",
    "                    }\n",
    "\n",
    "                print(f\"✅ {group} | Seed {seed}: ATT = {att:.4f}, SE = {se:.4f}, p = {p_val}\")\n",
    "            else:\n",
    "                print(f\"⚠️ No valid results for {group} | Seed {seed}\")\n",
    "\n",
    "        if best_result:\n",
    "            att_results.append(best_result)\n",
    "            print(f\"🏆 Best result for {group} → Seed {best_result['seed']} | SE = {best_result['se']:.4f}\")\n",
    "        \n",
    "        # Create diagnostic plots for this group\n",
    "        print(f\"📊 Creating diagnostic plots for {group}...\")\n",
    "        create_diagnostic_plots(group_residuals_data, group, output_folder)\n",
    "\n",
    "    # Save final output\n",
    "    pd.DataFrame(att_results).to_excel(\"dml_rubin_summary_cats_unweighted.xlsx\", index=False)\n",
    "    pd.DataFrame(balance_results).to_excel(\"smd_vr_summary_cats_unweighted.xlsx\", index=False)\n",
    "    print(\"\\n🎯 All summary files saved.\")\n",
    "    print(\"📊 All diagnostic plots saved in outputs/plots/ folder.\")\n",
    "\n",
    "run_dml_with_trimmed_data(final_covariates_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6cc497-ddbb-4668-86ee-8e5a2872b2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import sem, ttest_ind\n",
    "\n",
    "# ----------------------------------\n",
    "# File paths\n",
    "# ----------------------------------\n",
    "output_base = \"outputs\"\n",
    "att_file = \"dml_rubin_summary_cats.xlsx\"\n",
    "trimmed_file = \"trimmed_data_imp1.pkl\"\n",
    "auc_file = \"auc_scores.xlsx\"  # NEW\n",
    "\n",
    "# ----------------------------------\n",
    "# Load ATT Summary\n",
    "# ----------------------------------\n",
    "if os.path.exists(att_file):\n",
    "    att_df = pd.read_excel(att_file)\n",
    "else:\n",
    "    raise FileNotFoundError(\"❌ ATT summary file not found: dml_rubin_summary_cats.xlsx\")\n",
    "\n",
    "summary_rows = []\n",
    "\n",
    "# ----------------------------------\n",
    "# Loop over medication groups\n",
    "# ----------------------------------\n",
    "groups = [g for g in medication_groups if os.path.isdir(os.path.join(output_base, g))]\n",
    "\n",
    "for med in groups:\n",
    "    try:\n",
    "        group_path = os.path.join(output_base, med)\n",
    "\n",
    "        # Load trimmed data\n",
    "        df = pd.read_pickle(os.path.join(group_path, trimmed_file))\n",
    "\n",
    "        # Detect treatment column\n",
    "        treatment_cols = [col for col in df.columns if col.upper() == med.upper()]\n",
    "        if not treatment_cols:\n",
    "            print(f\"⚠️ Treatment column {med} not found in trimmed data. Skipping.\")\n",
    "            continue\n",
    "        treatment_var = treatment_cols[0]\n",
    "\n",
    "        # Extract treatment and outcome\n",
    "        T = df[treatment_var]\n",
    "        Y = df[\"caps5_change_baseline\"]\n",
    "\n",
    "        # Treated and control stats\n",
    "        treated = Y[T == 1]\n",
    "        control = Y[T == 0]\n",
    "\n",
    "        mean_treat = treated.mean()\n",
    "        se_treat = sem(treated) if len(treated) > 1 else np.nan\n",
    "\n",
    "        mean_ctrl = control.mean()\n",
    "        se_ctrl = sem(control) if len(control) > 1 else np.nan\n",
    "\n",
    "        # Cohen's d (unadjusted)\n",
    "        pooled_sd = np.sqrt(((treated.std() ** 2) + (control.std() ** 2)) / 2)\n",
    "        cohen_d = (mean_treat - mean_ctrl) / pooled_sd if pooled_sd > 0 else np.nan\n",
    "\n",
    "        # E-value (unadjusted)\n",
    "        delta = mean_treat - mean_ctrl\n",
    "        E = delta / abs(mean_ctrl) * 100 if mean_ctrl != 0 else np.nan\n",
    "\n",
    "        # Unadjusted p-value\n",
    "        try:\n",
    "            t_stat, p_val = ttest_ind(treated, control, equal_var=False, nan_policy=\"omit\")\n",
    "            rounded_p = round(p_val, 5)\n",
    "            formatted_p = \"< 0.00001\" if rounded_p < 0.00001 else rounded_p\n",
    "        except Exception:\n",
    "            formatted_p = np.nan\n",
    "\n",
    "        # AUC from new auc_scores.xlsx file\n",
    "        auc_val = np.nan\n",
    "        auc_path = os.path.join(group_path, auc_file)\n",
    "        if os.path.exists(auc_path):\n",
    "            auc_df = pd.read_excel(auc_path)\n",
    "            if \"AUC\" in auc_df.columns:\n",
    "                auc_val = auc_df[\"AUC\"].dropna().mean()\n",
    "\n",
    "        # Adjusted stats from Rubin summary\n",
    "        att_row = att_df[att_df[\"group\"].str.strip().str.upper() == med.strip().upper()]\n",
    "        if not att_row.empty:\n",
    "            att = att_row.iloc[0][\"att\"]\n",
    "            att_se = att_row.iloc[0][\"se\"]\n",
    "            att_p_val = att_row.iloc[0][\"p_value\"]\n",
    "            r2 = att_row.iloc[0][\"r2\"]\n",
    "            rmse = att_row.iloc[0][\"rmse\"]\n",
    "\n",
    "            try:\n",
    "                rounded_att_p = round(float(att_p_val), 5)\n",
    "                formatted_att_p = \"< 0.00001\" if rounded_att_p < 0.00001 else rounded_att_p\n",
    "            except:\n",
    "                formatted_att_p = att_p_val\n",
    "        else:\n",
    "            att, att_se, formatted_att_p, r2, rmse = np.nan, np.nan, np.nan, np.nan, np.nan\n",
    "\n",
    "        # Append full row\n",
    "        summary_rows.append({\n",
    "            'Medication Group': med,\n",
    "            'Mean Treated': mean_treat,\n",
    "            'SE Treated': se_treat,\n",
    "            'Mean Control': mean_ctrl,\n",
    "            'SE Control': se_ctrl,\n",
    "            'Cohen d': cohen_d,\n",
    "            'E (Unadjusted)': E,\n",
    "            'n Treated': len(treated),\n",
    "            'n Control': len(control),\n",
    "            #'Unadjusted p-value': formatted_p,\n",
    "            'ATT Estimate': att,\n",
    "            'ATT SE (Robust)': att_se,\n",
    "            'ATT p-value': formatted_att_p,\n",
    "            'R²': r2,\n",
    "            'RMSE': rmse,\n",
    "            'AUC': auc_val\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error in {med}: {e}\")\n",
    "\n",
    "# ----------------------------------\n",
    "# Save final summary\n",
    "# ----------------------------------\n",
    "summary_df = pd.DataFrame(summary_rows)\n",
    "summary_df = summary_df.sort_values(\"Medication Group\")\n",
    "summary_df.to_excel(\"Final_ATT_Summary_Cat.xlsx\", index=False)\n",
    "print(\"✅ Final_ATT_Summary_Cat saved.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e88cd42-b49e-4ff7-ac7d-c1e6be654c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "# ✅ Load the final summary table\n",
    "final_df = pd.read_excel(\"Final_ATT_Summary_Cat.xlsx\")\n",
    "\n",
    "# ✅ Parse DML p-values (handle \"< 0.00001\")\n",
    "def parse_pval(p):\n",
    "    try:\n",
    "        if isinstance(p, str) and \"<\" in p:\n",
    "            return 0.000001\n",
    "        return float(p)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "final_df['ATT p-value'] = final_df['ATT p-value'].apply(parse_pval)\n",
    "\n",
    "# ✅ Plot settings\n",
    "width = 0.35\n",
    "\n",
    "# ✅ Plotting function for a single medication group\n",
    "def plot_single_group(row):\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    bars1 = ax.bar(-width/2, row['Mean Control'], width, \n",
    "                   yerr=row['SE Control'], label='Control', hatch='//', color='gray', capsize=5)\n",
    "    bars2 = ax.bar(+width/2, row['Mean Treated'], width, \n",
    "                   yerr=row['SE Treated'], label='Treated', color='steelblue', capsize=5)\n",
    "\n",
    "    label = (\n",
    "        f\"ATT = {row['ATT Estimate']:.2f}\\n\"\n",
    "        f\"d = {row['Cohen d']:.2f}, p = {row['ATT p-value']:.3f}\\n\"\n",
    "        f\"nT = {row['n Treated']}, nC = {row['n Control']}\\n\"\n",
    "        f\"E = {row['E (Unadjusted)']:.1f}%\"\n",
    "    )\n",
    "    max_y = max(row['Mean Control'], row['Mean Treated']) + 1.5\n",
    "    ax.text(0, max_y, label, ha='center', va='bottom', fontsize=9, color='#FFD700')\n",
    "\n",
    "    ax.axhline(0, color='black', linewidth=0.8)\n",
    "    ax.set_xticks([-width/2, +width/2])\n",
    "    ax.set_xticklabels(['Control', 'Treated'])\n",
    "    ax.set_title(f\"Group: {row['Medication Group']}\", fontsize=12, weight='bold')\n",
    "    ax.set_ylabel(\"CAPS5 Change Score\")\n",
    "    ax.set_ylim(bottom=0, top=max_y + 2)\n",
    "    ax.legend()\n",
    "    fig.tight_layout()\n",
    "    return fig\n",
    "\n",
    "# ✅ Generate and save all plots into a multi-page PDF\n",
    "with PdfPages(\"dml_att_barplot_cat.pdf\") as pdf:\n",
    "    for idx, row in final_df.iterrows():\n",
    "        fig = plot_single_group(row)\n",
    "        pdf.savefig(fig, dpi=300, bbox_inches='tight')\n",
    "        plt.close(fig)\n",
    "\n",
    "print(\"✅ dml_att_barplot_cat saved.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7f3256-bb44-495c-a2fb-023d82a82611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Love plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fecd847-e224-4613-b4e9-5de8a3e77d73",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "# ----------------------------------------\n",
    "# Functions to calculate balance\n",
    "# ----------------------------------------\n",
    "def calculate_smd(x1, x2, w1=None, w2=None):\n",
    "    def weighted_mean(x, w): return np.average(x, weights=w)\n",
    "    def weighted_var(x, w):\n",
    "        m = np.average(x, weights=w)\n",
    "        return np.average((x - m) ** 2, weights=w)\n",
    "    \n",
    "    m1 = weighted_mean(x1, w1) if w1 is not None else np.mean(x1)\n",
    "    m2 = weighted_mean(x2, w2) if w2 is not None else np.mean(x2)\n",
    "    v1 = weighted_var(x1, w1) if w1 is not None else np.var(x1, ddof=1)\n",
    "    v2 = weighted_var(x2, w2) if w2 is not None else np.var(x2, ddof=1)\n",
    "    \n",
    "    pooled_sd = np.sqrt((v1 + v2) / 2)\n",
    "    return np.abs(m1 - m2) / pooled_sd if pooled_sd > 0 else 0\n",
    "\n",
    "def variance_ratio(x1, x2, w1=None, w2=None):\n",
    "    def weighted_var(x, w):\n",
    "        m = np.average(x, weights=w)\n",
    "        return np.average((x - m) ** 2, weights=w)\n",
    "    \n",
    "    v1 = weighted_var(x1, w1) if w1 is not None else np.var(x1, ddof=1)\n",
    "    v2 = weighted_var(x2, w2) if w2 is not None else np.var(x2, ddof=1)\n",
    "    \n",
    "    return max(v1 / v2, v2 / v1) if v1 > 0 and v2 > 0 else 1\n",
    "\n",
    "# ----------------------------------------\n",
    "# Setup\n",
    "# ----------------------------------------\n",
    "output_base = \"outputs\"\n",
    "groups = [g for g in os.listdir(output_base) if os.path.isdir(os.path.join(output_base, g))]\n",
    "\n",
    "# Create a case-insensitive mapping\n",
    "final_covariates_map_lower = {k.lower(): v for k, v in final_covariates_map.items()}\n",
    "\n",
    "# ----------------------------------------\n",
    "# Main Loop\n",
    "# ----------------------------------------\n",
    "for group in groups:\n",
    "    if group.lower() not in final_covariates_map_lower:\n",
    "        continue\n",
    "\n",
    "    print(f\"\\n🔍 Processing {group}...\")\n",
    "\n",
    "    try:\n",
    "        group_path = os.path.join(output_base, group)\n",
    "        covariates = final_covariates_map_lower[group.lower()]\n",
    "        \n",
    "        column_name = None\n",
    "        for col in pd.read_pickle(os.path.join(group_path, \"trimmed_data_imp1.pkl\")).columns:\n",
    "            if col.lower() == group.lower():\n",
    "                column_name = col\n",
    "                break\n",
    "        if column_name is None:\n",
    "            print(f\"⚠️ Column not found for {group}, skipping.\")\n",
    "            continue\n",
    "\n",
    "        smd_unw_all, smd_w_all = [], []\n",
    "        vr_unw_all, vr_w_all = [], []\n",
    "\n",
    "        for i in range(1, 6):\n",
    "            df_path = os.path.join(group_path, f\"trimmed_data_imp{i}.pkl\")\n",
    "            iptw_path = os.path.join(group_path, \"iptw_weights.xlsx\")\n",
    "\n",
    "            if not os.path.exists(df_path) or not os.path.exists(iptw_path):\n",
    "                print(f\"⚠️ Missing data for {group} imp{i}, skipping.\")\n",
    "                continue\n",
    "\n",
    "            df = pd.read_pickle(df_path)\n",
    "            iptw_df = pd.read_excel(iptw_path, index_col=0)\n",
    "            T = df[column_name]\n",
    "            W = iptw_df.loc[df.index, \"iptw_mean\"]\n",
    "\n",
    "            smd_unw_i, smd_w_i, vr_unw_i, vr_w_i = [], [], [], []\n",
    "\n",
    "            for cov in covariates:\n",
    "                x1, x0 = df.loc[T == 1, cov], df.loc[T == 0, cov]\n",
    "                w1, w0 = W[T == 1], W[T == 0]\n",
    "\n",
    "                su = calculate_smd(x1, x0)\n",
    "                sw = calculate_smd(x1, x0, w1, w0)\n",
    "\n",
    "                vu = variance_ratio(x1, x0) if cov in ['treatmentdurationdays', 'CAPS5score_baseline', 'age'] else np.nan\n",
    "                vw = variance_ratio(x1, x0, w1, w0) if cov in ['treatmentdurationdays', 'CAPS5score_baseline', 'age'] else np.nan\n",
    "\n",
    "                smd_unw_i.append(su)\n",
    "                smd_w_i.append(sw)\n",
    "                vr_unw_i.append(vu)\n",
    "                vr_w_i.append(vw)\n",
    "\n",
    "            smd_unw_all.append(smd_unw_i)\n",
    "            smd_w_all.append(smd_w_i)\n",
    "            vr_unw_all.append(vr_unw_i)\n",
    "            vr_w_all.append(vr_w_i)\n",
    "\n",
    "        smd_unw = np.mean(smd_unw_all, axis=0)\n",
    "        smd_w = np.mean(smd_w_all, axis=0)\n",
    "        vr_unw = np.nanmean(vr_unw_all, axis=0)\n",
    "        vr_w = np.nanmean(vr_w_all, axis=0)\n",
    "\n",
    "        severity = []\n",
    "        for sw in smd_w:\n",
    "            if sw <= 0.1:\n",
    "                severity.append(\"Good\")\n",
    "            elif sw <= 0.2:\n",
    "                severity.append(\"Moderate\")\n",
    "            else:\n",
    "                severity.append(\"Poor\")\n",
    "\n",
    "        covariate_names = covariates\n",
    "        numeric_df = pd.DataFrame({\n",
    "            \"Covariate\": covariate_names,\n",
    "            \"SMD_Unweighted\": smd_unw,\n",
    "            \"SMD_Weighted\": smd_w,\n",
    "            \"Imbalance_Severity\": severity,\n",
    "            \"VR_Unweighted\": vr_unw,\n",
    "            \"VR_Weighted\": vr_w\n",
    "        })\n",
    "\n",
    "        numeric_path = os.path.join(group_path, f\"covariate_balance_table_{group}.xlsx\")\n",
    "        numeric_df.to_excel(numeric_path, index=False)\n",
    "        print(f\"📊 Exported numeric summary to: {numeric_path}\")\n",
    "\n",
    "        # -------------------------\n",
    "        # Plot\n",
    "        # -------------------------\n",
    "        labels = covariates\n",
    "        y_pos = np.arange(len(labels))\n",
    "\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(18, len(labels) * 0.45))\n",
    "\n",
    "        axes[0].scatter(smd_unw, y_pos, color='red', label=\"Unweighted\")\n",
    "        axes[0].scatter(smd_w, y_pos, color='blue', label=\"Weighted\")\n",
    "        axes[0].axvline(0.1, color='gray', linestyle='--', label=\"Threshold 0.1\")\n",
    "        axes[0].axvline(0.2, color='black', linestyle='--', label=\"Threshold 0.2\")\n",
    "        axes[0].set_xlim(0, max(max(smd_unw), max(smd_w), 0.25) + 0.05)\n",
    "        axes[0].set_yticks(y_pos)\n",
    "        axes[0].set_yticklabels(labels)\n",
    "        axes[0].invert_yaxis()\n",
    "        axes[0].set_title(\"Standardized Mean Differences (SMD)\")\n",
    "        axes[0].legend(loc=\"upper right\")\n",
    "        axes[0].grid(True)\n",
    "\n",
    "        vr_mask = [cov in ['treatmentdurationdays', 'CAPS5score_baseline', 'age'] for cov in covariates]\n",
    "        filtered_y = [i for i, b in enumerate(vr_mask) if b]\n",
    "        filtered_labels = [labels[i] for i in filtered_y]\n",
    "        filtered_vr_unw = [vr_unw[i] for i in filtered_y]\n",
    "        filtered_vr_w = [vr_w[i] for i in filtered_y]\n",
    "\n",
    "        axes[1].scatter(filtered_vr_unw, filtered_y, color='blue', marker='o', label=\"Unweighted\")\n",
    "        axes[1].scatter(filtered_vr_w, filtered_y, color='red', marker='x', label=\"Weighted\")\n",
    "        axes[1].axvline(2, color='gray', linestyle='--')\n",
    "        axes[1].axvline(0.5, color='gray', linestyle='--')\n",
    "        axes[1].set_xlim(0, max(filtered_vr_unw + filtered_vr_w + [2.5]) + 0.5)\n",
    "        axes[1].set_yticks(filtered_y)\n",
    "        axes[1].set_yticklabels(filtered_labels)\n",
    "        axes[1].invert_yaxis()\n",
    "        axes[1].set_title(\"Variance Ratio (VR)\")\n",
    "        axes[1].legend()\n",
    "        axes[1].grid(True)\n",
    "\n",
    "        fig.suptitle(f\"Covariate Balance for {group.replace('CAT_', '')}\", fontsize=14, weight='bold')\n",
    "        fig.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "        plot_path = os.path.join(group_path, f\"love_plot_{group}.pdf\")\n",
    "        fig.savefig(plot_path, dpi=300)\n",
    "        plt.close()\n",
    "        print(f\"✅ Saved love plot: {plot_path}\")\n",
    "        print(f\"📏 Max weighted SMD for {group}: {np.max(smd_w):.3f}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error in {group}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a3ba94-b9c5-49bf-8daa-ecee013104ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9d0b67-ec65-431f-a804-f0af2bc85c96",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "#-----------------------------\n",
    "# Generate heatmaps\n",
    "# -------------------------------\n",
    "for treatment_var in medication_groups:\n",
    "    print(f\"\\n========== Creating Heatmap for {treatment_var} ==========\")\n",
    "\n",
    "    try:\n",
    "        output_folder = os.path.join('outputs', treatment_var)\n",
    "        balance_path = os.path.join(output_folder, f'covariate_balance_table_{treatment_var}.xlsx')\n",
    "\n",
    "        if not os.path.exists(balance_path):\n",
    "            print(f\"❌ Balance file not found: {balance_path}\")\n",
    "            continue\n",
    "\n",
    "        balance_df = pd.read_excel(balance_path)\n",
    "\n",
    "        # ✅ Use finalized covariates + 'Propensity Score'\n",
    "        covariates = final_covariates_map[treatment_var] + ['Propensity Score']\n",
    "        balance_df = balance_df[balance_df['Covariate'].isin(covariates)]\n",
    "\n",
    "        # ✅ Check for CAPS5score_baseline\n",
    "        highlight_caps = 'CAPS5score_baseline' in balance_df['Covariate'].values\n",
    "\n",
    "        # ✅ Format for heatmap\n",
    "        heatmap_df = balance_df[['Covariate', 'SMD_Unweighted', 'SMD_Weighted']].copy()\n",
    "        heatmap_df.columns = ['Covariate', 'Unweighted', 'Weighted']\n",
    "        heatmap_df = heatmap_df.set_index('Covariate')\n",
    "        heatmap_df = heatmap_df.sort_values(by='Unweighted', ascending=False)\n",
    "\n",
    "        # ✅ Plot\n",
    "        plt.figure(figsize=(12, max(10, len(heatmap_df) * 0.35)))\n",
    "        ax = sns.heatmap(\n",
    "            heatmap_df,\n",
    "            cmap=\"coolwarm\",\n",
    "            annot=True,\n",
    "            fmt=\".2f\",\n",
    "            linewidths=0.6,\n",
    "            linecolor='gray',\n",
    "            cbar_kws={\"label\": \"Standardized Mean Difference\"}\n",
    "        )\n",
    "\n",
    "        plt.title(f\"Covariate Balance Heatmap (Rubin IPTW)\\n{treatment_var}\", fontsize=15, weight='bold')\n",
    "        plt.xlabel(\"Condition\")\n",
    "        plt.ylabel(\"Covariate\")\n",
    "\n",
    "        # ✅ Bold CAPS5score_baseline if present\n",
    "        if highlight_caps:\n",
    "            ylabels = [label.get_text() for label in ax.get_yticklabels()]\n",
    "            ax.set_yticklabels([\n",
    "                f\"{label} ←\" if label == 'CAPS5score_baseline' else label for label in ylabels\n",
    "            ])\n",
    "\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # ✅ Save image\n",
    "        save_path = os.path.join(output_folder, f'heatmap_smd_{treatment_var}.png')\n",
    "        plt.savefig(save_path, dpi=300)\n",
    "        plt.close()\n",
    "\n",
    "        print(f\"✅ Heatmap saved: {save_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error processing {treatment_var}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05acf0f-8311-4c39-b99e-780b7f48d835",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f68fb1c3-430a-4ff8-893d-7f2a9f3c2c2c",
   "metadata": {},
   "source": [
    "## Subcat analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a961590-c301-4cfd-a108-fd1559a81ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "covariates_SUBCAT_Antipsychotica_atypisch = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline',\n",
    "    'CAT_Antidepressiva', 'CAT_Antipsychotica',\n",
    "    'DIAGNOSIS_CHILDHOOD_TRAUMA', 'DIAGNOSIS_CPTSD', 'DIAGNOSIS_SEXUAL_TRAUMA',\n",
    "    'DIAGNOSIS_SUICIDALITY', 'age',\n",
    "    'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"Bipolar_and_Mood_disorder\", \"ethnicity_Dutch\", \"ethnicity_other\"\n",
    "]\n",
    "\n",
    "covariates_SUBCAT_TCA = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline',\n",
    "    'CAT_Antipsychotica',\n",
    "    'DIAGNOSIS_CHILDHOOD_TRAUMA', 'DIAGNOSIS_CPTSD', 'DIAGNOSIS_SEXUAL_TRAUMA',\n",
    "    'DIAGNOSIS_SUICIDALITY', 'age',\n",
    "    'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"Bipolar_and_Mood_disorder\", \"ethnicity_Dutch\", \"ethnicity_other\"\n",
    "]\n",
    "\n",
    "covariates_SUBCAT_SSRI = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline',\n",
    "    'CAT_Antipsychotica',\n",
    "    'DIAGNOSIS_CHILDHOOD_TRAUMA', 'DIAGNOSIS_CPTSD', 'DIAGNOSIS_SEXUAL_TRAUMA',\n",
    "    'DIAGNOSIS_SUICIDALITY', 'age',\n",
    "    'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"Bipolar_and_Mood_disorder\", \"ethnicity_Dutch\", \"ethnicity_other\"\n",
    "]\n",
    "\n",
    "covariates_SUBCAT_SNRI = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline',\n",
    "    'CAT_Antipsychotica',\n",
    "    'DIAGNOSIS_CHILDHOOD_TRAUMA', 'DIAGNOSIS_CPTSD', 'DIAGNOSIS_SEXUAL_TRAUMA',\n",
    "    'DIAGNOSIS_SUICIDALITY', 'age',\n",
    "    'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"Bipolar_and_Mood_disorder\", \"ethnicity_Dutch\", \"ethnicity_other\"\n",
    "]\n",
    "\n",
    "\n",
    "covariates_SUBCAT_Tetracyclische_antidepressiva = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline',\n",
    "    'CAT_Antipsychotica',\n",
    "    'DIAGNOSIS_CHILDHOOD_TRAUMA', 'DIAGNOSIS_CPTSD', 'DIAGNOSIS_SEXUAL_TRAUMA',\n",
    "    'DIAGNOSIS_SUICIDALITY', 'age',\n",
    "    'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"Bipolar_and_Mood_disorder\", \"ethnicity_Dutch\", \"ethnicity_other\"\n",
    "]\n",
    "\n",
    "covariates_SUBCAT_Antidepressiva_overige = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline',\n",
    "    'CAT_Antipsychotica',\n",
    "    'DIAGNOSIS_CHILDHOOD_TRAUMA', 'DIAGNOSIS_CPTSD', 'DIAGNOSIS_SEXUAL_TRAUMA',\n",
    "    'DIAGNOSIS_SUICIDALITY', 'age',\n",
    "    'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"Bipolar_and_Mood_disorder\", \"ethnicity_Dutch\", \"ethnicity_other\"\n",
    "]\n",
    "\n",
    "covariates_SUBCAT_Systemische_antihistaminica = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline',\n",
    "    'CAT_Antidepressiva', 'CAT_Antipsychotica',\n",
    "    'DIAGNOSIS_CHILDHOOD_TRAUMA', 'DIAGNOSIS_CPTSD', 'DIAGNOSIS_SEXUAL_TRAUMA',\n",
    "    'DIAGNOSIS_SUICIDALITY', 'age',\n",
    "    'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"Bipolar_and_Mood_disorder\", \"ethnicity_Dutch\", \"ethnicity_other\"\n",
    "]\n",
    "\n",
    "\n",
    "covariates_SUBCAT_anxiolytica_Benzodiazepine = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline',\n",
    "    'CAT_Antidepressiva',\n",
    "    'CAT_Antipsychotica',\n",
    "    'DIAGNOSIS_CHILDHOOD_TRAUMA', 'DIAGNOSIS_CPTSD', 'DIAGNOSIS_SEXUAL_TRAUMA',\n",
    "    'DIAGNOSIS_SUICIDALITY', 'age',\n",
    "    'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"Bipolar_and_Mood_disorder\", \"ethnicity_Dutch\", \"ethnicity_other\"\n",
    "]\n",
    "\n",
    "covariates_SUBCAT_hypnotica_Benzodiazepine = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline',\n",
    "    'CAT_Antidepressiva',\n",
    "    'CAT_Antipsychotica',\n",
    "    'DIAGNOSIS_CHILDHOOD_TRAUMA', 'DIAGNOSIS_CPTSD', 'DIAGNOSIS_SEXUAL_TRAUMA',\n",
    "    'DIAGNOSIS_SUICIDALITY', 'age',\n",
    "    'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"Bipolar_and_Mood_disorder\", \"ethnicity_Dutch\", \"ethnicity_other\"\n",
    "]\n",
    "\n",
    "\n",
    "covariates_SUBCAT_Amfetaminen = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline', 'CAT_Antidepressiva', 'CAT_Antipsychotica',\n",
    "    'DIAGNOSIS_CHILDHOOD_TRAUMA', 'DIAGNOSIS_CPTSD',\n",
    "    'DIAGNOSIS_SEXUAL_TRAUMA', 'DIAGNOSIS_SUICIDALITY',\n",
    "    'age',\n",
    "    'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"Bipolar_and_Mood_disorder\", \"ethnicity_Dutch\", \"ethnicity_other\"\n",
    "]\n",
    "\n",
    "\n",
    "covariates_SUBCAT_Systemische_betablokkers = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline',\n",
    "    'CAT_Antidepressiva', 'CAT_Antipsychotica',\n",
    "    'DIAGNOSIS_CHILDHOOD_TRAUMA', 'DIAGNOSIS_CPTSD', 'DIAGNOSIS_SEXUAL_TRAUMA',\n",
    "    'DIAGNOSIS_SUICIDALITY', 'age',\n",
    "    'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"Bipolar_and_Mood_disorder\", \"ethnicity_Dutch\", \"ethnicity_other\"\n",
    "]\n",
    "\n",
    "\n",
    "covariates_SUBCAT_Paracetamol_mono = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline',\n",
    "    'CAT_Antidepressiva', 'CAT_Antipsychotica',\n",
    "    'DIAGNOSIS_CHILDHOOD_TRAUMA', 'DIAGNOSIS_CPTSD', 'DIAGNOSIS_SEXUAL_TRAUMA',\n",
    "    'DIAGNOSIS_SUICIDALITY', 'age',\n",
    "    'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"Bipolar_and_Mood_disorder\", \"ethnicity_Dutch\", \"ethnicity_other\"\n",
    "]\n",
    "\n",
    "\n",
    "covariates_SUBCAT_Anti_epileptica_stemmingsstabilisatoren = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline',\n",
    "    'CAT_Antidepressiva',\n",
    "    'CAT_Antipsychotica',\n",
    "    'DIAGNOSIS_CHILDHOOD_TRAUMA', 'DIAGNOSIS_CPTSD', 'DIAGNOSIS_SEXUAL_TRAUMA',\n",
    "    'DIAGNOSIS_SUICIDALITY', 'age', \n",
    "    'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"Bipolar_and_Mood_disorder\", \"ethnicity_Dutch\", \"ethnicity_other\"\n",
    "]\n",
    "\n",
    "covariates_SUBCAT_Opioden = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline',\n",
    "    'CAT_Antidepressiva',\n",
    "    'CAT_Antipsychotica',\n",
    "    'DIAGNOSIS_CHILDHOOD_TRAUMA', 'DIAGNOSIS_CPTSD', 'DIAGNOSIS_SEXUAL_TRAUMA',\n",
    "    'DIAGNOSIS_SUICIDALITY', 'age',\n",
    "    'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"Bipolar_and_Mood_disorder\", \"ethnicity_Dutch\", \"ethnicity_other\"\n",
    "]\n",
    "\n",
    "covariates_SUBCAT_Z_drugs = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline',\n",
    "    'CAT_Antidepressiva', 'CAT_Antipsychotica',\n",
    "    'DIAGNOSIS_CHILDHOOD_TRAUMA', 'DIAGNOSIS_CPTSD', 'DIAGNOSIS_SEXUAL_TRAUMA',\n",
    "    'DIAGNOSIS_SUICIDALITY', 'age',\n",
    "    'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"Bipolar_and_Mood_disorder\", \"ethnicity_Dutch\", \"ethnicity_other\"\n",
    "]\n",
    "\n",
    "covariates_SUBCAT_NSAIDs = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline',\n",
    "    'CAT_Antidepressiva',\n",
    "    'CAT_Antipsychotica',\n",
    "    'DIAGNOSIS_CHILDHOOD_TRAUMA', 'DIAGNOSIS_CPTSD', 'DIAGNOSIS_SEXUAL_TRAUMA',\n",
    "    'DIAGNOSIS_SUICIDALITY', 'age',\n",
    "    'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"Bipolar_and_Mood_disorder\", \"ethnicity_Dutch\", \"ethnicity_other\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d14a89b-45eb-4260-bee8-4594d9f4e39e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# This finds all variables that start with covariates_SUBCAT_\n",
    "final_covariates_map = defaultdict(list)\n",
    "final_covariates_map.update({\n",
    "    var.replace(\"covariates_\", \"\"): val\n",
    "    for var, val in globals().items()\n",
    "    if var.lower().startswith(\"covariates_subcat_\") and isinstance(val, list)\n",
    "})\n",
    "\n",
    "# Show detected group names\n",
    "print(\"Groups found:\", list(final_covariates_map.keys()))\n",
    "medication_groups = list(final_covariates_map.keys())\n",
    "print(medication_groups)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68dcdf5-6a5d-4fba-9d5e-0ff3ae44f705",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "def run_all_SUBCAT_group_models(imputed_dfs):\n",
    "    \"\"\"\n",
    "    Runs downstream analysis for each SUBCAT medisubcation group using imputed datasets.\n",
    "\n",
    "    Parameters:\n",
    "    - imputed_dfs: list of 5 imputed DataFrames (from df_imputed_final_imp1.pkl ... imp5.pkl)\n",
    "    \n",
    "    Notes:\n",
    "    - Covariate lists must be defined as global variables: covariates_subcat_<group>\n",
    "    - Outputs are saved in: outputs/SUBCAT_<GROUP>/\n",
    "    \"\"\"\n",
    "\n",
    "    print(\" Starting analysis for all SUBCAT groups\")\n",
    "\n",
    "    for var_name in globals():\n",
    "        if var_name.lower().startswith(\"covariates_subcat_\") and isinstance(globals()[var_name], list):\n",
    "            group_name = var_name.replace(\"covariates_\", \"\")\n",
    "            group_name = group_name.replace(\"_\", \" \").title().replace(\" \", \"_\")  # e.g., subcat_z_drugs → Subcat_Z_Drugs\n",
    "            group_name = group_name.replace(\"Subcat_\", \"SUBCAT_\")  # force prefix to uppercase\n",
    "\n",
    "            covariates = globals()[var_name]\n",
    "            output_dir = f\"outputs/{group_name}\"\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "            print(f\"\\n Processing {group_name}...\")\n",
    "\n",
    "            for k, df_imp in enumerate(imputed_dfs):\n",
    "                print(f\"  → Using imputation {k+1}\")\n",
    "\n",
    "                # Define X and Y\n",
    "                X = df_imp[covariates]\n",
    "                Y = df_imp[\"caps5_change_baseline\"]\n",
    "\n",
    "                # === Save X and Y as placeholder (replace with modeling later)\n",
    "                X.to_csv(f\"{output_dir}/X_imp{k+1}.csv\", index=False)\n",
    "                Y.to_frame(name=\"Y\").to_csv(f\"{output_dir}/Y_imp{k+1}.csv\", index=False)\n",
    "\n",
    "            print(f\" Done: {group_name}\")\n",
    "\n",
    "    print(\"\\n All SUBCAT group analyses complete.\")\n",
    "\n",
    "# ========= STEP 4: Execute ========= #\n",
    "run_all_SUBCAT_group_models(imputed_dfs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40cd90ed-a540-4498-92a6-58c70ad1d441",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for treatment_var in medication_groups:\n",
    "    print(f\"\\n {treatment_var}\")\n",
    "    \n",
    "    for i, df in enumerate(imputed_dfs):\n",
    "        if treatment_var not in df.columns:\n",
    "            print(f\"  Imp {i+1}:  Not found in columns.\")\n",
    "            continue\n",
    "\n",
    "        treated = (df[treatment_var] == 1).sum()\n",
    "        control = (df[treatment_var] == 0).sum()\n",
    "        missing = df[treatment_var].isna().sum()\n",
    "\n",
    "        print(f\"  Imp {i+1}: Treated = {treated}, Control = {control}, Missing = {missing}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57b3c56-b77b-40db-a63c-026f92d9214e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from collections import defaultdict\n",
    "\n",
    "# ✅ VIF computation function\n",
    "def compute_vif(X):\n",
    "    X = sm.add_constant(X, has_constant='add')\n",
    "    vif_df = pd.DataFrame()\n",
    "    vif_df[\"variable\"] = X.columns\n",
    "    vif_df[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "    return vif_df\n",
    "\n",
    "# ✅ Process each group\n",
    "for group in medication_groups:\n",
    "    print(f\"\\n🔍 Processing VIF for {group}\")\n",
    "\n",
    "    if group not in final_covariates_map:\n",
    "        print(f\" ⚠️ No covariates found for {group}. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    covariates = final_covariates_map[group]\n",
    "    vif_list = []\n",
    "\n",
    "    for i, df_imp in enumerate(imputed_dfs):\n",
    "        try:\n",
    "            X = df_imp[covariates].copy()\n",
    "            vif_df = compute_vif(X)\n",
    "            vif_df[\"imputation\"] = i + 1\n",
    "            vif_list.append(vif_df)\n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ Failed on imputation {i+1} for {group}: {e}\")\n",
    "\n",
    "    if vif_list:\n",
    "        all_vif = pd.concat(vif_list)\n",
    "        pooled_vif = all_vif.groupby(\"variable\")[\"VIF\"].mean().reset_index()\n",
    "        pooled_vif = pooled_vif.sort_values(by=\"VIF\", ascending=False)\n",
    "\n",
    "        output_folder = os.path.join(\"outputs\", group)\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "        pooled_vif.to_csv(os.path.join(output_folder, \"pooled_vif.csv\"), index=False)\n",
    "\n",
    "        print(f\" ✅ Saved: {output_folder}/pooled_vif.csv\")\n",
    "    else:\n",
    "        print(f\" ⚠️ Skipped {group}: No valid imputations.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc544c6-0815-4712-ac03-b378443452b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ---------- PS Estimation Function ----------\n",
    "def run_xgboost_ps_modeling(imputed_dfs, medication_groups, final_covariates_map):\n",
    "    for group in medication_groups:\n",
    "        print(f\" Running PS estimation for {group}\")\n",
    "\n",
    "        if group not in final_covariates_map:\n",
    "            print(f\" No covariates found for {group}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        output_folder = os.path.join(\"outputs\", group)\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "        covariates = final_covariates_map[group]\n",
    "        ps_matrix = pd.DataFrame()\n",
    "        auc_list = []\n",
    "\n",
    "        for i, df_imp in enumerate(imputed_dfs):\n",
    "            if group not in df_imp.columns:\n",
    "                print(f\" {group} not found in imputed dataset {i+1}. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            X = df_imp[covariates].copy()\n",
    "            T = df_imp[group]\n",
    "\n",
    "            # Drop missing treatment rows\n",
    "            valid_idx = T.dropna().index\n",
    "            X = X.loc[valid_idx]\n",
    "            T = T.loc[valid_idx]\n",
    "\n",
    "            # Train-test split for ROC\n",
    "            X_train, X_test, T_train, T_test = train_test_split(\n",
    "                X, T, stratify=T, test_size=0.3, random_state=42\n",
    "            )\n",
    "\n",
    "            try:\n",
    "                model = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "                model.fit(X_train, T_train)\n",
    "\n",
    "                ps_scores = model.predict_proba(X)[:, 1]\n",
    "                ps_matrix[f\"ps_imp{i+1}\"] = pd.Series(ps_scores, index=valid_idx)\n",
    "\n",
    "                # ROC & AUC\n",
    "                auc = roc_auc_score(T_test, model.predict_proba(X_test)[:, 1])\n",
    "                auc_list.append(auc)\n",
    "\n",
    "                fpr, tpr, _ = roc_curve(T_test, model.predict_proba(X_test)[:, 1])\n",
    "                plt.figure()\n",
    "                plt.plot(fpr, tpr, label=f\"AUC = {auc:.3f}\")\n",
    "                plt.plot([0, 1], [0, 1], 'k--')\n",
    "                plt.xlabel(\"False Positive Rate\")\n",
    "                plt.ylabel(\"True Positive Rate\")\n",
    "                plt.title(f\"ROC Curve - {group} (Imp {i+1})\")\n",
    "                plt.legend()\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(os.path.join(output_folder, f\"roc_curve_imp{i+1}.png\"))\n",
    "                plt.close()\n",
    "                print(f\"   Imp {i+1}: AUC = {auc:.3f}, ROC saved.\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"   Error in {group} (imp {i+1}): {e}\")\n",
    "\n",
    "        # Save AUCs and Composite PS\n",
    "        if not ps_matrix.empty:\n",
    "            # Fill NaN rows (from dropped subjects in some imputations) with mean\n",
    "            ps_matrix[\"composite_ps\"] = ps_matrix.mean(axis=1)\n",
    "            ps_matrix.to_excel(os.path.join(output_folder, \"propensity_scores.xlsx\"))\n",
    "\n",
    "            auc_df = pd.DataFrame({\n",
    "                \"imputation\": [f\"imp{i+1}\" for i in range(len(auc_list))],\n",
    "                \"AUC\": auc_list\n",
    "            })\n",
    "            auc_df.loc[len(auc_df.index)] = [\"mean\", np.mean(auc_list) if auc_list else np.nan]\n",
    "            auc_df.to_excel(os.path.join(output_folder, \"auc_scores.xlsx\"), index=False)\n",
    "\n",
    "            print(f\" Composite PS + AUC saved for {group}\")\n",
    "        else:\n",
    "            print(f\" No valid PS scores generated for {group}\")\n",
    "\n",
    "# ---------- Run ----------\n",
    "run_xgboost_ps_modeling(imputed_dfs, medication_groups, final_covariates_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47524442-ca28-4f91-9a75-b6d770877c44",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "def compute_feature_importance(imputed_dfs, medication_groups, final_covariates_map):\n",
    "    for group in medication_groups:\n",
    "        print(f\"\\n Computing feature importance for {group}\")\n",
    "\n",
    "        if group not in final_covariates_map:\n",
    "            print(f\" No covariates found for {group}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        covariates = final_covariates_map[group]\n",
    "        output_folder = os.path.join(\"outputs\", group)\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "        importance_df_list = []\n",
    "\n",
    "        for i, df_imp in enumerate(imputed_dfs):\n",
    "            if group not in df_imp.columns:\n",
    "                print(f\" {group} not in imputed dataset {i+1}. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            X = df_imp[covariates].copy()\n",
    "            T = df_imp[group]\n",
    "\n",
    "            # Drop NaNs in treatment\n",
    "            valid_idx = T.dropna().index\n",
    "            X = X.loc[valid_idx]\n",
    "            T = T.loc[valid_idx]\n",
    "\n",
    "            try:\n",
    "                model = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "                model.fit(X, T)\n",
    "\n",
    "                # Get feature importance\n",
    "                importances = model.get_booster().get_score(importance_type='gain')\n",
    "                df_feat = pd.DataFrame.from_dict(importances, orient='index', columns=[f\"imp{i+1}\"])\n",
    "                df_feat.index.name = 'feature'\n",
    "                importance_df_list.append(df_feat)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"   Error during modeling: {e}\")\n",
    "\n",
    "        if importance_df_list:\n",
    "            # Combine and average\n",
    "            all_feat = pd.concat(importance_df_list, axis=1).fillna(0)\n",
    "            all_feat[\"mean_importance\"] = all_feat.mean(axis=1)\n",
    "\n",
    "            # Filter top 30 non-zero\n",
    "            non_zero = all_feat[all_feat[\"mean_importance\"] > 0]\n",
    "            top30 = non_zero.sort_values(by=\"mean_importance\", ascending=False).head(30)\n",
    "\n",
    "            # Save to CSV\n",
    "            top30.to_csv(os.path.join(output_folder, \"feature_importance.csv\"))\n",
    "\n",
    "            # Plot\n",
    "            plt.figure(figsize=(10, 8))\n",
    "            plt.barh(top30.index[::-1], top30[\"mean_importance\"][::-1])  # plot top → bottom\n",
    "            plt.xlabel(\"Mean Gain Importance\")\n",
    "            plt.title(f\"Top 30 Feature Importance - {group}\")\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(output_folder, \"feature_importance_top30.png\"))\n",
    "            plt.close()\n",
    "\n",
    "            print(f\" Saved feature importance plot and CSV for {group}\")\n",
    "        else:\n",
    "            print(f\" No valid models for {group}\")\n",
    "\n",
    "#  Run\n",
    "compute_feature_importance(imputed_dfs, medication_groups, final_covariates_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7b6712-1c60-44d2-96a3-f57274cc0cf1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def compute_trimmed_clipped_iptw(ps_df, treatment, lower=0.05, upper=0.95, clip_max=10):\n",
    "    weights = []\n",
    "    keep_mask = (ps_df > lower) & (ps_df < upper)\n",
    "\n",
    "    for i in range(ps_df.shape[1]):\n",
    "        ps = ps_df.iloc[:, i].clip(lower=1e-6, upper=1 - 1e-6)  # avoid div by zero\n",
    "        mask = keep_mask.iloc[:, i]\n",
    "        w = pd.Series(np.nan, index=ps.index)\n",
    "\n",
    "        w[mask & (treatment == 1)] = 1 / ps[mask & (treatment == 1)]\n",
    "        w[mask & (treatment == 0)] = 1 / (1 - ps[mask & (treatment == 0)])\n",
    "        w = w.clip(upper=clip_max)\n",
    "        weights.append(w)\n",
    "\n",
    "    return pd.concat(weights, axis=1)\n",
    "\n",
    "\n",
    "def apply_rubins_rule_to_iptw(iptw_matrix):\n",
    "    \"\"\"\n",
    "    Given an IPTW matrix (n rows × M imputations), return Rubin’s rule pooled mean, SD, SE.\n",
    "    \"\"\"\n",
    "    M = iptw_matrix.shape[1]\n",
    "    q_bar = iptw_matrix.mean(axis=1)\n",
    "    u_bar = iptw_matrix.var(axis=1, ddof=1)\n",
    "    B = iptw_matrix.apply(lambda x: x.mean(), axis=1).var(ddof=1)\n",
    "    total_var = u_bar + (1 + 1/M) * B\n",
    "    total_se = np.sqrt(total_var)\n",
    "    return q_bar, u_bar.pow(0.5), total_se\n",
    "\n",
    "\n",
    "def run_trim_clip_save_all(imputed_dfs, medication_groups, final_covariates_map):\n",
    "    for group in medication_groups:\n",
    "        print(f\"\\n🔍 Processing IPTW + trimming + clipping for {group}\")\n",
    "        output_folder = os.path.join(\"outputs\", group)\n",
    "        ps_path = os.path.join(output_folder, \"propensity_scores.xlsx\")\n",
    "\n",
    "        if not os.path.exists(ps_path):\n",
    "            print(f\"⚠️ Missing PS file: {ps_path}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            ps_all = pd.read_excel(ps_path, index_col=0)\n",
    "            ps_cols = [col for col in ps_all.columns if col.startswith(\"ps_imp\")]\n",
    "            composite_index = ps_all.index\n",
    "\n",
    "            # Get treatment from one imputed dataset\n",
    "            T_full = None\n",
    "            for df in imputed_dfs:\n",
    "                if group in df.columns:\n",
    "                    T_full = df.loc[composite_index, group]\n",
    "                    break\n",
    "\n",
    "            if T_full is None:\n",
    "                print(f\"❌ Treatment column {group} not found in any imputed dataset.\")\n",
    "                continue\n",
    "\n",
    "            # Compute IPTW matrix (shape: n × M)\n",
    "            iptw_matrix = compute_trimmed_clipped_iptw(ps_all[ps_cols], T_full)\n",
    "            iptw_matrix.columns = [f\"iptw_imp{i+1}\" for i in range(iptw_matrix.shape[1])]\n",
    "\n",
    "            # Apply Rubin’s Rule for mean, SD, SE\n",
    "            iptw_matrix[\"iptw_mean\"], iptw_matrix[\"iptw_sd\"], iptw_matrix[\"iptw_se\"] = apply_rubins_rule_to_iptw(\n",
    "                iptw_matrix[[f\"iptw_imp{i+1}\" for i in range(iptw_matrix.shape[1])]]\n",
    "            )\n",
    "\n",
    "            # Save IPTW matrix separately\n",
    "            iptw_matrix.to_excel(os.path.join(output_folder, \"iptw_weights.xlsx\"))\n",
    "            print(f\"✅ Saved IPTW weights for {group}\")\n",
    "\n",
    "            # Save trimmed & clipped imputed datasets with IPTW\n",
    "            for i in range(5):\n",
    "                df = imputed_dfs[i].copy()\n",
    "                if group not in df.columns:\n",
    "                    continue\n",
    "\n",
    "                trimmed_idx = iptw_matrix.index.intersection(df.index)\n",
    "                needed_cols = final_covariates_map[group] + [group, \"caps5_change_baseline\"]\n",
    "\n",
    "                # Select only necessary columns\n",
    "                df_trimmed = df.loc[trimmed_idx, needed_cols].copy()\n",
    "                df_trimmed[\"iptw\"] = iptw_matrix[f\"iptw_imp{i+1}\"].loc[trimmed_idx]\n",
    "\n",
    "                # ✅ DROP rows with missing IPTW values\n",
    "                before = len(df_trimmed)\n",
    "                df_trimmed = df_trimmed.dropna(subset=[\"iptw\"])\n",
    "                after = len(df_trimmed)\n",
    "                print(f\"    ℹ️ Retained {after}/{before} rows after IPTW NaN drop.\")\n",
    "\n",
    "                # Save to .pkl\n",
    "                df_trimmed.to_pickle(os.path.join(output_folder, f\"trimmed_data_imp{i+1}.pkl\"))\n",
    "                print(f\"  💾 Saved trimmed dataset: {output_folder}/trimmed_data_imp{i+1}.*\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error in {group}: {e}\")\n",
    "\n",
    "\n",
    "run_trim_clip_save_all(imputed_dfs, medication_groups, final_covariates_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8132b1b6-2315-4d56-9728-4f4c73cde88e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_ps_overlap_all_groups(medication_groups):\n",
    "    for group in medication_groups:\n",
    "        print(f\"\\n📊 Plotting PS overlap for {group}\")\n",
    "\n",
    "        folder = os.path.join(\"outputs\", group)\n",
    "        ps_file = os.path.join(folder, \"propensity_scores.xlsx\")\n",
    "        iptw_file = os.path.join(folder, \"iptw_weights.xlsx\")\n",
    "        trimmed_file = os.path.join(folder, \"trimmed_data_imp1.pkl\")\n",
    "\n",
    "        if not all(os.path.exists(f) for f in [ps_file, iptw_file, trimmed_file]):\n",
    "            print(f\"⚠️ Missing required files for {group}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            ps_df = pd.read_excel(ps_file, index_col=0)\n",
    "            iptw_df = pd.read_excel(iptw_file, index_col=0)\n",
    "            trimmed_df = pd.read_pickle(trimmed_file)\n",
    "\n",
    "            # Extract\n",
    "            ps = ps_df[\"composite_ps\"].reindex(trimmed_df.index)\n",
    "            w = iptw_df[\"iptw_mean\"].reindex(trimmed_df.index)\n",
    "            T = trimmed_df[group]\n",
    "\n",
    "            # Masks to remove NaNs\n",
    "            treated_mask = (T == 1) & ps.notna() & w.notna()\n",
    "            control_mask = (T == 0) & ps.notna() & w.notna()\n",
    "\n",
    "            treated = ps[treated_mask]\n",
    "            treated_w = w[treated_mask]\n",
    "\n",
    "            control = ps[control_mask]\n",
    "            control_w = w[control_mask]\n",
    "\n",
    "            # === Unweighted Plot ===\n",
    "            plt.figure(figsize=(8, 5))\n",
    "            plt.hist([treated, control], bins=25, label=[\"Treated\", \"Control\"], alpha=0.6)\n",
    "            plt.title(f\"Unweighted PS Overlap - {group}\")\n",
    "            plt.xlabel(\"Composite Propensity Score\")\n",
    "            plt.ylabel(\"Count\")\n",
    "            plt.legend()\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(folder, \"ps_overlap_unweighted.png\"))\n",
    "            plt.close()\n",
    "\n",
    "            # === Weighted Plot ===\n",
    "            plt.figure(figsize=(8, 5))\n",
    "            plt.hist([treated, control], bins=25, weights=[treated_w, control_w], label=[\"Treated\", \"Control\"], alpha=0.6)\n",
    "            plt.title(f\"Weighted PS Overlap - {group}\")\n",
    "            plt.xlabel(\"Composite Propensity Score\")\n",
    "            plt.ylabel(\"Weighted Count\")\n",
    "            plt.legend()\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(folder, \"ps_overlap_weighted.png\"))\n",
    "            plt.close()\n",
    "\n",
    "            print(f\"✅ Saved unweighted and weighted PS plots for {group}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error processing {group}: {e}\")\n",
    "\n",
    "# 🔁 Run\n",
    "plot_ps_overlap_all_groups(medication_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08914d6c-189b-423c-8f41-c47cdac5c0cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set up base output folder\n",
    "output_base = \"outputs\"\n",
    "ps_file = \"propensity_scores.xlsx\"\n",
    "iptw_file = \"iptw_weights.xlsx\"\n",
    "trimmed_data_file = \"trimmed_data_imp1.pkl\"\n",
    "\n",
    "# Collect all treatment group folders\n",
    "groups = [g for g in medication_groups if os.path.isdir(os.path.join(output_base, g))]\n",
    "\n",
    "\n",
    "# Generate 4-panel overlap plots\n",
    "for group in groups:\n",
    "    group_path = os.path.join(output_base, group)\n",
    "    try:\n",
    "        # Load trimmed treatment info\n",
    "        trimmed_df = pd.read_pickle(os.path.join(group_path, trimmed_data_file))\n",
    "        index = trimmed_df.index\n",
    "\n",
    "        # Fix: case-insensitive match for treatment variable\n",
    "        possible_cols = [col for col in trimmed_df.columns if col.upper() == group.upper()]\n",
    "        if not possible_cols:\n",
    "            print(f\" Treatment variable {group} not found in {group}, skipping.\")\n",
    "            continue\n",
    "        treatment_var = possible_cols[0]\n",
    "        T = trimmed_df[treatment_var]\n",
    "\n",
    "        # Load composite PS (aligned to trimmed_df index)\n",
    "        ps_df = pd.read_excel(os.path.join(group_path, ps_file), index_col=0)\n",
    "        if 'composite_ps' not in ps_df.columns:\n",
    "            print(f\" Composite column missing in {ps_file}, skipping {group}.\")\n",
    "            continue\n",
    "        ps = ps_df.loc[index, 'composite_ps']\n",
    "\n",
    "        # Load IPTW weights (aligned to trimmed_df index)\n",
    "        weights_df = pd.read_excel(os.path.join(group_path, iptw_file), index_col=0)\n",
    "        if 'iptw_mean' not in weights_df.columns:\n",
    "            print(f\" IPTW weight column missing in {iptw_file}, skipping {group}.\")\n",
    "            continue\n",
    "        weights = weights_df.loc[index, 'iptw_mean']\n",
    "\n",
    "        # Prepare 4 datasets\n",
    "        raw_treated = ps[T == 1]\n",
    "        raw_control = ps[T == 0]\n",
    "        weighted_treated = (ps[T == 1], weights[T == 1])\n",
    "        weighted_control = (ps[T == 0], weights[T == 0])\n",
    "\n",
    "        # Create plot\n",
    "        fig, axs = plt.subplots(2, 2, figsize=(12, 8))\n",
    "        fig.suptitle(f\"Propensity Score Distribution - {group}\", fontsize=14)\n",
    "\n",
    "        axs[0, 0].hist(raw_treated, bins=20, alpha=0.7, color='blue')\n",
    "        axs[0, 0].set_title(\"Raw Treated\")\n",
    "\n",
    "        axs[0, 1].hist(raw_control, bins=20, alpha=0.7, color='green')\n",
    "        axs[0, 1].set_title(\"Raw Control\")\n",
    "\n",
    "        axs[1, 0].hist(weighted_treated[0], bins=20, weights=weighted_treated[1], alpha=0.7, color='blue')\n",
    "        axs[1, 0].set_title(\"Weighted Treated\")\n",
    "\n",
    "        axs[1, 1].hist(weighted_control[0], bins=20, weights=weighted_control[1], alpha=0.7, color='green')\n",
    "        axs[1, 1].set_title(\"Weighted Control\")\n",
    "\n",
    "        for ax in axs.flat:\n",
    "            ax.set_xlim(0, 1)\n",
    "            ax.set_xlabel(\"Propensity Score\")\n",
    "            ax.set_ylabel(\"Count\")\n",
    "\n",
    "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "\n",
    "        # Save figure\n",
    "        plot_path = os.path.join(group_path, f\"four_panel_overlap_{group}.png\")\n",
    "        plt.savefig(plot_path)\n",
    "        plt.close()\n",
    "        print(f\" ✅ Saved: {plot_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\" ❌ Error in {group}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c052e9d8-ca61-4770-893d-15ed4c07b221",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ATT calculation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6aafea-bc26-4052-979e-60c7518866f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weighted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378a97b3-6606-48f7-b72e-8b3612b25dfb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import KFold\n",
    "from econml.dml import LinearDML\n",
    "from scipy.stats import t, probplot\n",
    "import xgboost as xgb\n",
    "\n",
    "# -----------------------------\n",
    "# Configuration\n",
    "# -----------------------------\n",
    "n_repeats = 4\n",
    "seeds = list(range(1, 11))\n",
    "imputations = 5\n",
    "output_folder = \"outputs\"\n",
    "\n",
    "# -----------------------------\n",
    "# Diagnostic Plotting Function\n",
    "# -----------------------------\n",
    "def create_diagnostic_plots(residuals_data, fitted_data, group_name):\n",
    "    \"\"\"Create 4 diagnostic plots for model validation\"\"\"\n",
    "    plots_dir = os.path.join(output_folder, \"plots\")\n",
    "    os.makedirs(plots_dir, exist_ok=True)\n",
    "    \n",
    "    # Flatten the collected data\n",
    "    all_residuals = np.concatenate(residuals_data)\n",
    "    all_fitted = np.concatenate(fitted_data)\n",
    "    \n",
    "    # Create figure with 2x2 subplots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    fig.suptitle(f'Diagnostic Plots - {group_name}', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Residuals vs Fitted\n",
    "    axes[0,0].scatter(all_fitted, all_residuals, alpha=0.6, s=20)\n",
    "    axes[0,0].axhline(y=0, color='red', linestyle='--', alpha=0.8)\n",
    "    axes[0,0].set_xlabel('Fitted Values')\n",
    "    axes[0,0].set_ylabel('Residuals')\n",
    "    axes[0,0].set_title('Residuals vs Fitted')\n",
    "    axes[0,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. QQ Plot\n",
    "    probplot(all_residuals, dist=\"norm\", plot=axes[0,1])\n",
    "    axes[0,1].set_title('Q-Q Plot (Normal)')\n",
    "    axes[0,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Residual Histogram\n",
    "    axes[1,0].hist(all_residuals, bins=30, density=True, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    axes[1,0].set_xlabel('Residuals')\n",
    "    axes[1,0].set_ylabel('Density')\n",
    "    axes[1,0].set_title('Residual Distribution')\n",
    "    axes[1,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Scale-Location Plot\n",
    "    sqrt_abs_residuals = np.sqrt(np.abs(all_residuals))\n",
    "    axes[1,1].scatter(all_fitted, sqrt_abs_residuals, alpha=0.6, s=20)\n",
    "    axes[1,1].set_xlabel('Fitted Values')\n",
    "    axes[1,1].set_ylabel('√|Residuals|')\n",
    "    axes[1,1].set_title('Scale-Location Plot')\n",
    "    axes[1,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save plot\n",
    "    plot_filename = os.path.join(plots_dir, f'{group_name}.png')\n",
    "    plt.savefig(plot_filename, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"📊 Diagnostic plots saved for {group_name}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Rubin's Rule\n",
    "# -----------------------------\n",
    "def rubins_pool(estimates, ses):\n",
    "    m = len(estimates)\n",
    "    q_bar = np.mean(estimates)\n",
    "    u_bar = np.mean(np.square(ses))\n",
    "    b_m = np.var(estimates, ddof=1)\n",
    "    total_var = u_bar + ((1 + 1/m) * b_m)\n",
    "    total_se = np.sqrt(total_var)\n",
    "    ci_lower = q_bar - 1.96 * total_se\n",
    "    ci_upper = q_bar + 1.96 * total_se\n",
    "    p_value = 2 * (1 - t.cdf(np.abs(q_bar / total_se), df=m-1))\n",
    "    rounded_p = round(p_value, 5)\n",
    "    formatted_p = \"< 0.00001\" if rounded_p <= 0.00001 else f\"{rounded_p:.5f}\"\n",
    "    return q_bar, total_se, ci_lower, ci_upper, formatted_p\n",
    "\n",
    "# -----------------------------\n",
    "# SMD + Variance Ratio\n",
    "# -----------------------------\n",
    "def calculate_smd_vr(X, T, weights):\n",
    "    treated = X[T == 1]\n",
    "    control = X[T == 0]\n",
    "    w_treated = weights[T == 1]\n",
    "    w_control = weights[T == 0]\n",
    "    smd, vr = [], []\n",
    "    for col in X.columns:\n",
    "        try:\n",
    "            m1, m0 = np.average(treated[col], weights=w_treated), np.average(control[col], weights=w_control)\n",
    "            s1 = np.sqrt(np.average((treated[col] - m1) ** 2, weights=w_treated))\n",
    "            s0 = np.sqrt(np.average((control[col] - m0) ** 2, weights=w_control))\n",
    "            pooled_sd = np.sqrt((s1 ** 2 + s0 ** 2) / 2)\n",
    "            smd.append((m1 - m0) / pooled_sd if pooled_sd > 0 else 0)\n",
    "            vr.append(s1**2 / s0**2 if s0**2 > 0 else 0)\n",
    "        except Exception:\n",
    "            smd.append(np.nan)\n",
    "            vr.append(np.nan)\n",
    "    return np.nanmean(smd), np.nanmean(vr)\n",
    "\n",
    "# -----------------------------\n",
    "# DML Main Loop\n",
    "# -----------------------------\n",
    "def run_dml_with_trimmed_data(final_covariates_map):\n",
    "    att_results = []\n",
    "    balance_results = []\n",
    "\n",
    "    for group, covariates in final_covariates_map.items():\n",
    "        print(f\"\\n🚀 Running DML for {group}\")\n",
    "        group_dir = os.path.join(output_folder, group)\n",
    "        os.makedirs(group_dir, exist_ok=True)\n",
    "\n",
    "        best_result = None\n",
    "        best_se = float(\"inf\")\n",
    "        \n",
    "        # Initialize lists to collect residuals and fitted values for diagnostic plots\n",
    "        group_residuals = []\n",
    "        group_fitted = []\n",
    "\n",
    "        for seed in seeds:\n",
    "            att_list, se_list, r2_list, rmse_list, smd_list, vr_list = [], [], [], [], [], []\n",
    "\n",
    "            for imp in range(1, imputations + 1):\n",
    "                file_path = os.path.join(group_dir, f\"trimmed_data_imp{imp}.pkl\")\n",
    "                if not os.path.exists(file_path):\n",
    "                    continue\n",
    "\n",
    "                df = pd.read_pickle(file_path)\n",
    "                if group not in df.columns or \"iptw\" not in df.columns:\n",
    "                    continue\n",
    "\n",
    "                X = df[covariates].copy()\n",
    "                T = df[group]\n",
    "                Y = df[\"caps5_change_baseline\"]\n",
    "                W = df[\"iptw\"]\n",
    "\n",
    "                for repeat in range(n_repeats):\n",
    "                    kf = KFold(n_splits=5, shuffle=True, random_state=seed + repeat)\n",
    "                    for train_idx, test_idx in kf.split(X):\n",
    "                        try:\n",
    "                            X_train, T_train, Y_train, W_train = (\n",
    "                                X.iloc[train_idx],\n",
    "                                T.iloc[train_idx],\n",
    "                                Y.iloc[train_idx],\n",
    "                                W.iloc[train_idx],\n",
    "                            )\n",
    "\n",
    "                            model_y = xgb.XGBRegressor(n_estimators=100, max_depth=3, learning_rate=0.1, n_jobs=1, random_state=seed)\n",
    "                            model_t = xgb.XGBClassifier(n_estimators=100, max_depth=3, learning_rate=0.1, n_jobs=1,\n",
    "                                                        use_label_encoder=False, eval_metric=\"logloss\", random_state=seed)\n",
    "\n",
    "                            dml = LinearDML(model_y=model_y, model_t=model_t, discrete_treatment=True,\n",
    "                                            cv=KFold(n_splits=3, shuffle=True, random_state=seed), random_state=seed)\n",
    "                            dml.fit(Y_train, T_train, X=X_train, sample_weight=W_train)\n",
    "\n",
    "                            tau = dml.effect(X_train)\n",
    "                            att = np.mean(tau)\n",
    "                            influence = tau - att\n",
    "                            se = np.sqrt(np.mean(influence ** 2) / len(tau))\n",
    "\n",
    "                            att_list.append(att)\n",
    "                            se_list.append(se)\n",
    "\n",
    "                            Y_pred = model_y.fit(X_train, Y_train, sample_weight=W_train).predict(X_train)\n",
    "                            residuals = Y_train - Y_pred\n",
    "                            rmse = mean_squared_error(Y_train, Y_pred, squared=False)\n",
    "                            r2 = r2_score(Y_train, Y_pred)\n",
    "                            r2_list.append(r2)\n",
    "                            rmse_list.append(rmse)\n",
    "                            \n",
    "                            # Collect residuals and fitted values for diagnostic plots\n",
    "                            group_residuals.append(residuals.values)\n",
    "                            group_fitted.append(Y_pred)\n",
    "\n",
    "                            smd, vr = calculate_smd_vr(X_train, T_train, W_train)\n",
    "                            smd_list.append(smd)\n",
    "                            vr_list.append(vr)\n",
    "                        except Exception as e:\n",
    "                            print(f\"⚠️ Error in {group}, seed {seed}, imp {imp}, rep {repeat}: {e}\")\n",
    "\n",
    "            if att_list:\n",
    "                att, se, ci_l, ci_u, p_val = rubins_pool(att_list, se_list)\n",
    "                avg_r2 = np.mean(r2_list)\n",
    "                avg_rmse = np.mean(rmse_list)\n",
    "                avg_smd = np.mean(smd_list)\n",
    "                avg_vr = np.mean(vr_list)\n",
    "\n",
    "                balance_results.append({\n",
    "                    \"group\": group, \"seed\": seed, \"smd\": avg_smd, \"vr\": avg_vr\n",
    "                })\n",
    "\n",
    "                if se < best_se:\n",
    "                    best_se = se\n",
    "                    best_result = {\n",
    "                        \"group\": group, \"seed\": seed, \"att\": att, \"se\": se,\n",
    "                        \"ci_lower\": ci_l, \"ci_upper\": ci_u, \"p_value\": p_val,\n",
    "                        \"r2\": avg_r2, \"rmse\": avg_rmse\n",
    "                    }\n",
    "\n",
    "                print(f\"✅ {group} | Seed {seed}: ATT = {att:.4f}, SE = {se:.4f}, p = {p_val}\")\n",
    "            else:\n",
    "                print(f\"⚠️ No valid results for {group} | Seed {seed}\")\n",
    "\n",
    "        # Create diagnostic plots for this group\n",
    "        if group_residuals and group_fitted:\n",
    "            create_diagnostic_plots(group_residuals, group_fitted, group)\n",
    "\n",
    "        if best_result:\n",
    "            att_results.append(best_result)\n",
    "            print(f\"🏆 Best result for {group} → Seed {best_result['seed']} | SE = {best_result['se']:.4f}\")\n",
    "\n",
    "    # Save final output\n",
    "    pd.DataFrame(att_results).to_excel(\"dml_rubin_summary_subcats.xlsx\", index=False)\n",
    "    pd.DataFrame(balance_results).to_excel(\"smd_vr_summary_subcats.xlsx\", index=False)\n",
    "    print(\"\\n🎯 All summary files saved.\")\n",
    "\n",
    "run_dml_with_trimmed_data(final_covariates_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c415b70-f8a1-4cb5-ae0a-de4ff06d2ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unweighted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994ef029-9b41-45cd-a72f-d2fca16504ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import KFold\n",
    "from econml.dml import LinearDML\n",
    "from scipy.stats import t, probplot\n",
    "import xgboost as xgb\n",
    "\n",
    "# -----------------------------\n",
    "# Configuration\n",
    "# -----------------------------\n",
    "n_repeats = 4\n",
    "seeds = list(range(1, 11))\n",
    "imputations = 5\n",
    "output_folder = \"outputs\"\n",
    "\n",
    "# -----------------------------\n",
    "# Plotting Functions\n",
    "# -----------------------------\n",
    "def create_diagnostic_plots(residuals_data, group_name, output_folder):\n",
    "    \"\"\"Create diagnostic plots for each group\"\"\"\n",
    "    plots_dir = os.path.join(output_folder, \"plots\")\n",
    "    os.makedirs(plots_dir, exist_ok=True)\n",
    "    \n",
    "    all_residuals = residuals_data['residuals']\n",
    "    all_fitted = residuals_data['fitted']\n",
    "    \n",
    "    if len(all_residuals) == 0:\n",
    "        return\n",
    "    \n",
    "    # Create figure with subplots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    fig.suptitle(f'Diagnostic Plots - {group_name}', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # 1. Residuals vs Fitted\n",
    "    axes[0, 0].scatter(all_fitted, all_residuals, alpha=0.5, s=1)\n",
    "    axes[0, 0].axhline(y=0, color='red', linestyle='--')\n",
    "    axes[0, 0].set_xlabel('Fitted Values')\n",
    "    axes[0, 0].set_ylabel('Residuals')\n",
    "    axes[0, 0].set_title('Residuals vs Fitted')\n",
    "    \n",
    "    # 2. QQ Plot\n",
    "    probplot(all_residuals, dist=\"norm\", plot=axes[0, 1])\n",
    "    axes[0, 1].set_title('QQ Plot (Normal)')\n",
    "    \n",
    "    # 3. Histogram of Residuals\n",
    "    axes[1, 0].hist(all_residuals, bins=50, alpha=0.7, edgecolor='black')\n",
    "    axes[1, 0].axvline(x=0, color='red', linestyle='--')\n",
    "    axes[1, 0].set_xlabel('Residuals')\n",
    "    axes[1, 0].set_ylabel('Frequency')\n",
    "    axes[1, 0].set_title('Residual Distribution')\n",
    "    \n",
    "    # 4. Scale-Location Plot\n",
    "    sqrt_abs_resid = np.sqrt(np.abs(all_residuals))\n",
    "    axes[1, 1].scatter(all_fitted, sqrt_abs_resid, alpha=0.5, s=1)\n",
    "    axes[1, 1].set_xlabel('Fitted Values')\n",
    "    axes[1, 1].set_ylabel('√|Residuals|')\n",
    "    axes[1, 1].set_title('Scale-Location Plot')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(plots_dir, f'{group_name}_unweighted.png'), \n",
    "                dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "# -----------------------------\n",
    "# Rubin's Rule\n",
    "# -----------------------------\n",
    "def rubins_pool(estimates, ses):\n",
    "    m = len(estimates)\n",
    "    q_bar = np.mean(estimates)\n",
    "    u_bar = np.mean(np.square(ses))\n",
    "    b_m = np.var(estimates, ddof=1)\n",
    "    total_var = u_bar + ((1 + 1/m) * b_m)\n",
    "    total_se = np.sqrt(total_var)\n",
    "    ci_lower = q_bar - 1.96 * total_se\n",
    "    ci_upper = q_bar + 1.96 * total_se\n",
    "    p_value = 2 * (1 - t.cdf(np.abs(q_bar / total_se), df=m-1))\n",
    "    rounded_p = round(p_value, 5)\n",
    "    formatted_p = \"< 0.00001\" if rounded_p <= 0.00001 else f\"{rounded_p:.5f}\"\n",
    "    return q_bar, total_se, ci_lower, ci_upper, formatted_p\n",
    "\n",
    "# -----------------------------\n",
    "# SMD + Variance Ratio\n",
    "# -----------------------------\n",
    "def calculate_smd_vr(X, T):\n",
    "    treated = X[T == 1]\n",
    "    control = X[T == 0]\n",
    "    smd, vr = [], []\n",
    "    for col in X.columns:\n",
    "        try:\n",
    "            m1, m0 = treated[col].mean(), control[col].mean()\n",
    "            s1, s0 = treated[col].std(), control[col].std()\n",
    "            pooled_sd = np.sqrt((s1 ** 2 + s0 ** 2) / 2)\n",
    "            smd.append((m1 - m0) / pooled_sd if pooled_sd > 0 else 0)\n",
    "            vr.append(s1**2 / s0**2 if s0**2 > 0 else 0)\n",
    "        except Exception:\n",
    "            smd.append(np.nan)\n",
    "            vr.append(np.nan)\n",
    "    return np.nanmean(smd), np.nanmean(vr)\n",
    "\n",
    "# -----------------------------\n",
    "# DML Main Loop\n",
    "# -----------------------------\n",
    "def run_dml_with_trimmed_data(final_covariates_map):\n",
    "    att_results = []\n",
    "    balance_results = []\n",
    "\n",
    "    for group, covariates in final_covariates_map.items():\n",
    "        print(f\"\\n🚀 Running DML for {group}\")\n",
    "        group_dir = os.path.join(output_folder, group)\n",
    "        os.makedirs(group_dir, exist_ok=True)\n",
    "\n",
    "        best_result = None\n",
    "        best_se = float(\"inf\")\n",
    "        \n",
    "        # Initialize residuals collection for this group\n",
    "        group_residuals_data = {'residuals': [], 'fitted': []}\n",
    "\n",
    "        for seed in seeds:\n",
    "            att_list, se_list, r2_list, rmse_list, smd_list, vr_list = [], [], [], [], [], []\n",
    "\n",
    "            for imp in range(1, imputations + 1):\n",
    "                file_path = os.path.join(group_dir, f\"trimmed_data_imp{imp}.pkl\")\n",
    "                if not os.path.exists(file_path):\n",
    "                    continue\n",
    "\n",
    "                df = pd.read_pickle(file_path)\n",
    "                if group not in df.columns:\n",
    "                    continue\n",
    "\n",
    "                X = df[covariates].copy()\n",
    "                T = df[group]\n",
    "                Y = df[\"caps5_change_baseline\"]\n",
    "\n",
    "                for repeat in range(n_repeats):\n",
    "                    kf = KFold(n_splits=5, shuffle=True, random_state=seed + repeat)\n",
    "                    for train_idx, test_idx in kf.split(X):\n",
    "                        try:\n",
    "                            X_train, T_train, Y_train = (\n",
    "                                X.iloc[train_idx],\n",
    "                                T.iloc[train_idx],\n",
    "                                Y.iloc[train_idx],\n",
    "                            )\n",
    "\n",
    "                            model_y = xgb.XGBRegressor(n_estimators=100, max_depth=3, learning_rate=0.1, n_jobs=1, random_state=seed)\n",
    "                            model_t = xgb.XGBClassifier(n_estimators=100, max_depth=3, learning_rate=0.1, n_jobs=1,\n",
    "                                                        use_label_encoder=False, eval_metric=\"logloss\", random_state=seed)\n",
    "\n",
    "                            dml = LinearDML(model_y=model_y, model_t=model_t, discrete_treatment=True,\n",
    "                                            cv=KFold(n_splits=3, shuffle=True, random_state=seed), random_state=seed)\n",
    "                            dml.fit(Y_train, T_train, X=X_train)\n",
    "\n",
    "                            tau = dml.effect(X_train)\n",
    "                            att = np.mean(tau)\n",
    "                            influence = tau - att\n",
    "                            se = np.sqrt(np.mean(influence ** 2) / len(tau))\n",
    "\n",
    "                            att_list.append(att)\n",
    "                            se_list.append(se)\n",
    "\n",
    "                            Y_pred = model_y.fit(X_train, Y_train).predict(X_train)\n",
    "                            residuals = Y_train - Y_pred\n",
    "                            \n",
    "                            # Collect residuals and fitted values for plotting\n",
    "                            group_residuals_data['residuals'].extend(residuals.tolist())\n",
    "                            group_residuals_data['fitted'].extend(Y_pred.tolist())\n",
    "                            \n",
    "                            rmse = mean_squared_error(Y_train, Y_pred, squared=False)\n",
    "                            r2 = r2_score(Y_train, Y_pred)\n",
    "                            r2_list.append(r2)\n",
    "                            rmse_list.append(rmse)\n",
    "\n",
    "                            smd, vr = calculate_smd_vr(X_train, T_train)\n",
    "                            smd_list.append(smd)\n",
    "                            vr_list.append(vr)\n",
    "                        except Exception as e:\n",
    "                            print(f\"⚠️ Error in {group}, seed {seed}, imp {imp}, rep {repeat}: {e}\")\n",
    "\n",
    "            if att_list:\n",
    "                att, se, ci_l, ci_u, p_val = rubins_pool(att_list, se_list)\n",
    "                avg_r2 = np.mean(r2_list)\n",
    "                avg_rmse = np.mean(rmse_list)\n",
    "                avg_smd = np.mean(smd_list)\n",
    "                avg_vr = np.mean(vr_list)\n",
    "\n",
    "                balance_results.append({\n",
    "                    \"group\": group, \"seed\": seed, \"smd\": avg_smd, \"vr\": avg_vr\n",
    "                })\n",
    "\n",
    "                if se < best_se:\n",
    "                    best_se = se\n",
    "                    best_result = {\n",
    "                        \"group\": group, \"seed\": seed, \"att\": att, \"se\": se,\n",
    "                        \"ci_lower\": ci_l, \"ci_upper\": ci_u, \"p_value\": p_val,\n",
    "                        \"r2\": avg_r2, \"rmse\": avg_rmse\n",
    "                    }\n",
    "\n",
    "                print(f\"✅ {group} | Seed {seed}: ATT = {att:.4f}, SE = {se:.4f}, p = {p_val}\")\n",
    "            else:\n",
    "                print(f\"⚠️ No valid results for {group} | Seed {seed}\")\n",
    "\n",
    "        if best_result:\n",
    "            att_results.append(best_result)\n",
    "            print(f\"🏆 Best result for {group} → Seed {best_result['seed']} | SE = {best_result['se']:.4f}\")\n",
    "        \n",
    "        # Create diagnostic plots for this group\n",
    "        print(f\"📊 Creating diagnostic plots for {group}...\")\n",
    "        create_diagnostic_plots(group_residuals_data, group, output_folder)\n",
    "\n",
    "    # Save final output\n",
    "    pd.DataFrame(att_results).to_excel(\"dml_rubin_summary_subcats_unweighted.xlsx\", index=False)\n",
    "    pd.DataFrame(balance_results).to_excel(\"smd_vr_summary_subcats_unweighted.xlsx\", index=False)\n",
    "    print(\"\\n🎯 All summary files saved.\")\n",
    "    print(\"📊 All diagnostic plots saved in outputs/plots/ folder.\")\n",
    "\n",
    "run_dml_with_trimmed_data(final_covariates_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3a88b2-9215-46c0-8fd2-b0251e58352e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import sem, ttest_ind\n",
    "\n",
    "# ----------------------------------\n",
    "# File paths\n",
    "# ----------------------------------\n",
    "output_base = \"outputs\"\n",
    "att_file = \"dml_rubin_summary_subcats.xlsx\"\n",
    "trimmed_file = \"trimmed_data_imp1.pkl\"\n",
    "auc_file = \"auc_scores.xlsx\"  \n",
    "\n",
    "# ----------------------------------\n",
    "# Load ATT Summary\n",
    "# ----------------------------------\n",
    "if os.path.exists(att_file):\n",
    "    att_df = pd.read_excel(att_file)\n",
    "else:\n",
    "    raise FileNotFoundError(\"❌ ATT summary file not found: dml_rubin_summary_subcats.xlsx\")\n",
    "\n",
    "summary_rows = []\n",
    "\n",
    "# ----------------------------------\n",
    "# Loop over medication groups\n",
    "# ----------------------------------\n",
    "groups = [g for g in medication_groups if os.path.isdir(os.path.join(output_base, g))]\n",
    "\n",
    "for med in groups:\n",
    "    try:\n",
    "        group_path = os.path.join(output_base, med)\n",
    "\n",
    "        # Load trimmed data\n",
    "        df = pd.read_pickle(os.path.join(group_path, trimmed_file))\n",
    "\n",
    "        # Detect treatment column\n",
    "        treatment_cols = [col for col in df.columns if col.upper() == med.upper()]\n",
    "        if not treatment_cols:\n",
    "            print(f\"⚠️ Treatment column {med} not found in trimmed data. Skipping.\")\n",
    "            continue\n",
    "        treatment_var = treatment_cols[0]\n",
    "\n",
    "        # Extract treatment and outcome\n",
    "        T = df[treatment_var]\n",
    "        Y = df[\"caps5_change_baseline\"]\n",
    "\n",
    "        # Treated and control stats\n",
    "        treated = Y[T == 1]\n",
    "        control = Y[T == 0]\n",
    "\n",
    "        mean_treat = treated.mean()\n",
    "        se_treat = sem(treated) if len(treated) > 1 else np.nan\n",
    "\n",
    "        mean_ctrl = control.mean()\n",
    "        se_ctrl = sem(control) if len(control) > 1 else np.nan\n",
    "\n",
    "        # Cohen's d (unadjusted)\n",
    "        pooled_sd = np.sqrt(((treated.std() ** 2) + (control.std() ** 2)) / 2)\n",
    "        cohen_d = (mean_treat - mean_ctrl) / pooled_sd if pooled_sd > 0 else np.nan\n",
    "\n",
    "        # E-value (unadjusted)\n",
    "        delta = mean_treat - mean_ctrl\n",
    "        E = delta / abs(mean_ctrl) * 100 if mean_ctrl != 0 else np.nan\n",
    "\n",
    "        # Unadjusted p-value\n",
    "        try:\n",
    "            t_stat, p_val = ttest_ind(treated, control, equal_var=False, nan_policy=\"omit\")\n",
    "            rounded_p = round(p_val, 5)\n",
    "            formatted_p = \"< 0.00001\" if rounded_p < 0.00001 else rounded_p\n",
    "        except Exception:\n",
    "            formatted_p = np.nan\n",
    "\n",
    "        # AUC from new auc_scores.xlsx file\n",
    "        auc_val = np.nan\n",
    "        auc_path = os.path.join(group_path, auc_file)\n",
    "        if os.path.exists(auc_path):\n",
    "            auc_df = pd.read_excel(auc_path)\n",
    "            if \"AUC\" in auc_df.columns:\n",
    "                auc_val = auc_df[\"AUC\"].dropna().mean()\n",
    "\n",
    "        # Adjusted stats from Rubin summary\n",
    "        att_row = att_df[att_df[\"group\"].str.strip().str.upper() == med.strip().upper()]\n",
    "        if not att_row.empty:\n",
    "            att = att_row.iloc[0][\"att\"]\n",
    "            att_se = att_row.iloc[0][\"se\"]\n",
    "            att_p_val = att_row.iloc[0][\"p_value\"]\n",
    "            r2 = att_row.iloc[0][\"r2\"]\n",
    "            rmse = att_row.iloc[0][\"rmse\"]\n",
    "\n",
    "            try:\n",
    "                rounded_att_p = round(float(att_p_val), 5)\n",
    "                formatted_att_p = \"< 0.00001\" if rounded_att_p < 0.00001 else rounded_att_p\n",
    "            except:\n",
    "                formatted_att_p = att_p_val\n",
    "        else:\n",
    "            att, att_se, formatted_att_p, r2, rmse = np.nan, np.nan, np.nan, np.nan, np.nan\n",
    "\n",
    "        # Append full row\n",
    "        summary_rows.append({\n",
    "            'Medication Group': med,\n",
    "            'Mean Treated': mean_treat,\n",
    "            'SE Treated': se_treat,\n",
    "            'Mean Control': mean_ctrl,\n",
    "            'SE Control': se_ctrl,\n",
    "            'Cohen d': cohen_d,\n",
    "            'E (Unadjusted)': E,\n",
    "            'n Treated': len(treated),\n",
    "            'n Control': len(control),\n",
    "            #'Unadjusted p-value': formatted_p,\n",
    "            'ATT Estimate': att,\n",
    "            'ATT SE (Robust)': att_se,\n",
    "            'ATT p-value': formatted_att_p,\n",
    "            'R²': r2,\n",
    "            'RMSE': rmse,\n",
    "            'AUC': auc_val\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error in {med}: {e}\")\n",
    "\n",
    "# ----------------------------------\n",
    "# Save final summary\n",
    "# ----------------------------------\n",
    "summary_df = pd.DataFrame(summary_rows)\n",
    "summary_df = summary_df.sort_values(\"Medication Group\")\n",
    "summary_df.to_excel(\"Final_ATT_Summary_SubCat.xlsx\", index=False)\n",
    "print(\"✅ Final_ATT_Summary_SubCat saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd753254-1fa1-4c8e-8a3c-cbec86b6169a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "# ✅ Load the final summary table\n",
    "final_df = pd.read_excel(\"Final_ATT_Summary_SubCat.xlsx\")\n",
    "\n",
    "# ✅ Parse DML p-values (handle \"< 0.00001\")\n",
    "def parse_pval(p):\n",
    "    try:\n",
    "        if isinstance(p, str) and \"<\" in p:\n",
    "            return 0.000001\n",
    "        return float(p)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "final_df['ATT p-value'] = final_df['ATT p-value'].apply(parse_pval)\n",
    "\n",
    "# ✅ Plot settings\n",
    "width = 0.35\n",
    "\n",
    "# ✅ Plotting function for a single medication group\n",
    "def plot_single_group(row):\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    bars1 = ax.bar(-width/2, row['Mean Control'], width, \n",
    "                   yerr=row['SE Control'], label='Control', hatch='//', color='gray', capsize=5)\n",
    "    bars2 = ax.bar(+width/2, row['Mean Treated'], width, \n",
    "                   yerr=row['SE Treated'], label='Treated', color='steelblue', capsize=5)\n",
    "\n",
    "    label = (\n",
    "        f\"ATT = {row['ATT Estimate']:.2f}\\n\"\n",
    "        f\"d = {row['Cohen d']:.2f}, p = {row['ATT p-value']:.3f}\\n\"\n",
    "        f\"nT = {row['n Treated']}, nC = {row['n Control']}\\n\"\n",
    "        f\"E = {row['E (Unadjusted)']:.1f}%\"\n",
    "    )\n",
    "    max_y = max(row['Mean Control'], row['Mean Treated']) + 1.5\n",
    "    ax.text(0, max_y, label, ha='center', va='bottom', fontsize=9, color='#FFD700')\n",
    "\n",
    "    ax.axhline(0, color='black', linewidth=0.8)\n",
    "    ax.set_xticks([-width/2, +width/2])\n",
    "    ax.set_xticklabels(['Control', 'Treated'])\n",
    "    ax.set_title(f\"Group: {row['Medication Group']}\", fontsize=12, weight='bold')\n",
    "    ax.set_ylabel(\"CAPS5 Change Score\")\n",
    "    ax.set_ylim(bottom=0, top=max_y + 2)\n",
    "    ax.legend()\n",
    "    fig.tight_layout()\n",
    "    return fig\n",
    "\n",
    "# ✅ Generate and save all plots into a multi-page PDF\n",
    "with PdfPages(\"dml_att_barplot_subcat.pdf\") as pdf:\n",
    "    for idx, row in final_df.iterrows():\n",
    "        fig = plot_single_group(row)\n",
    "        pdf.savefig(fig, dpi=300, bbox_inches='tight')\n",
    "        plt.close(fig)\n",
    "print(\"✅ dml_att_barplot_subcat saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808a2609-b153-43da-9068-afb6bbf09dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Love plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba20ca3-c1b5-4c2b-a48b-f90d8b1734fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "# ----------------------------------------\n",
    "# Functions to calculate balance\n",
    "# ----------------------------------------\n",
    "def calculate_smd(x1, x2, w1=None, w2=None):\n",
    "    def weighted_mean(x, w): return np.average(x, weights=w)\n",
    "    def weighted_var(x, w):\n",
    "        m = np.average(x, weights=w)\n",
    "        return np.average((x - m) ** 2, weights=w)\n",
    "    \n",
    "    m1 = weighted_mean(x1, w1) if w1 is not None else np.mean(x1)\n",
    "    m2 = weighted_mean(x2, w2) if w2 is not None else np.mean(x2)\n",
    "    v1 = weighted_var(x1, w1) if w1 is not None else np.var(x1, ddof=1)\n",
    "    v2 = weighted_var(x2, w2) if w2 is not None else np.var(x2, ddof=1)\n",
    "    \n",
    "    pooled_sd = np.sqrt((v1 + v2) / 2)\n",
    "    return np.abs(m1 - m2) / pooled_sd if pooled_sd > 0 else 0\n",
    "\n",
    "def variance_ratio(x1, x2, w1=None, w2=None):\n",
    "    def weighted_var(x, w):\n",
    "        m = np.average(x, weights=w)\n",
    "        return np.average((x - m) ** 2, weights=w)\n",
    "    \n",
    "    v1 = weighted_var(x1, w1) if w1 is not None else np.var(x1, ddof=1)\n",
    "    v2 = weighted_var(x2, w2) if w2 is not None else np.var(x2, ddof=1)\n",
    "    \n",
    "    return max(v1 / v2, v2 / v1) if v1 > 0 and v2 > 0 else 1\n",
    "\n",
    "# ----------------------------------------\n",
    "# Setup\n",
    "# ----------------------------------------\n",
    "output_base = \"outputs\"\n",
    "groups = [g for g in os.listdir(output_base) if os.path.isdir(os.path.join(output_base, g))]\n",
    "\n",
    "# Create a case-insensitive mapping\n",
    "final_covariates_map_lower = {k.lower(): v for k, v in final_covariates_map.items()}\n",
    "\n",
    "# ----------------------------------------\n",
    "# Main Loop\n",
    "# ----------------------------------------\n",
    "for group in groups:\n",
    "    if group.lower() not in final_covariates_map_lower:\n",
    "        continue\n",
    "\n",
    "    print(f\"\\n🔍 Processing {group}...\")\n",
    "\n",
    "    try:\n",
    "        group_path = os.path.join(output_base, group)\n",
    "        covariates = final_covariates_map_lower[group.lower()]\n",
    "        \n",
    "        column_name = None\n",
    "        for col in pd.read_pickle(os.path.join(group_path, \"trimmed_data_imp1.pkl\")).columns:\n",
    "            if col.lower() == group.lower():\n",
    "                column_name = col\n",
    "                break\n",
    "        if column_name is None:\n",
    "            print(f\"⚠️ Column not found for {group}, skipping.\")\n",
    "            continue\n",
    "\n",
    "        smd_unw_all, smd_w_all = [], []\n",
    "        vr_unw_all, vr_w_all = [], []\n",
    "\n",
    "        for i in range(1, 6):\n",
    "            df_path = os.path.join(group_path, f\"trimmed_data_imp{i}.pkl\")\n",
    "            iptw_path = os.path.join(group_path, \"iptw_weights.xlsx\")\n",
    "\n",
    "            if not os.path.exists(df_path) or not os.path.exists(iptw_path):\n",
    "                print(f\"⚠️ Missing data for {group} imp{i}, skipping.\")\n",
    "                continue\n",
    "\n",
    "            df = pd.read_pickle(df_path)\n",
    "            iptw_df = pd.read_excel(iptw_path, index_col=0)\n",
    "            T = df[column_name]\n",
    "            W = iptw_df.loc[df.index, \"iptw_mean\"]\n",
    "\n",
    "            smd_unw_i, smd_w_i, vr_unw_i, vr_w_i = [], [], [], []\n",
    "\n",
    "            for cov in covariates:\n",
    "                x1, x0 = df.loc[T == 1, cov], df.loc[T == 0, cov]\n",
    "                w1, w0 = W[T == 1], W[T == 0]\n",
    "\n",
    "                su = calculate_smd(x1, x0)\n",
    "                sw = calculate_smd(x1, x0, w1, w0)\n",
    "\n",
    "                vu = variance_ratio(x1, x0) if cov in ['treatmentdurationdays', 'CAPS5score_baseline', 'age'] else np.nan\n",
    "                vw = variance_ratio(x1, x0, w1, w0) if cov in ['treatmentdurationdays', 'CAPS5score_baseline', 'age'] else np.nan\n",
    "\n",
    "                smd_unw_i.append(su)\n",
    "                smd_w_i.append(sw)\n",
    "                vr_unw_i.append(vu)\n",
    "                vr_w_i.append(vw)\n",
    "\n",
    "            smd_unw_all.append(smd_unw_i)\n",
    "            smd_w_all.append(smd_w_i)\n",
    "            vr_unw_all.append(vr_unw_i)\n",
    "            vr_w_all.append(vr_w_i)\n",
    "\n",
    "        smd_unw = np.mean(smd_unw_all, axis=0)\n",
    "        smd_w = np.mean(smd_w_all, axis=0)\n",
    "        vr_unw = np.nanmean(vr_unw_all, axis=0)\n",
    "        vr_w = np.nanmean(vr_w_all, axis=0)\n",
    "\n",
    "        severity = []\n",
    "        for sw in smd_w:\n",
    "            if sw <= 0.1:\n",
    "                severity.append(\"Good\")\n",
    "            elif sw <= 0.2:\n",
    "                severity.append(\"Moderate\")\n",
    "            else:\n",
    "                severity.append(\"Poor\")\n",
    "\n",
    "        covariate_names = covariates\n",
    "        numeric_df = pd.DataFrame({\n",
    "            \"Covariate\": covariate_names,\n",
    "            \"SMD_Unweighted\": smd_unw,\n",
    "            \"SMD_Weighted\": smd_w,\n",
    "            \"Imbalance_Severity\": severity,\n",
    "            \"VR_Unweighted\": vr_unw,\n",
    "            \"VR_Weighted\": vr_w\n",
    "        })\n",
    "\n",
    "        numeric_path = os.path.join(group_path, f\"covariate_balance_table_{group}.xlsx\")\n",
    "        numeric_df.to_excel(numeric_path, index=False)\n",
    "        print(f\"📊 Exported numeric summary to: {numeric_path}\")\n",
    "\n",
    "        # -------------------------\n",
    "        # Plot\n",
    "        # -------------------------\n",
    "        labels = covariates\n",
    "        y_pos = np.arange(len(labels))\n",
    "\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(18, len(labels) * 0.45))\n",
    "\n",
    "        axes[0].scatter(smd_unw, y_pos, color='red', label=\"Unweighted\")\n",
    "        axes[0].scatter(smd_w, y_pos, color='blue', label=\"Weighted\")\n",
    "        axes[0].axvline(0.1, color='gray', linestyle='--', label=\"Threshold 0.1\")\n",
    "        axes[0].axvline(0.2, color='black', linestyle='--', label=\"Threshold 0.2\")\n",
    "        axes[0].set_xlim(0, max(max(smd_unw), max(smd_w), 0.25) + 0.05)\n",
    "        axes[0].set_yticks(y_pos)\n",
    "        axes[0].set_yticklabels(labels)\n",
    "        axes[0].invert_yaxis()\n",
    "        axes[0].set_title(\"Standardized Mean Differences (SMD)\")\n",
    "        axes[0].legend(loc=\"upper right\")\n",
    "        axes[0].grid(True)\n",
    "\n",
    "        vr_mask = [cov in ['treatmentdurationdays', 'CAPS5score_baseline', 'age'] for cov in covariates]\n",
    "        filtered_y = [i for i, b in enumerate(vr_mask) if b]\n",
    "        filtered_labels = [labels[i] for i in filtered_y]\n",
    "        filtered_vr_unw = [vr_unw[i] for i in filtered_y]\n",
    "        filtered_vr_w = [vr_w[i] for i in filtered_y]\n",
    "\n",
    "        axes[1].scatter(filtered_vr_unw, filtered_y, color='blue', marker='o', label=\"Unweighted\")\n",
    "        axes[1].scatter(filtered_vr_w, filtered_y, color='red', marker='x', label=\"Weighted\")\n",
    "        axes[1].axvline(2, color='gray', linestyle='--')\n",
    "        axes[1].axvline(0.5, color='gray', linestyle='--')\n",
    "        axes[1].set_xlim(0, max(filtered_vr_unw + filtered_vr_w + [2.5]) + 0.5)\n",
    "        axes[1].set_yticks(filtered_y)\n",
    "        axes[1].set_yticklabels(filtered_labels)\n",
    "        axes[1].invert_yaxis()\n",
    "        axes[1].set_title(\"Variance Ratio (VR)\")\n",
    "        axes[1].legend()\n",
    "        axes[1].grid(True)\n",
    "\n",
    "        fig.suptitle(f\"Covariate Balance for {group.replace('CAT_', '')}\", fontsize=14, weight='bold')\n",
    "        fig.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "        plot_path = os.path.join(group_path, f\"love_plot_{group}.pdf\")\n",
    "        fig.savefig(plot_path, dpi=300)\n",
    "        plt.close()\n",
    "        print(f\"✅ Saved love plot: {plot_path}\")\n",
    "        print(f\"📏 Max weighted SMD for {group}: {np.max(smd_w):.3f}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error in {group}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab142d4-d10c-4ad7-ba2a-fdeb4d489dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb78ca6-383e-4d8c-95c9-43271e6ecc64",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "#-----------------------------\n",
    "# Generate heatmaps\n",
    "# -------------------------------\n",
    "for treatment_var in medication_groups:\n",
    "    print(f\"\\n========== Creating Heatmap for {treatment_var} ==========\")\n",
    "\n",
    "    try:\n",
    "        output_folder = os.path.join('outputs', treatment_var)\n",
    "        balance_path = os.path.join(output_folder, f'covariate_balance_table_{treatment_var}.xlsx')\n",
    "\n",
    "        if not os.path.exists(balance_path):\n",
    "            print(f\"❌ Balance file not found: {balance_path}\")\n",
    "            continue\n",
    "\n",
    "        balance_df = pd.read_excel(balance_path)\n",
    "\n",
    "        # ✅ Use finalized covariates + 'Propensity Score'\n",
    "        covariates = final_covariates_map[treatment_var] + ['Propensity Score']\n",
    "        balance_df = balance_df[balance_df['Covariate'].isin(covariates)]\n",
    "\n",
    "        # ✅ Check for CAPS5score_baseline\n",
    "        highlight_caps = 'CAPS5score_baseline' in balance_df['Covariate'].values\n",
    "\n",
    "        # ✅ Format for heatmap\n",
    "        heatmap_df = balance_df[['Covariate', 'SMD_Unweighted', 'SMD_Weighted']].copy()\n",
    "        heatmap_df.columns = ['Covariate', 'Unweighted', 'Weighted']\n",
    "        heatmap_df = heatmap_df.set_index('Covariate')\n",
    "        heatmap_df = heatmap_df.sort_values(by='Unweighted', ascending=False)\n",
    "\n",
    "        # ✅ Plot\n",
    "        plt.figure(figsize=(12, max(10, len(heatmap_df) * 0.35)))\n",
    "        ax = sns.heatmap(\n",
    "            heatmap_df,\n",
    "            cmap=\"coolwarm\",\n",
    "            annot=True,\n",
    "            fmt=\".2f\",\n",
    "            linewidths=0.6,\n",
    "            linecolor='gray',\n",
    "            cbar_kws={\"label\": \"Standardized Mean Difference\"}\n",
    "        )\n",
    "\n",
    "        plt.title(f\"Covariate Balance Heatmap (Rubin IPTW)\\n{treatment_var}\", fontsize=15, weight='bold')\n",
    "        plt.xlabel(\"Condition\")\n",
    "        plt.ylabel(\"Covariate\")\n",
    "\n",
    "        # ✅ Bold CAPS5score_baseline if present\n",
    "        if highlight_caps:\n",
    "            ylabels = [label.get_text() for label in ax.get_yticklabels()]\n",
    "            ax.set_yticklabels([\n",
    "                f\"{label} ←\" if label == 'CAPS5score_baseline' else label for label in ylabels\n",
    "            ])\n",
    "\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # ✅ Save image\n",
    "        save_path = os.path.join(output_folder, f'heatmap_smd_{treatment_var}.png')\n",
    "        plt.savefig(save_path, dpi=300)\n",
    "        plt.close()\n",
    "\n",
    "        print(f\"✅ Heatmap saved: {save_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error processing {treatment_var}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8017939-91a9-45e2-90b0-e309b89f8a89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fa951dd2-27ff-43ba-b9fd-c07e234136ff",
   "metadata": {},
   "source": [
    "## SubSubCat Analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69094b37-2a1f-4e4f-9430-b1f89e29fdf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "covariates_SubSubCat_Oxazepam = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline',\n",
    "    'CAT_Antidepressiva', 'CAT_Antipsychotica',\n",
    "    'DIAGNOSIS_CHILDHOOD_TRAUMA', 'DIAGNOSIS_CPTSD', 'DIAGNOSIS_SEXUAL_TRAUMA',\n",
    "    'DIAGNOSIS_SUICIDALITY', 'age',\n",
    "    'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"Bipolar_and_Mood_disorder\", \"ethnicity_Dutch\", \"ethnicity_other\"\n",
    "]\n",
    "\n",
    "covariates_SubSubCat_Diazepam = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline',\n",
    "    'CAT_Antidepressiva', 'CAT_Antipsychotica',\n",
    "    'DIAGNOSIS_CHILDHOOD_TRAUMA', 'DIAGNOSIS_CPTSD', 'DIAGNOSIS_SEXUAL_TRAUMA',\n",
    "    'DIAGNOSIS_SUICIDALITY', 'age',\n",
    "    'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"Bipolar_and_Mood_disorder\", \"ethnicity_Dutch\", \"ethnicity_other\"\n",
    "]\n",
    "\n",
    "\n",
    "covariates_SubSubCat_Paracetamol = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline',\n",
    "    'CAT_Antidepressiva',\n",
    "    'CAT_Antipsychotica',\n",
    "    'CAT_Benzodiazepine',\n",
    "    'DIAGNOSIS_CHILDHOOD_TRAUMA', 'DIAGNOSIS_CPTSD', 'DIAGNOSIS_SEXUAL_TRAUMA',\n",
    "    'DIAGNOSIS_SUICIDALITY', 'age',\n",
    "    'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"Bipolar_and_Mood_disorder\", \"ethnicity_Dutch\", \"ethnicity_other\"\n",
    "]\n",
    "\n",
    "covariates_SubSubCat_Lorazepam = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline',\n",
    "    'CAT_Antidepressiva', 'CAT_Antipsychotica',\n",
    "    'DIAGNOSIS_CHILDHOOD_TRAUMA', 'DIAGNOSIS_CPTSD', 'DIAGNOSIS_SEXUAL_TRAUMA',\n",
    "    'DIAGNOSIS_SUICIDALITY', 'age',\n",
    "    'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"Bipolar_and_Mood_disorder\", \"ethnicity_Dutch\", \"ethnicity_other\"\n",
    "]\n",
    "\n",
    "covariates_SubSubCat_Mirtazapine = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline',\n",
    "    'CAT_Antipsychotica',\n",
    "    'CAT_Benzodiazepine',\n",
    "    'DIAGNOSIS_CHILDHOOD_TRAUMA', 'DIAGNOSIS_CPTSD', 'DIAGNOSIS_SEXUAL_TRAUMA',\n",
    "    'DIAGNOSIS_SUICIDALITY', 'age',\n",
    "    'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"Bipolar_and_Mood_disorder\", \"ethnicity_Dutch\", \"ethnicity_other\"\n",
    "]\n",
    "\n",
    "covariates_SubSubCat_Escitalopram = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline',\n",
    "    'CAT_Antipsychotica',\n",
    "    'CAT_Benzodiazepine',\n",
    "    'DIAGNOSIS_CHILDHOOD_TRAUMA', 'DIAGNOSIS_CPTSD', 'DIAGNOSIS_SEXUAL_TRAUMA',\n",
    "    'DIAGNOSIS_SUICIDALITY', 'age',\n",
    "    'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"Bipolar_and_Mood_disorder\", \"ethnicity_Dutch\", \"ethnicity_other\"\n",
    "]\n",
    "\n",
    "\n",
    "covariates_SubSubCat_Sertraline = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline',\n",
    "    'CAT_Antipsychotica',\n",
    "    'CAT_Benzodiazepine',\n",
    "    'DIAGNOSIS_CHILDHOOD_TRAUMA', 'DIAGNOSIS_CPTSD', 'DIAGNOSIS_SEXUAL_TRAUMA',\n",
    "    'DIAGNOSIS_SUICIDALITY', 'age',\n",
    "    'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"Bipolar_and_Mood_disorder\", \"ethnicity_Dutch\", \"ethnicity_other\"\n",
    "]\n",
    "\n",
    "covariates_SubSubCat_Temazepam = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline',\n",
    "    'CAT_Antidepressiva', 'CAT_Antipsychotica',\n",
    "    'DIAGNOSIS_CHILDHOOD_TRAUMA', 'DIAGNOSIS_CPTSD', 'DIAGNOSIS_SEXUAL_TRAUMA',\n",
    "    'DIAGNOSIS_SUICIDALITY', 'age',\n",
    "    'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"Bipolar_and_Mood_disorder\", \"ethnicity_Dutch\", \"ethnicity_other\"\n",
    "]\n",
    "\n",
    "covariates_SubSubCat_Citalopram = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline',\n",
    "    'CAT_Antipsychotica',\n",
    "    'CAT_Benzodiazepine',\n",
    "    'DIAGNOSIS_CHILDHOOD_TRAUMA', 'DIAGNOSIS_CPTSD', 'DIAGNOSIS_SEXUAL_TRAUMA',\n",
    "    'DIAGNOSIS_SUICIDALITY', 'age',\n",
    "    'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"Bipolar_and_Mood_disorder\", \"ethnicity_Dutch\", \"ethnicity_other\"\n",
    "]\n",
    "\n",
    "covariates_SubSubCat_Quetiapine = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline',\n",
    "    'CAT_Antidepressiva',\n",
    "    'CAT_Benzodiazepine',\n",
    "    'DIAGNOSIS_CHILDHOOD_TRAUMA', 'DIAGNOSIS_CPTSD', 'DIAGNOSIS_SEXUAL_TRAUMA',\n",
    "    'DIAGNOSIS_SUICIDALITY', 'age',\n",
    "    'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"Bipolar_and_Mood_disorder\", \"ethnicity_Dutch\", \"ethnicity_other\"\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "covariates_SubSubCat_Amitriptyline = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline',\n",
    "    'CAT_Antipsychotica',\n",
    "    'CAT_Benzodiazepine',\n",
    "    'DIAGNOSIS_CHILDHOOD_TRAUMA', 'DIAGNOSIS_CPTSD', 'DIAGNOSIS_SEXUAL_TRAUMA',\n",
    "    'DIAGNOSIS_SUICIDALITY', 'age',\n",
    "    'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"Bipolar_and_Mood_disorder\", \"ethnicity_Dutch\", \"ethnicity_other\"\n",
    "]\n",
    "\n",
    "\n",
    "covariates_SubSubCat_Venlafaxine = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline',\n",
    "    'CAT_Antipsychotica',\n",
    "    'CAT_Benzodiazepine',\n",
    "    'DIAGNOSIS_CHILDHOOD_TRAUMA', 'DIAGNOSIS_CPTSD', 'DIAGNOSIS_SEXUAL_TRAUMA',\n",
    "    'DIAGNOSIS_SUICIDALITY', 'age',\n",
    "    'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"Bipolar_and_Mood_disorder\", \"ethnicity_Dutch\", \"ethnicity_other\"\n",
    "]\n",
    "\n",
    "covariates_SubSubCat_Fluoxetine = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline',\n",
    "    'CAT_Antipsychotica',\n",
    "    'CAT_Benzodiazepine',\n",
    "    'DIAGNOSIS_CHILDHOOD_TRAUMA', 'DIAGNOSIS_CPTSD', 'DIAGNOSIS_SEXUAL_TRAUMA',\n",
    "    'DIAGNOSIS_SUICIDALITY', 'age',\n",
    "    'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"Bipolar_and_Mood_disorder\", \"ethnicity_Dutch\", \"ethnicity_other\"\n",
    "]\n",
    "\n",
    "covariates_SubSubCat_Topiramaat = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline',\n",
    "    'CAT_Antidepressiva',\n",
    "    'CAT_Antipsychotica',\n",
    "    'CAT_Benzodiazepine',\n",
    "    'DIAGNOSIS_CHILDHOOD_TRAUMA', 'DIAGNOSIS_CPTSD', 'DIAGNOSIS_SEXUAL_TRAUMA',\n",
    "    'DIAGNOSIS_SUICIDALITY', 'age',\n",
    "    'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"Bipolar_and_Mood_disorder\", \"ethnicity_Dutch\", \"ethnicity_other\"\n",
    "]\n",
    "\n",
    "covariates_SubSubCat_Tramadol = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline',\n",
    "    'CAT_Antidepressiva', 'CAT_Antipsychotica', 'CAT_Benzodiazepine',\n",
    "    'DIAGNOSIS_CHILDHOOD_TRAUMA', 'DIAGNOSIS_CPTSD', 'DIAGNOSIS_SEXUAL_TRAUMA',\n",
    "    'DIAGNOSIS_SUICIDALITY', 'age',\n",
    "    'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"Bipolar_and_Mood_disorder\", \"ethnicity_Dutch\", \"ethnicity_other\"\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "covariates_SubSubCat_Zopiclon = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline',\n",
    "    'CAT_Antidepressiva',\n",
    "    'CAT_Antipsychotica',\n",
    "    'CAT_Benzodiazepine',\n",
    "    'DIAGNOSIS_CHILDHOOD_TRAUMA', 'DIAGNOSIS_CPTSD', 'DIAGNOSIS_SEXUAL_TRAUMA',\n",
    "    'DIAGNOSIS_SUICIDALITY', 'age',\n",
    "    'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"Bipolar_and_Mood_disorder\", \"ethnicity_Dutch\", \"ethnicity_other\"\n",
    "]\n",
    "\n",
    "covariates_SubSubCat_Loprazolam = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline',\n",
    "    'CAT_Antidepressiva', 'CAT_Antipsychotica',\n",
    "    'DIAGNOSIS_CHILDHOOD_TRAUMA', 'DIAGNOSIS_CPTSD', 'DIAGNOSIS_SEXUAL_TRAUMA',\n",
    "    'DIAGNOSIS_SUICIDALITY', 'age',\n",
    "    'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"Bipolar_and_Mood_disorder\", \"ethnicity_Dutch\", \"ethnicity_other\"\n",
    "]\n",
    "\n",
    "covariates_SubSubCat_Alprazolam = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline',\n",
    "    'CAT_Antidepressiva', 'CAT_Antipsychotica',\n",
    "    'DIAGNOSIS_CHILDHOOD_TRAUMA', 'DIAGNOSIS_CPTSD', 'DIAGNOSIS_SEXUAL_TRAUMA',\n",
    "    'DIAGNOSIS_SUICIDALITY', 'age',\n",
    "    'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"Bipolar_and_Mood_disorder\", \"ethnicity_Dutch\", \"ethnicity_other\"\n",
    "]\n",
    "\n",
    "covariates_SubSubCat_promethazine = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline',\n",
    "    'CAT_Antidepressiva', 'CAT_Antipsychotica',\n",
    "    'CAT_Benzodiazepine',\n",
    "    'DIAGNOSIS_CHILDHOOD_TRAUMA', 'DIAGNOSIS_CPTSD', 'DIAGNOSIS_SEXUAL_TRAUMA',\n",
    "    'DIAGNOSIS_SUICIDALITY', 'age',\n",
    "    'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"Bipolar_and_Mood_disorder\", \"ethnicity_Dutch\", \"ethnicity_other\"\n",
    "]\n",
    "\n",
    "covariates_SubSubCat_Paroxetine = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline',\n",
    "    'CAT_Antipsychotica',\n",
    "    'CAT_Benzodiazepine',\n",
    "    'DIAGNOSIS_CHILDHOOD_TRAUMA', 'DIAGNOSIS_CPTSD', 'DIAGNOSIS_SEXUAL_TRAUMA',\n",
    "    'DIAGNOSIS_SUICIDALITY', 'age',\n",
    "    'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"Bipolar_and_Mood_disorder\", \"ethnicity_Dutch\", \"ethnicity_other\"\n",
    "]\n",
    "\n",
    "\n",
    "covariates_SubSubCat_Bupropion = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline',\n",
    "    'CAT_Antipsychotica',\n",
    "    'CAT_Benzodiazepine',\n",
    "    'DIAGNOSIS_CHILDHOOD_TRAUMA', 'DIAGNOSIS_CPTSD', 'DIAGNOSIS_SEXUAL_TRAUMA',\n",
    "    'DIAGNOSIS_SUICIDALITY', 'age',\n",
    "    'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"Bipolar_and_Mood_disorder\", \"ethnicity_Dutch\", \"ethnicity_other\"\n",
    "]\n",
    "\n",
    "covariates_SubSubCat_Methylfenidaat = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline',\n",
    "    'CAT_Antidepressiva',\n",
    "    'CAT_Antipsychotica',\n",
    "    'CAT_Benzodiazepine',\n",
    "    'DIAGNOSIS_CHILDHOOD_TRAUMA', 'DIAGNOSIS_CPTSD',\n",
    "    'DIAGNOSIS_SEXUAL_TRAUMA', 'DIAGNOSIS_SUICIDALITY',\n",
    "    'age',\n",
    "    'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"Bipolar_and_Mood_disorder\", \"ethnicity_Dutch\", \"ethnicity_other\"\n",
    "]\n",
    "\n",
    "covariates_SubSubCat_Olanzapine = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline',\n",
    "    'CAT_Antidepressiva',\n",
    "    'CAT_Benzodiazepine',\n",
    "    'DIAGNOSIS_CHILDHOOD_TRAUMA', 'DIAGNOSIS_CPTSD', 'DIAGNOSIS_SEXUAL_TRAUMA',\n",
    "    'DIAGNOSIS_SUICIDALITY', 'age',\n",
    "    'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"Bipolar_and_Mood_disorder\", \"ethnicity_Dutch\", \"ethnicity_other\"\n",
    "]\n",
    "\n",
    "\n",
    "covariates_SubSubCat_Zolpidem = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline',\n",
    "    'CAT_Antidepressiva', 'CAT_Antipsychotica',\n",
    "    'CAT_Benzodiazepine',\n",
    "    'DIAGNOSIS_CHILDHOOD_TRAUMA', 'DIAGNOSIS_CPTSD', 'DIAGNOSIS_SEXUAL_TRAUMA',\n",
    "    'DIAGNOSIS_SUICIDALITY', 'age',\n",
    "    'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"Bipolar_and_Mood_disorder\", \"ethnicity_Dutch\", \"ethnicity_other\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96d7337-d69e-4554-8337-57adf4d3c977",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# This finds all variables that start with covariates_SUbSubCAT_ or covariates_SubSubcat_\n",
    "final_covariates_map = defaultdict(list)\n",
    "final_covariates_map.update({\n",
    "    var.replace(\"covariates_\", \"\"): val\n",
    "    for var, val in globals().items()\n",
    "    if var.lower().startswith(\"covariates_subsubcat_\") and isinstance(val, list)\n",
    "})\n",
    "\n",
    "# Show detected group names\n",
    "print(\"Groups found:\", list(final_covariates_map.keys()))\n",
    "medication_groups = list(final_covariates_map.keys())\n",
    "print(medication_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0640ebd5-cfed-4ddb-a643-ec647517528a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "def run_all_SUBSUBCAT_group_models(imputed_dfs):\n",
    "    \"\"\"\n",
    "    Runs downstream analysis for each SUBSUBCAT medisubsubcation group using imputed datasets.\n",
    "\n",
    "    Parameters:\n",
    "    - imputed_dfs: list of 5 imputed DataFrames (from df_imputed_final_imp1.pkl ... imp5.pkl)\n",
    "    \n",
    "    Notes:\n",
    "    - Covariate lists must be defined as global variables: covariates_subsubcat_<group>\n",
    "    - Outputs are saved in: outputs/SUBSUBCAT_<GROUP>/\n",
    "    \"\"\"\n",
    "\n",
    "    print(\" Starting analysis for all SUBSUBCAT groups\")\n",
    "\n",
    "    for var_name in globals():\n",
    "        if var_name.lower().startswith(\"covariates_subsubcat_\") and isinstance(globals()[var_name], list):\n",
    "            group_name = var_name.replace(\"covariates_\", \"\")\n",
    "            group_name = group_name.replace(\"_\", \" \").title().replace(\" \", \"_\")  # e.g., subsubcat_z_drugs → Subsubcat_Z_Drugs\n",
    "            group_name = group_name.replace(\"Subsubcat_\", \"SUBSUBCAT_\")  # force prefix to uppercase\n",
    "\n",
    "            covariates = globals()[var_name]\n",
    "            output_dir = f\"outputs/{group_name}\"\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "            print(f\"\\n Processing {group_name}...\")\n",
    "\n",
    "            for k, df_imp in enumerate(imputed_dfs):\n",
    "                print(f\"  → Using imputation {k+1}\")\n",
    "\n",
    "                # Define X and Y\n",
    "                X = df_imp[covariates]\n",
    "                Y = df_imp[\"caps5_change_baseline\"]\n",
    "\n",
    "                # === Save X and Y as placeholder (replace with modeling later)\n",
    "                X.to_csv(f\"{output_dir}/X_imp{k+1}.csv\", index=False)\n",
    "                Y.to_frame(name=\"Y\").to_csv(f\"{output_dir}/Y_imp{k+1}.csv\", index=False)\n",
    "\n",
    "            print(f\" Done: {group_name}\")\n",
    "\n",
    "    print(\"\\n All SUBSUBCAT group analyses complete.\")\n",
    "\n",
    "# ========= STEP 4: Execute ========= #\n",
    "run_all_SUBSUBCAT_group_models(imputed_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77713ae-0355-4e4f-9a8c-910b53ed4871",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for treatment_var in medication_groups:\n",
    "    print(f\"\\n {treatment_var}\")\n",
    "    \n",
    "    for i, df in enumerate(imputed_dfs):\n",
    "        if treatment_var not in df.columns:\n",
    "            print(f\"  Imp {i+1}:  Not found in columns.\")\n",
    "            continue\n",
    "\n",
    "        treated = (df[treatment_var] == 1).sum()\n",
    "        control = (df[treatment_var] == 0).sum()\n",
    "        missing = df[treatment_var].isna().sum()\n",
    "\n",
    "        print(f\"  Imp {i+1}: Treated = {treated}, Control = {control}, Missing = {missing}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34e2dcc-a576-4532-bbb3-69af4010257b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from collections import defaultdict\n",
    "\n",
    "# ✅ VIF computation function\n",
    "def compute_vif(X):\n",
    "    X = sm.add_constant(X, has_constant='add')\n",
    "    vif_df = pd.DataFrame()\n",
    "    vif_df[\"variable\"] = X.columns\n",
    "    vif_df[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "    return vif_df\n",
    "\n",
    "# ✅ Process each group\n",
    "for group in medication_groups:\n",
    "    print(f\"\\n🔍 Processing VIF for {group}\")\n",
    "\n",
    "    if group not in final_covariates_map:\n",
    "        print(f\" ⚠️ No covariates found for {group}. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    covariates = final_covariates_map[group]\n",
    "    vif_list = []\n",
    "\n",
    "    for i, df_imp in enumerate(imputed_dfs):\n",
    "        try:\n",
    "            X = df_imp[covariates].copy()\n",
    "            vif_df = compute_vif(X)\n",
    "            vif_df[\"imputation\"] = i + 1\n",
    "            vif_list.append(vif_df)\n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ Failed on imputation {i+1} for {group}: {e}\")\n",
    "\n",
    "    if vif_list:\n",
    "        all_vif = pd.concat(vif_list)\n",
    "        pooled_vif = all_vif.groupby(\"variable\")[\"VIF\"].mean().reset_index()\n",
    "        pooled_vif = pooled_vif.sort_values(by=\"VIF\", ascending=False)\n",
    "\n",
    "        output_folder = os.path.join(\"outputs\", group)\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "        pooled_vif.to_csv(os.path.join(output_folder, \"pooled_vif.csv\"), index=False)\n",
    "\n",
    "        print(f\" ✅ Saved: {output_folder}/pooled_vif.csv\")\n",
    "    else:\n",
    "        print(f\" ⚠️ Skipped {group}: No valid imputations.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8d4450-4832-4ef7-b6d2-d897899a857b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ---------- PS Estimation Function ----------\n",
    "def run_xgboost_ps_modeling(imputed_dfs, medication_groups, final_covariates_map):\n",
    "    for group in medication_groups:\n",
    "        print(f\" Running PS estimation for {group}\")\n",
    "\n",
    "        if group not in final_covariates_map:\n",
    "            print(f\" No covariates found for {group}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        output_folder = os.path.join(\"outputs\", group)\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "        covariates = final_covariates_map[group]\n",
    "        ps_matrix = pd.DataFrame()\n",
    "        auc_list = []\n",
    "\n",
    "        for i, df_imp in enumerate(imputed_dfs):\n",
    "            if group not in df_imp.columns:\n",
    "                print(f\" {group} not found in imputed dataset {i+1}. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            X = df_imp[covariates].copy()\n",
    "            T = df_imp[group]\n",
    "\n",
    "            # Drop missing treatment rows\n",
    "            valid_idx = T.dropna().index\n",
    "            X = X.loc[valid_idx]\n",
    "            T = T.loc[valid_idx]\n",
    "\n",
    "            # Train-test split for ROC\n",
    "            X_train, X_test, T_train, T_test = train_test_split(\n",
    "                X, T, stratify=T, test_size=0.3, random_state=42\n",
    "            )\n",
    "\n",
    "            try:\n",
    "                model = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "                model.fit(X_train, T_train)\n",
    "\n",
    "                ps_scores = model.predict_proba(X)[:, 1]\n",
    "                ps_matrix[f\"ps_imp{i+1}\"] = pd.Series(ps_scores, index=valid_idx)\n",
    "\n",
    "                # ROC & AUC\n",
    "                auc = roc_auc_score(T_test, model.predict_proba(X_test)[:, 1])\n",
    "                auc_list.append(auc)\n",
    "\n",
    "                fpr, tpr, _ = roc_curve(T_test, model.predict_proba(X_test)[:, 1])\n",
    "                plt.figure()\n",
    "                plt.plot(fpr, tpr, label=f\"AUC = {auc:.3f}\")\n",
    "                plt.plot([0, 1], [0, 1], 'k--')\n",
    "                plt.xlabel(\"False Positive Rate\")\n",
    "                plt.ylabel(\"True Positive Rate\")\n",
    "                plt.title(f\"ROC Curve - {group} (Imp {i+1})\")\n",
    "                plt.legend()\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(os.path.join(output_folder, f\"roc_curve_imp{i+1}.png\"))\n",
    "                plt.close()\n",
    "                print(f\"   Imp {i+1}: AUC = {auc:.3f}, ROC saved.\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"   Error in {group} (imp {i+1}): {e}\")\n",
    "\n",
    "        # Save AUCs and Composite PS\n",
    "        if not ps_matrix.empty:\n",
    "            # Fill NaN rows (from dropped subjects in some imputations) with mean\n",
    "            ps_matrix[\"composite_ps\"] = ps_matrix.mean(axis=1)\n",
    "            ps_matrix.to_excel(os.path.join(output_folder, \"propensity_scores.xlsx\"))\n",
    "\n",
    "            auc_df = pd.DataFrame({\n",
    "                \"imputation\": [f\"imp{i+1}\" for i in range(len(auc_list))],\n",
    "                \"AUC\": auc_list\n",
    "            })\n",
    "            auc_df.loc[len(auc_df.index)] = [\"mean\", np.mean(auc_list) if auc_list else np.nan]\n",
    "            auc_df.to_excel(os.path.join(output_folder, \"auc_scores.xlsx\"), index=False)\n",
    "\n",
    "            print(f\" Composite PS + AUC saved for {group}\")\n",
    "        else:\n",
    "            print(f\" No valid PS scores generated for {group}\")\n",
    "\n",
    "# ---------- Run ----------\n",
    "run_xgboost_ps_modeling(imputed_dfs, medication_groups, final_covariates_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72499bbd-b193-4443-8b30-450bd11aa408",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "def compute_feature_importance(imputed_dfs, medication_groups, final_covariates_map):\n",
    "    for group in medication_groups:\n",
    "        print(f\"\\n Computing feature importance for {group}\")\n",
    "\n",
    "        if group not in final_covariates_map:\n",
    "            print(f\" No covariates found for {group}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        covariates = final_covariates_map[group]\n",
    "        output_folder = os.path.join(\"outputs\", group)\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "        importance_df_list = []\n",
    "\n",
    "        for i, df_imp in enumerate(imputed_dfs):\n",
    "            if group not in df_imp.columns:\n",
    "                print(f\" {group} not in imputed dataset {i+1}. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            X = df_imp[covariates].copy()\n",
    "            T = df_imp[group]\n",
    "\n",
    "            # Drop NaNs in treatment\n",
    "            valid_idx = T.dropna().index\n",
    "            X = X.loc[valid_idx]\n",
    "            T = T.loc[valid_idx]\n",
    "\n",
    "            try:\n",
    "                model = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "                model.fit(X, T)\n",
    "\n",
    "                # Get feature importance\n",
    "                importances = model.get_booster().get_score(importance_type='gain')\n",
    "                df_feat = pd.DataFrame.from_dict(importances, orient='index', columns=[f\"imp{i+1}\"])\n",
    "                df_feat.index.name = 'feature'\n",
    "                importance_df_list.append(df_feat)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"   Error during modeling: {e}\")\n",
    "\n",
    "        if importance_df_list:\n",
    "            # Combine and average\n",
    "            all_feat = pd.concat(importance_df_list, axis=1).fillna(0)\n",
    "            all_feat[\"mean_importance\"] = all_feat.mean(axis=1)\n",
    "\n",
    "            # Filter top 30 non-zero\n",
    "            non_zero = all_feat[all_feat[\"mean_importance\"] > 0]\n",
    "            top30 = non_zero.sort_values(by=\"mean_importance\", ascending=False).head(30)\n",
    "\n",
    "            # Save to CSV\n",
    "            top30.to_csv(os.path.join(output_folder, \"feature_importance.csv\"))\n",
    "\n",
    "            # Plot\n",
    "            plt.figure(figsize=(10, 8))\n",
    "            plt.barh(top30.index[::-1], top30[\"mean_importance\"][::-1])  # plot top → bottom\n",
    "            plt.xlabel(\"Mean Gain Importance\")\n",
    "            plt.title(f\"Top 30 Feature Importance - {group}\")\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(output_folder, \"feature_importance_top30.png\"))\n",
    "            plt.close()\n",
    "\n",
    "            print(f\" Saved feature importance plot and CSV for {group}\")\n",
    "        else:\n",
    "            print(f\" No valid models for {group}\")\n",
    "\n",
    "#  Run\n",
    "compute_feature_importance(imputed_dfs, medication_groups, final_covariates_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f765c036-655f-4b4b-abee-ae90a001eb24",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def compute_trimmed_clipped_iptw(ps_df, treatment, lower=0.05, upper=0.95, clip_max=10):\n",
    "    weights = []\n",
    "    keep_mask = (ps_df > lower) & (ps_df < upper)\n",
    "\n",
    "    for i in range(ps_df.shape[1]):\n",
    "        ps = ps_df.iloc[:, i].clip(lower=1e-6, upper=1 - 1e-6)  # avoid div by zero\n",
    "        mask = keep_mask.iloc[:, i]\n",
    "        w = pd.Series(np.nan, index=ps.index)\n",
    "\n",
    "        w[mask & (treatment == 1)] = 1 / ps[mask & (treatment == 1)]\n",
    "        w[mask & (treatment == 0)] = 1 / (1 - ps[mask & (treatment == 0)])\n",
    "        w = w.clip(upper=clip_max)\n",
    "        weights.append(w)\n",
    "\n",
    "    return pd.concat(weights, axis=1)\n",
    "\n",
    "\n",
    "def apply_rubins_rule_to_iptw(iptw_matrix):\n",
    "    \"\"\"\n",
    "    Given an IPTW matrix (n rows × M imputations), return Rubin’s rule pooled mean, SD, SE.\n",
    "    \"\"\"\n",
    "    M = iptw_matrix.shape[1]\n",
    "    q_bar = iptw_matrix.mean(axis=1)\n",
    "    u_bar = iptw_matrix.var(axis=1, ddof=1)\n",
    "    B = iptw_matrix.apply(lambda x: x.mean(), axis=1).var(ddof=1)\n",
    "    total_var = u_bar + (1 + 1/M) * B\n",
    "    total_se = np.sqrt(total_var)\n",
    "    return q_bar, u_bar.pow(0.5), total_se\n",
    "\n",
    "\n",
    "def run_trim_clip_save_all(imputed_dfs, medication_groups, final_covariates_map):\n",
    "    for group in medication_groups:\n",
    "        print(f\"\\n🔍 Processing IPTW + trimming + clipping for {group}\")\n",
    "        output_folder = os.path.join(\"outputs\", group)\n",
    "        ps_path = os.path.join(output_folder, \"propensity_scores.xlsx\")\n",
    "\n",
    "        if not os.path.exists(ps_path):\n",
    "            print(f\"⚠️ Missing PS file: {ps_path}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            ps_all = pd.read_excel(ps_path, index_col=0)\n",
    "            ps_cols = [col for col in ps_all.columns if col.startswith(\"ps_imp\")]\n",
    "            composite_index = ps_all.index\n",
    "\n",
    "            # Get treatment from one imputed dataset\n",
    "            T_full = None\n",
    "            for df in imputed_dfs:\n",
    "                if group in df.columns:\n",
    "                    T_full = df.loc[composite_index, group]\n",
    "                    break\n",
    "\n",
    "            if T_full is None:\n",
    "                print(f\"❌ Treatment column {group} not found in any imputed dataset.\")\n",
    "                continue\n",
    "\n",
    "            # Compute IPTW matrix (shape: n × M)\n",
    "            iptw_matrix = compute_trimmed_clipped_iptw(ps_all[ps_cols], T_full)\n",
    "            iptw_matrix.columns = [f\"iptw_imp{i+1}\" for i in range(iptw_matrix.shape[1])]\n",
    "\n",
    "            # Apply Rubin’s Rule for mean, SD, SE\n",
    "            iptw_matrix[\"iptw_mean\"], iptw_matrix[\"iptw_sd\"], iptw_matrix[\"iptw_se\"] = apply_rubins_rule_to_iptw(\n",
    "                iptw_matrix[[f\"iptw_imp{i+1}\" for i in range(iptw_matrix.shape[1])]]\n",
    "            )\n",
    "\n",
    "            # Save IPTW matrix separately\n",
    "            iptw_matrix.to_excel(os.path.join(output_folder, \"iptw_weights.xlsx\"))\n",
    "            print(f\"✅ Saved IPTW weights for {group}\")\n",
    "\n",
    "            # Save trimmed & clipped imputed datasets with IPTW\n",
    "            for i in range(5):\n",
    "                df = imputed_dfs[i].copy()\n",
    "                if group not in df.columns:\n",
    "                    continue\n",
    "\n",
    "                trimmed_idx = iptw_matrix.index.intersection(df.index)\n",
    "                needed_cols = final_covariates_map[group] + [group, \"caps5_change_baseline\"]\n",
    "\n",
    "                # Select only necessary columns\n",
    "                df_trimmed = df.loc[trimmed_idx, needed_cols].copy()\n",
    "                df_trimmed[\"iptw\"] = iptw_matrix[f\"iptw_imp{i+1}\"].loc[trimmed_idx]\n",
    "\n",
    "                # ✅ DROP rows with missing IPTW values\n",
    "                before = len(df_trimmed)\n",
    "                df_trimmed = df_trimmed.dropna(subset=[\"iptw\"])\n",
    "                after = len(df_trimmed)\n",
    "                print(f\"    ℹ️ Retained {after}/{before} rows after IPTW NaN drop.\")\n",
    "\n",
    "                # Save to .pkl\n",
    "                df_trimmed.to_pickle(os.path.join(output_folder, f\"trimmed_data_imp{i+1}.pkl\"))\n",
    "                print(f\"  💾 Saved trimmed dataset: {output_folder}/trimmed_data_imp{i+1}.*\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error in {group}: {e}\")\n",
    "\n",
    "\n",
    "run_trim_clip_save_all(imputed_dfs, medication_groups, final_covariates_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268b3d7e-ac3b-4300-9aeb-6089727c4787",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_ps_overlap_all_groups(medication_groups):\n",
    "    for group in medication_groups:\n",
    "        print(f\"\\n📊 Plotting PS overlap for {group}\")\n",
    "\n",
    "        folder = os.path.join(\"outputs\", group)\n",
    "        ps_file = os.path.join(folder, \"propensity_scores.xlsx\")\n",
    "        iptw_file = os.path.join(folder, \"iptw_weights.xlsx\")\n",
    "        trimmed_file = os.path.join(folder, \"trimmed_data_imp1.pkl\")\n",
    "\n",
    "        if not all(os.path.exists(f) for f in [ps_file, iptw_file, trimmed_file]):\n",
    "            print(f\"⚠️ Missing required files for {group}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            ps_df = pd.read_excel(ps_file, index_col=0)\n",
    "            iptw_df = pd.read_excel(iptw_file, index_col=0)\n",
    "            trimmed_df = pd.read_pickle(trimmed_file)\n",
    "\n",
    "            # Extract\n",
    "            ps = ps_df[\"composite_ps\"].reindex(trimmed_df.index)\n",
    "            w = iptw_df[\"iptw_mean\"].reindex(trimmed_df.index)\n",
    "            T = trimmed_df[group]\n",
    "\n",
    "            # Masks to remove NaNs\n",
    "            treated_mask = (T == 1) & ps.notna() & w.notna()\n",
    "            control_mask = (T == 0) & ps.notna() & w.notna()\n",
    "\n",
    "            treated = ps[treated_mask]\n",
    "            treated_w = w[treated_mask]\n",
    "\n",
    "            control = ps[control_mask]\n",
    "            control_w = w[control_mask]\n",
    "\n",
    "            # === Unweighted Plot ===\n",
    "            plt.figure(figsize=(8, 5))\n",
    "            plt.hist([treated, control], bins=25, label=[\"Treated\", \"Control\"], alpha=0.6)\n",
    "            plt.title(f\"Unweighted PS Overlap - {group}\")\n",
    "            plt.xlabel(\"Composite Propensity Score\")\n",
    "            plt.ylabel(\"Count\")\n",
    "            plt.legend()\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(folder, \"ps_overlap_unweighted.png\"))\n",
    "            plt.close()\n",
    "\n",
    "            # === Weighted Plot ===\n",
    "            plt.figure(figsize=(8, 5))\n",
    "            plt.hist([treated, control], bins=25, weights=[treated_w, control_w], label=[\"Treated\", \"Control\"], alpha=0.6)\n",
    "            plt.title(f\"Weighted PS Overlap - {group}\")\n",
    "            plt.xlabel(\"Composite Propensity Score\")\n",
    "            plt.ylabel(\"Weighted Count\")\n",
    "            plt.legend()\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(folder, \"ps_overlap_weighted.png\"))\n",
    "            plt.close()\n",
    "\n",
    "            print(f\"✅ Saved unweighted and weighted PS plots for {group}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error processing {group}: {e}\")\n",
    "\n",
    "# 🔁 Run\n",
    "plot_ps_overlap_all_groups(medication_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd15f13-2739-42d0-9532-0904b6f03336",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set up base output folder\n",
    "output_base = \"outputs\"\n",
    "ps_file = \"propensity_scores.xlsx\"\n",
    "iptw_file = \"iptw_weights.xlsx\"\n",
    "trimmed_data_file = \"trimmed_data_imp1.pkl\"\n",
    "\n",
    "# Collect all treatment group folders\n",
    "groups = [g for g in medication_groups if os.path.isdir(os.path.join(output_base, g))]\n",
    "\n",
    "# Generate 4-panel overlap plots\n",
    "for group in groups:\n",
    "    group_path = os.path.join(output_base, group)\n",
    "    try:\n",
    "        # Load trimmed treatment info\n",
    "        trimmed_df = pd.read_pickle(os.path.join(group_path, trimmed_data_file))\n",
    "        index = trimmed_df.index\n",
    "\n",
    "        # Fix: case-insensitive match for treatment variable\n",
    "        possible_cols = [col for col in trimmed_df.columns if col.upper() == group.upper()]\n",
    "        if not possible_cols:\n",
    "            print(f\" Treatment variable {group} not found in {group}, skipping.\")\n",
    "            continue\n",
    "        treatment_var = possible_cols[0]\n",
    "        T = trimmed_df[treatment_var]\n",
    "\n",
    "        # Load composite PS (aligned to trimmed_df index)\n",
    "        ps_df = pd.read_excel(os.path.join(group_path, ps_file), index_col=0)\n",
    "        if 'composite_ps' not in ps_df.columns:\n",
    "            print(f\" Composite column missing in {ps_file}, skipping {group}.\")\n",
    "            continue\n",
    "        ps = ps_df.loc[index, 'composite_ps']\n",
    "\n",
    "        # Load IPTW weights (aligned to trimmed_df index)\n",
    "        weights_df = pd.read_excel(os.path.join(group_path, iptw_file), index_col=0)\n",
    "        if 'iptw_mean' not in weights_df.columns:\n",
    "            print(f\" IPTW weight column missing in {iptw_file}, skipping {group}.\")\n",
    "            continue\n",
    "        weights = weights_df.loc[index, 'iptw_mean']\n",
    "\n",
    "        # Prepare 4 datasets\n",
    "        raw_treated = ps[T == 1]\n",
    "        raw_control = ps[T == 0]\n",
    "        weighted_treated = (ps[T == 1], weights[T == 1])\n",
    "        weighted_control = (ps[T == 0], weights[T == 0])\n",
    "\n",
    "        # Create plot\n",
    "        fig, axs = plt.subplots(2, 2, figsize=(12, 8))\n",
    "        fig.suptitle(f\"Propensity Score Distribution - {group}\", fontsize=14)\n",
    "\n",
    "        axs[0, 0].hist(raw_treated, bins=20, alpha=0.7, color='blue')\n",
    "        axs[0, 0].set_title(\"Raw Treated\")\n",
    "\n",
    "        axs[0, 1].hist(raw_control, bins=20, alpha=0.7, color='green')\n",
    "        axs[0, 1].set_title(\"Raw Control\")\n",
    "\n",
    "        axs[1, 0].hist(weighted_treated[0], bins=20, weights=weighted_treated[1], alpha=0.7, color='blue')\n",
    "        axs[1, 0].set_title(\"Weighted Treated\")\n",
    "\n",
    "        axs[1, 1].hist(weighted_control[0], bins=20, weights=weighted_control[1], alpha=0.7, color='green')\n",
    "        axs[1, 1].set_title(\"Weighted Control\")\n",
    "\n",
    "        for ax in axs.flat:\n",
    "            ax.set_xlim(0, 1)\n",
    "            ax.set_xlabel(\"Propensity Score\")\n",
    "            ax.set_ylabel(\"Count\")\n",
    "\n",
    "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "\n",
    "        # Save figure\n",
    "        plot_path = os.path.join(group_path, f\"four_panel_overlap_{group}.png\")\n",
    "        plt.savefig(plot_path)\n",
    "        plt.close()\n",
    "        print(f\" ✅ Saved: {plot_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\" ❌ Error in {group}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e9e7aa-87a4-4cba-bfcd-245fd505fec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ATT calculation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ecf92b-090c-4b5f-8a78-cc19d881e434",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weighted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fa2d03-12f9-4dc4-b4d1-06ce5b1731cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import KFold\n",
    "from econml.dml import LinearDML\n",
    "from scipy.stats import t, probplot\n",
    "import xgboost as xgb\n",
    "\n",
    "# -----------------------------\n",
    "# Configuration\n",
    "# -----------------------------\n",
    "n_repeats = 4\n",
    "seeds = list(range(1, 11))\n",
    "imputations = 5\n",
    "output_folder = \"outputs\"\n",
    "\n",
    "# -----------------------------\n",
    "# Diagnostic Plotting Function\n",
    "# -----------------------------\n",
    "def create_diagnostic_plots(residuals_data, fitted_data, group_name):\n",
    "    \"\"\"Create 4 diagnostic plots for model validation\"\"\"\n",
    "    plots_dir = os.path.join(output_folder, \"plots\")\n",
    "    os.makedirs(plots_dir, exist_ok=True)\n",
    "    \n",
    "    # Flatten the collected data\n",
    "    all_residuals = np.concatenate(residuals_data)\n",
    "    all_fitted = np.concatenate(fitted_data)\n",
    "    \n",
    "    # Create figure with 2x2 subplots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    fig.suptitle(f'Diagnostic Plots - {group_name}', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Residuals vs Fitted\n",
    "    axes[0,0].scatter(all_fitted, all_residuals, alpha=0.6, s=20)\n",
    "    axes[0,0].axhline(y=0, color='red', linestyle='--', alpha=0.8)\n",
    "    axes[0,0].set_xlabel('Fitted Values')\n",
    "    axes[0,0].set_ylabel('Residuals')\n",
    "    axes[0,0].set_title('Residuals vs Fitted')\n",
    "    axes[0,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. QQ Plot\n",
    "    probplot(all_residuals, dist=\"norm\", plot=axes[0,1])\n",
    "    axes[0,1].set_title('Q-Q Plot (Normal)')\n",
    "    axes[0,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Residual Histogram\n",
    "    axes[1,0].hist(all_residuals, bins=30, density=True, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    axes[1,0].set_xlabel('Residuals')\n",
    "    axes[1,0].set_ylabel('Density')\n",
    "    axes[1,0].set_title('Residual Distribution')\n",
    "    axes[1,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Scale-Location Plot\n",
    "    sqrt_abs_residuals = np.sqrt(np.abs(all_residuals))\n",
    "    axes[1,1].scatter(all_fitted, sqrt_abs_residuals, alpha=0.6, s=20)\n",
    "    axes[1,1].set_xlabel('Fitted Values')\n",
    "    axes[1,1].set_ylabel('√|Residuals|')\n",
    "    axes[1,1].set_title('Scale-Location Plot')\n",
    "    axes[1,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save plot\n",
    "    plot_filename = os.path.join(plots_dir, f'{group_name}.png')\n",
    "    plt.savefig(plot_filename, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"📊 Diagnostic plots saved for {group_name}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Rubin's Rule\n",
    "# -----------------------------\n",
    "def rubins_pool(estimates, ses):\n",
    "    m = len(estimates)\n",
    "    q_bar = np.mean(estimates)\n",
    "    u_bar = np.mean(np.square(ses))\n",
    "    b_m = np.var(estimates, ddof=1)\n",
    "    total_var = u_bar + ((1 + 1/m) * b_m)\n",
    "    total_se = np.sqrt(total_var)\n",
    "    ci_lower = q_bar - 1.96 * total_se\n",
    "    ci_upper = q_bar + 1.96 * total_se\n",
    "    p_value = 2 * (1 - t.cdf(np.abs(q_bar / total_se), df=m-1))\n",
    "    rounded_p = round(p_value, 5)\n",
    "    formatted_p = \"< 0.00001\" if rounded_p <= 0.00001 else f\"{rounded_p:.5f}\"\n",
    "    return q_bar, total_se, ci_lower, ci_upper, formatted_p\n",
    "\n",
    "# -----------------------------\n",
    "# SMD + Variance Ratio\n",
    "# -----------------------------\n",
    "def calculate_smd_vr(X, T, weights):\n",
    "    treated = X[T == 1]\n",
    "    control = X[T == 0]\n",
    "    w_treated = weights[T == 1]\n",
    "    w_control = weights[T == 0]\n",
    "    smd, vr = [], []\n",
    "    for col in X.columns:\n",
    "        try:\n",
    "            m1, m0 = np.average(treated[col], weights=w_treated), np.average(control[col], weights=w_control)\n",
    "            s1 = np.sqrt(np.average((treated[col] - m1) ** 2, weights=w_treated))\n",
    "            s0 = np.sqrt(np.average((control[col] - m0) ** 2, weights=w_control))\n",
    "            pooled_sd = np.sqrt((s1 ** 2 + s0 ** 2) / 2)\n",
    "            smd.append((m1 - m0) / pooled_sd if pooled_sd > 0 else 0)\n",
    "            vr.append(s1**2 / s0**2 if s0**2 > 0 else 0)\n",
    "        except Exception:\n",
    "            smd.append(np.nan)\n",
    "            vr.append(np.nan)\n",
    "    return np.nanmean(smd), np.nanmean(vr)\n",
    "\n",
    "# -----------------------------\n",
    "# DML Main Loop\n",
    "# -----------------------------\n",
    "def run_dml_with_trimmed_data(final_covariates_map):\n",
    "    att_results = []\n",
    "    balance_results = []\n",
    "\n",
    "    for group, covariates in final_covariates_map.items():\n",
    "        print(f\"\\n🚀 Running DML for {group}\")\n",
    "        group_dir = os.path.join(output_folder, group)\n",
    "        os.makedirs(group_dir, exist_ok=True)\n",
    "\n",
    "        best_result = None\n",
    "        best_se = float(\"inf\")\n",
    "        \n",
    "        # Initialize lists to collect residuals and fitted values for diagnostic plots\n",
    "        group_residuals = []\n",
    "        group_fitted = []\n",
    "\n",
    "        for seed in seeds:\n",
    "            att_list, se_list, r2_list, rmse_list, smd_list, vr_list = [], [], [], [], [], []\n",
    "\n",
    "            for imp in range(1, imputations + 1):\n",
    "                file_path = os.path.join(group_dir, f\"trimmed_data_imp{imp}.pkl\")\n",
    "                if not os.path.exists(file_path):\n",
    "                    continue\n",
    "\n",
    "                df = pd.read_pickle(file_path)\n",
    "                if group not in df.columns or \"iptw\" not in df.columns:\n",
    "                    continue\n",
    "\n",
    "                X = df[covariates].copy()\n",
    "                T = df[group]\n",
    "                Y = df[\"caps5_change_baseline\"]\n",
    "                W = df[\"iptw\"]\n",
    "\n",
    "                for repeat in range(n_repeats):\n",
    "                    kf = KFold(n_splits=5, shuffle=True, random_state=seed + repeat)\n",
    "                    for train_idx, test_idx in kf.split(X):\n",
    "                        try:\n",
    "                            X_train, T_train, Y_train, W_train = (\n",
    "                                X.iloc[train_idx],\n",
    "                                T.iloc[train_idx],\n",
    "                                Y.iloc[train_idx],\n",
    "                                W.iloc[train_idx],\n",
    "                            )\n",
    "\n",
    "                            model_y = xgb.XGBRegressor(n_estimators=100, max_depth=3, learning_rate=0.1, n_jobs=1, random_state=seed)\n",
    "                            model_t = xgb.XGBClassifier(n_estimators=100, max_depth=3, learning_rate=0.1, n_jobs=1,\n",
    "                                                        use_label_encoder=False, eval_metric=\"logloss\", random_state=seed)\n",
    "\n",
    "                            dml = LinearDML(model_y=model_y, model_t=model_t, discrete_treatment=True,\n",
    "                                            cv=KFold(n_splits=3, shuffle=True, random_state=seed), random_state=seed)\n",
    "                            dml.fit(Y_train, T_train, X=X_train, sample_weight=W_train)\n",
    "\n",
    "                            tau = dml.effect(X_train)\n",
    "                            att = np.mean(tau)\n",
    "                            influence = tau - att\n",
    "                            se = np.sqrt(np.mean(influence ** 2) / len(tau))\n",
    "\n",
    "                            att_list.append(att)\n",
    "                            se_list.append(se)\n",
    "\n",
    "                            Y_pred = model_y.fit(X_train, Y_train, sample_weight=W_train).predict(X_train)\n",
    "                            residuals = Y_train - Y_pred\n",
    "                            rmse = mean_squared_error(Y_train, Y_pred, squared=False)\n",
    "                            r2 = r2_score(Y_train, Y_pred)\n",
    "                            r2_list.append(r2)\n",
    "                            rmse_list.append(rmse)\n",
    "                            \n",
    "                            # Collect residuals and fitted values for diagnostic plots\n",
    "                            group_residuals.append(residuals.values)\n",
    "                            group_fitted.append(Y_pred)\n",
    "\n",
    "                            smd, vr = calculate_smd_vr(X_train, T_train, W_train)\n",
    "                            smd_list.append(smd)\n",
    "                            vr_list.append(vr)\n",
    "                        except Exception as e:\n",
    "                            print(f\"⚠️ Error in {group}, seed {seed}, imp {imp}, rep {repeat}: {e}\")\n",
    "\n",
    "            if att_list:\n",
    "                att, se, ci_l, ci_u, p_val = rubins_pool(att_list, se_list)\n",
    "                avg_r2 = np.mean(r2_list)\n",
    "                avg_rmse = np.mean(rmse_list)\n",
    "                avg_smd = np.mean(smd_list)\n",
    "                avg_vr = np.mean(vr_list)\n",
    "\n",
    "                balance_results.append({\n",
    "                    \"group\": group, \"seed\": seed, \"smd\": avg_smd, \"vr\": avg_vr\n",
    "                })\n",
    "\n",
    "                if se < best_se:\n",
    "                    best_se = se\n",
    "                    best_result = {\n",
    "                        \"group\": group, \"seed\": seed, \"att\": att, \"se\": se,\n",
    "                        \"ci_lower\": ci_l, \"ci_upper\": ci_u, \"p_value\": p_val,\n",
    "                        \"r2\": avg_r2, \"rmse\": avg_rmse\n",
    "                    }\n",
    "\n",
    "                print(f\"✅ {group} | Seed {seed}: ATT = {att:.4f}, SE = {se:.4f}, p = {p_val}\")\n",
    "            else:\n",
    "                print(f\"⚠️ No valid results for {group} | Seed {seed}\")\n",
    "\n",
    "        # Create diagnostic plots for this group\n",
    "        if group_residuals and group_fitted:\n",
    "            create_diagnostic_plots(group_residuals, group_fitted, group)\n",
    "\n",
    "        if best_result:\n",
    "            att_results.append(best_result)\n",
    "            print(f\"🏆 Best result for {group} → Seed {best_result['seed']} | SE = {best_result['se']:.4f}\")\n",
    "\n",
    "    # Save final output\n",
    "    pd.DataFrame(att_results).to_excel(\"dml_rubin_summary_subsubcats.xlsx\", index=False)\n",
    "    pd.DataFrame(balance_results).to_excel(\"smd_vr_summary_subsubcats.xlsx\", index=False)\n",
    "    print(\"\\n🎯 All summary files saved.\")\n",
    "\n",
    "run_dml_with_trimmed_data(final_covariates_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3e1c00-9abe-425a-972e-c96b06b05f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unweighted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f22f4a-6eb0-41c2-9404-d412340c7710",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import KFold\n",
    "from econml.dml import LinearDML\n",
    "from scipy.stats import t, probplot\n",
    "import xgboost as xgb\n",
    "\n",
    "# -----------------------------\n",
    "# Configuration\n",
    "# -----------------------------\n",
    "n_repeats = 4\n",
    "seeds = list(range(1, 11))\n",
    "imputations = 5\n",
    "output_folder = \"outputs\"\n",
    "\n",
    "# -----------------------------\n",
    "# Plotting Functions\n",
    "# -----------------------------\n",
    "def create_diagnostic_plots(residuals_data, group_name, output_folder):\n",
    "    \"\"\"Create diagnostic plots for each group\"\"\"\n",
    "    plots_dir = os.path.join(output_folder, \"plots\")\n",
    "    os.makedirs(plots_dir, exist_ok=True)\n",
    "    \n",
    "    all_residuals = residuals_data['residuals']\n",
    "    all_fitted = residuals_data['fitted']\n",
    "    \n",
    "    if len(all_residuals) == 0:\n",
    "        return\n",
    "    \n",
    "    # Create figure with subplots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    fig.suptitle(f'Diagnostic Plots - {group_name}', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # 1. Residuals vs Fitted\n",
    "    axes[0, 0].scatter(all_fitted, all_residuals, alpha=0.5, s=1)\n",
    "    axes[0, 0].axhline(y=0, color='red', linestyle='--')\n",
    "    axes[0, 0].set_xlabel('Fitted Values')\n",
    "    axes[0, 0].set_ylabel('Residuals')\n",
    "    axes[0, 0].set_title('Residuals vs Fitted')\n",
    "    \n",
    "    # 2. QQ Plot\n",
    "    probplot(all_residuals, dist=\"norm\", plot=axes[0, 1])\n",
    "    axes[0, 1].set_title('QQ Plot (Normal)')\n",
    "    \n",
    "    # 3. Histogram of Residuals\n",
    "    axes[1, 0].hist(all_residuals, bins=50, alpha=0.7, edgecolor='black')\n",
    "    axes[1, 0].axvline(x=0, color='red', linestyle='--')\n",
    "    axes[1, 0].set_xlabel('Residuals')\n",
    "    axes[1, 0].set_ylabel('Frequency')\n",
    "    axes[1, 0].set_title('Residual Distribution')\n",
    "    \n",
    "    # 4. Scale-Location Plot\n",
    "    sqrt_abs_resid = np.sqrt(np.abs(all_residuals))\n",
    "    axes[1, 1].scatter(all_fitted, sqrt_abs_resid, alpha=0.5, s=1)\n",
    "    axes[1, 1].set_xlabel('Fitted Values')\n",
    "    axes[1, 1].set_ylabel('√|Residuals|')\n",
    "    axes[1, 1].set_title('Scale-Location Plot')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(plots_dir, f'{group_name}_unweighted.png'), \n",
    "                dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "# -----------------------------\n",
    "# Rubin's Rule\n",
    "# -----------------------------\n",
    "def rubins_pool(estimates, ses):\n",
    "    m = len(estimates)\n",
    "    q_bar = np.mean(estimates)\n",
    "    u_bar = np.mean(np.square(ses))\n",
    "    b_m = np.var(estimates, ddof=1)\n",
    "    total_var = u_bar + ((1 + 1/m) * b_m)\n",
    "    total_se = np.sqrt(total_var)\n",
    "    ci_lower = q_bar - 1.96 * total_se\n",
    "    ci_upper = q_bar + 1.96 * total_se\n",
    "    p_value = 2 * (1 - t.cdf(np.abs(q_bar / total_se), df=m-1))\n",
    "    rounded_p = round(p_value, 5)\n",
    "    formatted_p = \"< 0.00001\" if rounded_p <= 0.00001 else f\"{rounded_p:.5f}\"\n",
    "    return q_bar, total_se, ci_lower, ci_upper, formatted_p\n",
    "\n",
    "# -----------------------------\n",
    "# SMD + Variance Ratio\n",
    "# -----------------------------\n",
    "def calculate_smd_vr(X, T):\n",
    "    treated = X[T == 1]\n",
    "    control = X[T == 0]\n",
    "    smd, vr = [], []\n",
    "    for col in X.columns:\n",
    "        try:\n",
    "            m1, m0 = treated[col].mean(), control[col].mean()\n",
    "            s1, s0 = treated[col].std(), control[col].std()\n",
    "            pooled_sd = np.sqrt((s1 ** 2 + s0 ** 2) / 2)\n",
    "            smd.append((m1 - m0) / pooled_sd if pooled_sd > 0 else 0)\n",
    "            vr.append(s1**2 / s0**2 if s0**2 > 0 else 0)\n",
    "        except Exception:\n",
    "            smd.append(np.nan)\n",
    "            vr.append(np.nan)\n",
    "    return np.nanmean(smd), np.nanmean(vr)\n",
    "\n",
    "# -----------------------------\n",
    "# DML Main Loop\n",
    "# -----------------------------\n",
    "def run_dml_with_trimmed_data(final_covariates_map):\n",
    "    att_results = []\n",
    "    balance_results = []\n",
    "\n",
    "    for group, covariates in final_covariates_map.items():\n",
    "        print(f\"\\n🚀 Running DML for {group}\")\n",
    "        group_dir = os.path.join(output_folder, group)\n",
    "        os.makedirs(group_dir, exist_ok=True)\n",
    "\n",
    "        best_result = None\n",
    "        best_se = float(\"inf\")\n",
    "        \n",
    "        # Initialize residuals collection for this group\n",
    "        group_residuals_data = {'residuals': [], 'fitted': []}\n",
    "\n",
    "        for seed in seeds:\n",
    "            att_list, se_list, r2_list, rmse_list, smd_list, vr_list = [], [], [], [], [], []\n",
    "\n",
    "            for imp in range(1, imputations + 1):\n",
    "                file_path = os.path.join(group_dir, f\"trimmed_data_imp{imp}.pkl\")\n",
    "                if not os.path.exists(file_path):\n",
    "                    continue\n",
    "\n",
    "                df = pd.read_pickle(file_path)\n",
    "                if group not in df.columns:\n",
    "                    continue\n",
    "\n",
    "                X = df[covariates].copy()\n",
    "                T = df[group]\n",
    "                Y = df[\"caps5_change_baseline\"]\n",
    "\n",
    "                for repeat in range(n_repeats):\n",
    "                    kf = KFold(n_splits=5, shuffle=True, random_state=seed + repeat)\n",
    "                    for train_idx, test_idx in kf.split(X):\n",
    "                        try:\n",
    "                            X_train, T_train, Y_train = (\n",
    "                                X.iloc[train_idx],\n",
    "                                T.iloc[train_idx],\n",
    "                                Y.iloc[train_idx],\n",
    "                            )\n",
    "\n",
    "                            model_y = xgb.XGBRegressor(n_estimators=100, max_depth=3, learning_rate=0.1, n_jobs=1, random_state=seed)\n",
    "                            model_t = xgb.XGBClassifier(n_estimators=100, max_depth=3, learning_rate=0.1, n_jobs=1,\n",
    "                                                        use_label_encoder=False, eval_metric=\"logloss\", random_state=seed)\n",
    "\n",
    "                            dml = LinearDML(model_y=model_y, model_t=model_t, discrete_treatment=True,\n",
    "                                            cv=KFold(n_splits=3, shuffle=True, random_state=seed), random_state=seed)\n",
    "                            dml.fit(Y_train, T_train, X=X_train)\n",
    "\n",
    "                            tau = dml.effect(X_train)\n",
    "                            att = np.mean(tau)\n",
    "                            influence = tau - att\n",
    "                            se = np.sqrt(np.mean(influence ** 2) / len(tau))\n",
    "\n",
    "                            att_list.append(att)\n",
    "                            se_list.append(se)\n",
    "\n",
    "                            Y_pred = model_y.fit(X_train, Y_train).predict(X_train)\n",
    "                            residuals = Y_train - Y_pred\n",
    "                            \n",
    "                            # Collect residuals and fitted values for plotting\n",
    "                            group_residuals_data['residuals'].extend(residuals.tolist())\n",
    "                            group_residuals_data['fitted'].extend(Y_pred.tolist())\n",
    "                            \n",
    "                            rmse = mean_squared_error(Y_train, Y_pred, squared=False)\n",
    "                            r2 = r2_score(Y_train, Y_pred)\n",
    "                            r2_list.append(r2)\n",
    "                            rmse_list.append(rmse)\n",
    "\n",
    "                            smd, vr = calculate_smd_vr(X_train, T_train)\n",
    "                            smd_list.append(smd)\n",
    "                            vr_list.append(vr)\n",
    "                        except Exception as e:\n",
    "                            print(f\"⚠️ Error in {group}, seed {seed}, imp {imp}, rep {repeat}: {e}\")\n",
    "\n",
    "            if att_list:\n",
    "                att, se, ci_l, ci_u, p_val = rubins_pool(att_list, se_list)\n",
    "                avg_r2 = np.mean(r2_list)\n",
    "                avg_rmse = np.mean(rmse_list)\n",
    "                avg_smd = np.mean(smd_list)\n",
    "                avg_vr = np.mean(vr_list)\n",
    "\n",
    "                balance_results.append({\n",
    "                    \"group\": group, \"seed\": seed, \"smd\": avg_smd, \"vr\": avg_vr\n",
    "                })\n",
    "\n",
    "                if se < best_se:\n",
    "                    best_se = se\n",
    "                    best_result = {\n",
    "                        \"group\": group, \"seed\": seed, \"att\": att, \"se\": se,\n",
    "                        \"ci_lower\": ci_l, \"ci_upper\": ci_u, \"p_value\": p_val,\n",
    "                        \"r2\": avg_r2, \"rmse\": avg_rmse\n",
    "                    }\n",
    "\n",
    "                print(f\"✅ {group} | Seed {seed}: ATT = {att:.4f}, SE = {se:.4f}, p = {p_val}\")\n",
    "            else:\n",
    "                print(f\"⚠️ No valid results for {group} | Seed {seed}\")\n",
    "\n",
    "        if best_result:\n",
    "            att_results.append(best_result)\n",
    "            print(f\"🏆 Best result for {group} → Seed {best_result['seed']} | SE = {best_result['se']:.4f}\")\n",
    "        \n",
    "        # Create diagnostic plots for this group\n",
    "        print(f\"📊 Creating diagnostic plots for {group}...\")\n",
    "        create_diagnostic_plots(group_residuals_data, group, output_folder)\n",
    "\n",
    "    # Save final output\n",
    "    pd.DataFrame(att_results).to_excel(\"dml_rubin_summary_subsubcats_unweighted.xlsx\", index=False)\n",
    "    pd.DataFrame(balance_results).to_excel(\"smd_vr_summary_subsubcats_unweighted.xlsx\", index=False)\n",
    "    print(\"\\n🎯 All summary files saved.\")\n",
    "    print(\"📊 All diagnostic plots saved in outputs/plots/ folder.\")\n",
    "\n",
    "run_dml_with_trimmed_data(final_covariates_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23bb9b1-4751-49b5-a763-e0e5fdcafc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import sem, ttest_ind\n",
    "\n",
    "# ----------------------------------\n",
    "# File paths\n",
    "# ----------------------------------\n",
    "output_base = \"outputs\"\n",
    "att_file = \"dml_rubin_summary_subsubcats.xlsx\"\n",
    "trimmed_file = \"trimmed_data_imp1.pkl\"\n",
    "auc_file = \"auc_scores.xlsx\"  # NEW\n",
    "\n",
    "# ----------------------------------\n",
    "# Load ATT Summary\n",
    "# ----------------------------------\n",
    "if os.path.exists(att_file):\n",
    "    att_df = pd.read_excel(att_file)\n",
    "else:\n",
    "    raise FileNotFoundError(\"❌ ATT summary file not found: dml_rubin_summary_subsubcats.xlsx\")\n",
    "\n",
    "summary_rows = []\n",
    "\n",
    "# ----------------------------------\n",
    "# Loop over medication groups\n",
    "# ----------------------------------\n",
    "groups = [g for g in medication_groups if os.path.isdir(os.path.join(output_base, g))]\n",
    "\n",
    "for med in groups:\n",
    "    try:\n",
    "        group_path = os.path.join(output_base, med)\n",
    "\n",
    "        # Load trimmed data\n",
    "        df = pd.read_pickle(os.path.join(group_path, trimmed_file))\n",
    "\n",
    "        # Detect treatment column\n",
    "        treatment_cols = [col for col in df.columns if col.upper() == med.upper()]\n",
    "        if not treatment_cols:\n",
    "            print(f\"⚠️ Treatment column {med} not found in trimmed data. Skipping.\")\n",
    "            continue\n",
    "        treatment_var = treatment_cols[0]\n",
    "\n",
    "        # Extract treatment and outcome\n",
    "        T = df[treatment_var]\n",
    "        Y = df[\"caps5_change_baseline\"]\n",
    "\n",
    "        # Treated and control stats\n",
    "        treated = Y[T == 1]\n",
    "        control = Y[T == 0]\n",
    "\n",
    "        mean_treat = treated.mean()\n",
    "        se_treat = sem(treated) if len(treated) > 1 else np.nan\n",
    "\n",
    "        mean_ctrl = control.mean()\n",
    "        se_ctrl = sem(control) if len(control) > 1 else np.nan\n",
    "\n",
    "        # Cohen's d (unadjusted)\n",
    "        pooled_sd = np.sqrt(((treated.std() ** 2) + (control.std() ** 2)) / 2)\n",
    "        cohen_d = (mean_treat - mean_ctrl) / pooled_sd if pooled_sd > 0 else np.nan\n",
    "\n",
    "        # E-value (unadjusted)\n",
    "        delta = mean_treat - mean_ctrl\n",
    "        E = delta / abs(mean_ctrl) * 100 if mean_ctrl != 0 else np.nan\n",
    "\n",
    "        # Unadjusted p-value\n",
    "        try:\n",
    "            t_stat, p_val = ttest_ind(treated, control, equal_var=False, nan_policy=\"omit\")\n",
    "            rounded_p = round(p_val, 5)\n",
    "            formatted_p = \"< 0.00001\" if rounded_p < 0.00001 else rounded_p\n",
    "        except Exception:\n",
    "            formatted_p = np.nan\n",
    "\n",
    "        # AUC from new auc_scores.xlsx file\n",
    "        auc_val = np.nan\n",
    "        auc_path = os.path.join(group_path, auc_file)\n",
    "        if os.path.exists(auc_path):\n",
    "            auc_df = pd.read_excel(auc_path)\n",
    "            if \"AUC\" in auc_df.columns:\n",
    "                auc_val = auc_df[\"AUC\"].dropna().mean()\n",
    "\n",
    "        # Adjusted stats from Rubin summary\n",
    "        att_row = att_df[att_df[\"group\"].str.strip().str.upper() == med.strip().upper()]\n",
    "        if not att_row.empty:\n",
    "            att = att_row.iloc[0][\"att\"]\n",
    "            att_se = att_row.iloc[0][\"se\"]\n",
    "            att_p_val = att_row.iloc[0][\"p_value\"]\n",
    "            r2 = att_row.iloc[0][\"r2\"]\n",
    "            rmse = att_row.iloc[0][\"rmse\"]\n",
    "\n",
    "            try:\n",
    "                rounded_att_p = round(float(att_p_val), 5)\n",
    "                formatted_att_p = \"< 0.00001\" if rounded_att_p < 0.00001 else rounded_att_p\n",
    "            except:\n",
    "                formatted_att_p = att_p_val\n",
    "        else:\n",
    "            att, att_se, formatted_att_p, r2, rmse = np.nan, np.nan, np.nan, np.nan, np.nan\n",
    "\n",
    "        # Append full row\n",
    "        summary_rows.append({\n",
    "            'Medication Group': med,\n",
    "            'Mean Treated': mean_treat,\n",
    "            'SE Treated': se_treat,\n",
    "            'Mean Control': mean_ctrl,\n",
    "            'SE Control': se_ctrl,\n",
    "            'Cohen d': cohen_d,\n",
    "            'E (Unadjusted)': E,\n",
    "            'n Treated': len(treated),\n",
    "            'n Control': len(control),\n",
    "            #'Unadjusted p-value': formatted_p,\n",
    "            'ATT Estimate': att,\n",
    "            'ATT SE (Robust)': att_se,\n",
    "            'ATT p-value': formatted_att_p,\n",
    "            'R²': r2,\n",
    "            'RMSE': rmse,\n",
    "            'AUC': auc_val\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error in {med}: {e}\")\n",
    "\n",
    "# ----------------------------------\n",
    "# Save final summary\n",
    "# ----------------------------------\n",
    "summary_df = pd.DataFrame(summary_rows)\n",
    "summary_df = summary_df.sort_values(\"Medication Group\")\n",
    "summary_df.to_excel(\"Final_ATT_Summary_SubSubCat.xlsx\", index=False)\n",
    "print(\"✅ Final_ATT_Summary_SubSubCat saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b5aa46-fbe0-494a-9624-19a4b9e2d32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "# ✅ Load the final summary table\n",
    "final_df = pd.read_excel(\"Final_ATT_Summary_SubSubCat.xlsx\")\n",
    "\n",
    "# ✅ Parse DML p-values (handle \"< 0.00001\")\n",
    "def parse_pval(p):\n",
    "    try:\n",
    "        if isinstance(p, str) and \"<\" in p:\n",
    "            return 0.000001\n",
    "        return float(p)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "final_df['ATT p-value'] = final_df['ATT p-value'].apply(parse_pval)\n",
    "\n",
    "# ✅ Plot settings\n",
    "width = 0.35\n",
    "\n",
    "# ✅ Plotting function for a single medication group\n",
    "def plot_single_group(row):\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    bars1 = ax.bar(-width/2, row['Mean Control'], width, \n",
    "                   yerr=row['SE Control'], label='Control', hatch='//', color='gray', capsize=5)\n",
    "    bars2 = ax.bar(+width/2, row['Mean Treated'], width, \n",
    "                   yerr=row['SE Treated'], label='Treated', color='steelblue', capsize=5)\n",
    "\n",
    "    label = (\n",
    "        f\"ATT = {row['ATT Estimate']:.2f}\\n\"\n",
    "        f\"d = {row['Cohen d']:.2f}, p = {row['ATT p-value']:.3f}\\n\"\n",
    "        f\"nT = {row['n Treated']}, nC = {row['n Control']}\\n\"\n",
    "        f\"E = {row['E (Unadjusted)']:.1f}%\"\n",
    "    )\n",
    "    max_y = max(row['Mean Control'], row['Mean Treated']) + 1.5\n",
    "    ax.text(0, max_y, label, ha='center', va='bottom', fontsize=9, color='#FFD700')\n",
    "\n",
    "    ax.axhline(0, color='black', linewidth=0.8)\n",
    "    ax.set_xticks([-width/2, +width/2])\n",
    "    ax.set_xticklabels(['Control', 'Treated'])\n",
    "    ax.set_title(f\"Group: {row['Medication Group']}\", fontsize=12, weight='bold')\n",
    "    ax.set_ylabel(\"CAPS5 Change Score\")\n",
    "    ax.set_ylim(bottom=0, top=max_y + 2)\n",
    "    ax.legend()\n",
    "    fig.tight_layout()\n",
    "    return fig\n",
    "\n",
    "# ✅ Generate and save all plots into a multi-page PDF\n",
    "with PdfPages(\"dml_att_barplot_subsubcat.pdf\") as pdf:\n",
    "    for idx, row in final_df.iterrows():\n",
    "        fig = plot_single_group(row)\n",
    "        pdf.savefig(fig, dpi=300, bbox_inches='tight')\n",
    "        plt.close(fig)\n",
    "print(\"✅ dml_att_barplot_subsubcat is saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114d4815-ba90-4c26-b19c-95420b6cfd65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Love plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe90a956-216d-4c92-8adb-1a2611aa0b0b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "# ----------------------------------------\n",
    "# Functions to calculate balance\n",
    "# ----------------------------------------\n",
    "def calculate_smd(x1, x2, w1=None, w2=None):\n",
    "    def weighted_mean(x, w): return np.average(x, weights=w)\n",
    "    def weighted_var(x, w):\n",
    "        m = np.average(x, weights=w)\n",
    "        return np.average((x - m) ** 2, weights=w)\n",
    "    \n",
    "    m1 = weighted_mean(x1, w1) if w1 is not None else np.mean(x1)\n",
    "    m2 = weighted_mean(x2, w2) if w2 is not None else np.mean(x2)\n",
    "    v1 = weighted_var(x1, w1) if w1 is not None else np.var(x1, ddof=1)\n",
    "    v2 = weighted_var(x2, w2) if w2 is not None else np.var(x2, ddof=1)\n",
    "    \n",
    "    pooled_sd = np.sqrt((v1 + v2) / 2)\n",
    "    return np.abs(m1 - m2) / pooled_sd if pooled_sd > 0 else 0\n",
    "\n",
    "def variance_ratio(x1, x2, w1=None, w2=None):\n",
    "    def weighted_var(x, w):\n",
    "        m = np.average(x, weights=w)\n",
    "        return np.average((x - m) ** 2, weights=w)\n",
    "    \n",
    "    v1 = weighted_var(x1, w1) if w1 is not None else np.var(x1, ddof=1)\n",
    "    v2 = weighted_var(x2, w2) if w2 is not None else np.var(x2, ddof=1)\n",
    "    \n",
    "    return max(v1 / v2, v2 / v1) if v1 > 0 and v2 > 0 else 1\n",
    "\n",
    "# ----------------------------------------\n",
    "# Setup\n",
    "# ----------------------------------------\n",
    "output_base = \"outputs\"\n",
    "groups = [g for g in os.listdir(output_base) if os.path.isdir(os.path.join(output_base, g))]\n",
    "\n",
    "# Create a case-insensitive mapping\n",
    "final_covariates_map_lower = {k.lower(): v for k, v in final_covariates_map.items()}\n",
    "\n",
    "# ----------------------------------------\n",
    "# Main Loop\n",
    "# ----------------------------------------\n",
    "for group in groups:\n",
    "    if group.lower() not in final_covariates_map_lower:\n",
    "        continue\n",
    "\n",
    "    print(f\"\\n🔍 Processing {group}...\")\n",
    "\n",
    "    try:\n",
    "        group_path = os.path.join(output_base, group)\n",
    "        covariates = final_covariates_map_lower[group.lower()]\n",
    "        \n",
    "        column_name = None\n",
    "        for col in pd.read_pickle(os.path.join(group_path, \"trimmed_data_imp1.pkl\")).columns:\n",
    "            if col.lower() == group.lower():\n",
    "                column_name = col\n",
    "                break\n",
    "        if column_name is None:\n",
    "            print(f\"⚠️ Column not found for {group}, skipping.\")\n",
    "            continue\n",
    "\n",
    "        smd_unw_all, smd_w_all = [], []\n",
    "        vr_unw_all, vr_w_all = [], []\n",
    "\n",
    "        for i in range(1, 6):\n",
    "            df_path = os.path.join(group_path, f\"trimmed_data_imp{i}.pkl\")\n",
    "            iptw_path = os.path.join(group_path, \"iptw_weights.xlsx\")\n",
    "\n",
    "            if not os.path.exists(df_path) or not os.path.exists(iptw_path):\n",
    "                print(f\"⚠️ Missing data for {group} imp{i}, skipping.\")\n",
    "                continue\n",
    "\n",
    "            df = pd.read_pickle(df_path)\n",
    "            iptw_df = pd.read_excel(iptw_path, index_col=0)\n",
    "            T = df[column_name]\n",
    "            W = iptw_df.loc[df.index, \"iptw_mean\"]\n",
    "\n",
    "            smd_unw_i, smd_w_i, vr_unw_i, vr_w_i = [], [], [], []\n",
    "\n",
    "            for cov in covariates:\n",
    "                x1, x0 = df.loc[T == 1, cov], df.loc[T == 0, cov]\n",
    "                w1, w0 = W[T == 1], W[T == 0]\n",
    "\n",
    "                su = calculate_smd(x1, x0)\n",
    "                sw = calculate_smd(x1, x0, w1, w0)\n",
    "\n",
    "                vu = variance_ratio(x1, x0) if cov in ['treatmentdurationdays', 'CAPS5score_baseline', 'age'] else np.nan\n",
    "                vw = variance_ratio(x1, x0, w1, w0) if cov in ['treatmentdurationdays', 'CAPS5score_baseline', 'age'] else np.nan\n",
    "\n",
    "                smd_unw_i.append(su)\n",
    "                smd_w_i.append(sw)\n",
    "                vr_unw_i.append(vu)\n",
    "                vr_w_i.append(vw)\n",
    "\n",
    "            smd_unw_all.append(smd_unw_i)\n",
    "            smd_w_all.append(smd_w_i)\n",
    "            vr_unw_all.append(vr_unw_i)\n",
    "            vr_w_all.append(vr_w_i)\n",
    "\n",
    "        smd_unw = np.mean(smd_unw_all, axis=0)\n",
    "        smd_w = np.mean(smd_w_all, axis=0)\n",
    "        vr_unw = np.nanmean(vr_unw_all, axis=0)\n",
    "        vr_w = np.nanmean(vr_w_all, axis=0)\n",
    "\n",
    "        severity = []\n",
    "        for sw in smd_w:\n",
    "            if sw <= 0.1:\n",
    "                severity.append(\"Good\")\n",
    "            elif sw <= 0.2:\n",
    "                severity.append(\"Moderate\")\n",
    "            else:\n",
    "                severity.append(\"Poor\")\n",
    "\n",
    "        covariate_names = covariates\n",
    "        numeric_df = pd.DataFrame({\n",
    "            \"Covariate\": covariate_names,\n",
    "            \"SMD_Unweighted\": smd_unw,\n",
    "            \"SMD_Weighted\": smd_w,\n",
    "            \"Imbalance_Severity\": severity,\n",
    "            \"VR_Unweighted\": vr_unw,\n",
    "            \"VR_Weighted\": vr_w\n",
    "        })\n",
    "\n",
    "        numeric_path = os.path.join(group_path, f\"covariate_balance_table_{group}.xlsx\")\n",
    "        numeric_df.to_excel(numeric_path, index=False)\n",
    "        print(f\"📊 Exported numeric summary to: {numeric_path}\")\n",
    "\n",
    "        # -------------------------\n",
    "        # Plot\n",
    "        # -------------------------\n",
    "        labels = covariates\n",
    "        y_pos = np.arange(len(labels))\n",
    "\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(18, len(labels) * 0.45))\n",
    "\n",
    "        axes[0].scatter(smd_unw, y_pos, color='red', label=\"Unweighted\")\n",
    "        axes[0].scatter(smd_w, y_pos, color='blue', label=\"Weighted\")\n",
    "        axes[0].axvline(0.1, color='gray', linestyle='--', label=\"Threshold 0.1\")\n",
    "        axes[0].axvline(0.2, color='black', linestyle='--', label=\"Threshold 0.2\")\n",
    "        axes[0].set_xlim(0, max(max(smd_unw), max(smd_w), 0.25) + 0.05)\n",
    "        axes[0].set_yticks(y_pos)\n",
    "        axes[0].set_yticklabels(labels)\n",
    "        axes[0].invert_yaxis()\n",
    "        axes[0].set_title(\"Standardized Mean Differences (SMD)\")\n",
    "        axes[0].legend(loc=\"upper right\")\n",
    "        axes[0].grid(True)\n",
    "\n",
    "        vr_mask = [cov in ['treatmentdurationdays', 'CAPS5score_baseline', 'age'] for cov in covariates]\n",
    "        filtered_y = [i for i, b in enumerate(vr_mask) if b]\n",
    "        filtered_labels = [labels[i] for i in filtered_y]\n",
    "        filtered_vr_unw = [vr_unw[i] for i in filtered_y]\n",
    "        filtered_vr_w = [vr_w[i] for i in filtered_y]\n",
    "\n",
    "        axes[1].scatter(filtered_vr_unw, filtered_y, color='blue', marker='o', label=\"Unweighted\")\n",
    "        axes[1].scatter(filtered_vr_w, filtered_y, color='red', marker='x', label=\"Weighted\")\n",
    "        axes[1].axvline(2, color='gray', linestyle='--')\n",
    "        axes[1].axvline(0.5, color='gray', linestyle='--')\n",
    "        axes[1].set_xlim(0, max(filtered_vr_unw + filtered_vr_w + [2.5]) + 0.5)\n",
    "        axes[1].set_yticks(filtered_y)\n",
    "        axes[1].set_yticklabels(filtered_labels)\n",
    "        axes[1].invert_yaxis()\n",
    "        axes[1].set_title(\"Variance Ratio (VR)\")\n",
    "        axes[1].legend()\n",
    "        axes[1].grid(True)\n",
    "\n",
    "        fig.suptitle(f\"Covariate Balance for {group.replace('CAT_', '')}\", fontsize=14, weight='bold')\n",
    "        fig.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "        plot_path = os.path.join(group_path, f\"love_plot_{group}.pdf\")\n",
    "        fig.savefig(plot_path, dpi=300)\n",
    "        plt.close()\n",
    "        print(f\"✅ Saved love plot: {plot_path}\")\n",
    "        print(f\"📏 Max weighted SMD for {group}: {np.max(smd_w):.3f}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error in {group}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04890d8f-3ebd-485f-8faa-42e595e1b1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691f11f8-4131-4292-8b21-03e8f19df707",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "#-----------------------------\n",
    "# Generate heatmaps\n",
    "# -------------------------------\n",
    "for treatment_var in medication_groups:\n",
    "    print(f\"\\n========== Creating Heatmap for {treatment_var} ==========\")\n",
    "\n",
    "    try:\n",
    "        output_folder = os.path.join('outputs', treatment_var)\n",
    "        balance_path = os.path.join(output_folder, f'covariate_balance_table_{treatment_var}.xlsx')\n",
    "\n",
    "        if not os.path.exists(balance_path):\n",
    "            print(f\"❌ Balance file not found: {balance_path}\")\n",
    "            continue\n",
    "\n",
    "        balance_df = pd.read_excel(balance_path)\n",
    "\n",
    "        # ✅ Use finalized covariates + 'Propensity Score'\n",
    "        covariates = final_covariates_map[treatment_var] + ['Propensity Score']\n",
    "        balance_df = balance_df[balance_df['Covariate'].isin(covariates)]\n",
    "\n",
    "        # ✅ Check for CAPS5score_baseline\n",
    "        highlight_caps = 'CAPS5score_baseline' in balance_df['Covariate'].values\n",
    "\n",
    "        # ✅ Format for heatmap\n",
    "        heatmap_df = balance_df[['Covariate', 'SMD_Unweighted', 'SMD_Weighted']].copy()\n",
    "        heatmap_df.columns = ['Covariate', 'Unweighted', 'Weighted']\n",
    "        heatmap_df = heatmap_df.set_index('Covariate')\n",
    "        heatmap_df = heatmap_df.sort_values(by='Unweighted', ascending=False)\n",
    "\n",
    "        # ✅ Plot\n",
    "        plt.figure(figsize=(12, max(10, len(heatmap_df) * 0.35)))\n",
    "        ax = sns.heatmap(\n",
    "            heatmap_df,\n",
    "            cmap=\"coolwarm\",\n",
    "            annot=True,\n",
    "            fmt=\".2f\",\n",
    "            linewidths=0.6,\n",
    "            linecolor='gray',\n",
    "            cbar_kws={\"label\": \"Standardized Mean Difference\"}\n",
    "        )\n",
    "\n",
    "        plt.title(f\"Covariate Balance Heatmap (Rubin IPTW)\\n{treatment_var}\", fontsize=15, weight='bold')\n",
    "        plt.xlabel(\"Condition\")\n",
    "        plt.ylabel(\"Covariate\")\n",
    "\n",
    "        # ✅ Bold CAPS5score_baseline if present\n",
    "        if highlight_caps:\n",
    "            ylabels = [label.get_text() for label in ax.get_yticklabels()]\n",
    "            ax.set_yticklabels([\n",
    "                f\"{label} ←\" if label == 'CAPS5score_baseline' else label for label in ylabels\n",
    "            ])\n",
    "\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # ✅ Save image\n",
    "        save_path = os.path.join(output_folder, f'heatmap_smd_{treatment_var}.png')\n",
    "        plt.savefig(save_path, dpi=300)\n",
    "        plt.close()\n",
    "\n",
    "        print(f\"✅ Heatmap saved: {save_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error processing {treatment_var}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8b09ff-b822-4215-914f-d0c7eff9d1e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a106ae6e-11d7-4f9a-88a0-aece43009c21",
   "metadata": {},
   "source": [
    "#### XGBOOST:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07982f2f-0c44-4cb6-b160-9503e73823f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")  # optional\n",
    "\n",
    "# Option 1 (recommended)\n",
    "new_path = r\"D:\\Work\\PTSD\\XGBoost\"\n",
    "\n",
    "os.chdir(new_path)  # Change working directory\n",
    "print(\"Changed to:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2621a257-5400-454e-b562-e4fe7819d2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(r\"data_baseline.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c01f361-e551-42fc-924c-069010cd0a3d",
   "metadata": {},
   "source": [
    "## CAT analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c51cc7-4bbe-48de-88d5-4c1ad720814c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all necessary packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "# For visualization and future steps\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.experimental import enable_iterative_imputer  # Needed to enable the experimental feature\n",
    "from sklearn.impute import IterativeImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47fb8af6-310c-42fc-89c1-39f8ea616894",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check basic structure\n",
    "print(\"Shape of dataset:\", df.shape)\n",
    "print(\"\\nSample columns:\", df.columns.tolist()[:10])\n",
    "print(\"\\nMissing values:\\n\", df.isnull().sum().sort_values(ascending=False).head(10))\n",
    "\n",
    "# Remove duplicate rows\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "# Confirm shape after removing duplicates\n",
    "print(\"Shape after removing duplicates:\", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb247f3-f275-4664-8ff6-6a31c04a8607",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyreadstat\n",
    "\n",
    "# Load gender info\n",
    "gender_df, meta = pyreadstat.read_sav(\"SDV_IN_Gender_2019_2024.sav\")\n",
    "\n",
    "# Just extract SDV_SEXE column and append to df\n",
    "df[\"SDV_SEXE\"] = gender_df[\"SDV_SEXE\"].reset_index(drop=True)\n",
    "\n",
    "# Optional: map to labels\n",
    "gender_map = {1.0: \"Male\", 2.0: \"Female\", 3.0: \"Other\"}\n",
    "df[\"gender_label\"] = df[\"SDV_SEXE\"].map(gender_map)\n",
    "\n",
    "# Done! Check a sample\n",
    "print(df[[\"SDV_SEXE\", \"gender_label\"]].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa490857-cce0-4727-b0bb-2bc44c774b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'gender' and 'SDV_SEXE' columns are in df\n",
    "\n",
    "# Gender dummy variables\n",
    "df['gender_1'] = (df['gender'] == 1).astype(int)\n",
    "df['gender_2'] = (df['gender'] == 2).astype(int)\n",
    "\n",
    "# SDV_SEXE dummy variables\n",
    "df['SDV_SEXE_1'] = (df['SDV_SEXE'] == 1).astype(int)\n",
    "df['SDV_SEXE_2'] = (df['SDV_SEXE'] == 2).astype(int)\n",
    "df['SDV_SEXE_3'] = (df['SDV_SEXE'] == 3).astype(int)\n",
    "\n",
    "# Create binary columns\n",
    "df['ethnicity_Dutch'] = np.where(df['ethnicity'] == 1, 1, 0)\n",
    "df['ethnicity_other'] = np.where(df['ethnicity'] != 1, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58abf5c6-b900-48da-b440-c1363e17e6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns manually identified for removal (example set from the R script)\n",
    "cols_to_drop = [\n",
    "    'gender', 'ethnicity', 'CIN5', 'SDV_SEXE', 'StartDatum', 'STARTDATUM', 'DROPOUT_EARLYCOMPLETER', 'TOEST_WO',\n",
    "    'depressie_IN', 'TERUGKOMER', 'VROEGK_ST', 'gender_label',\n",
    "    'depr_m_psychose_huid', 'depr_z_psychose_huid', 'depr_z_psychose_verl',\n",
    "    'depr_m_psychose_verl', 'CAPS5score_followup', 'CAPS5_DAT_IN'\n",
    "]\n",
    "\n",
    "df = df.drop(columns=[col for col in cols_to_drop if col in df.columns])\n",
    "print(\"Remaining columns:\", df.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed28f36-9f83-46ee-ad68-ce8bb17f0b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'BEH_DAGEN' in df.columns:\n",
    "    df.rename(columns={'BEH_DAGEN': 'treatmentdurationdays'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ebe3d23-51c4-450e-9401-3653a8d2f548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean and standardize column names\n",
    "df.columns = (\n",
    "    df.columns\n",
    "    .str.replace(r\"\\.+\", \"_\", regex=True)\n",
    "    .str.replace(r\"[^a-zA-Z0-9_]\", \"\", regex=True)\n",
    "    .str.replace(\" \", \"_\")\n",
    "    .str.strip()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63bf737a-7d2b-40ae-a33a-78fb08d6fa41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview key outcome variables\n",
    "outcome_vars = ['CAPS5score_baseline', 'CAPS5Score_TK']\n",
    "for col in outcome_vars:\n",
    "    if col in df.columns:\n",
    "        print(f\"{col}: {df[col].isnull().sum()} missing\")\n",
    "\n",
    "# Calculate change score\n",
    "if 'CAPS5score_baseline' in df.columns and 'CAPS5Score_TK' in df.columns:\n",
    "    df['caps5_change_baseline'] = df['CAPS5Score_TK'] - df['CAPS5score_baseline'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db544569-90b1-428a-9dfa-181d1fbd95e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define exceptions to keep\n",
    "protected_cols = [\n",
    "    \"DIAGNOSIS_ANXIETY_OCD\",\n",
    "    \"DIAGNOSIS_PSYCHOTIC\",\n",
    "    \"DIAGNOSIS_EATING_DISORDER\",\n",
    "    \"DIAGNOSIS_SUBSTANCE_DISORDER\", \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"EB_NON_TF_THERAPY\", \"OTHER_TREATM_APPROACH\", \"Bipolar_and_Mood_disorder\", 'SUBCAT_Selectieve_immunosuppresiva', 'treatmentdurationdays',\n",
    "'SUBCAT_Corticosteroiden',\n",
    "'SUBCAT_Immunomodulerend_Coxibs',\n",
    "'SUBCAT_Aminosalicylaten',\n",
    "'SUBCAT_calcineurineremmers',\n",
    "'SUBCAT_Anti_epileptica_Benzodiazepine',\n",
    "'SUBCAT_Paracetamol_overig_combinatie', 'SUBCAT_MAO_remmers', 'SUBCAT_psychostimulans_overige', 'SUBCAT_Interleukine_remmers'\n",
    "\n",
    "]\n",
    "\n",
    "\n",
    "# ----------------------------------------\n",
    "# 1. Drop columns with >95% missing values (except protected)\n",
    "thresh_missing = int(0.95 * len(df))\n",
    "missing_cols = [col for col in df.columns if df[col].isnull().sum() > (len(df) - thresh_missing)]\n",
    "missing_cols_to_drop = [col for col in missing_cols if col not in protected_cols]\n",
    "df = df.drop(columns=missing_cols_to_drop)\n",
    "\n",
    "# ----------------------------------------\n",
    "# 2. Drop near-zero variance columns (except protected)\n",
    "low_variance_cols = [col for col in df.columns if df[col].nunique(dropna=True) <= 1 and col not in protected_cols]\n",
    "df = df.drop(columns=low_variance_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164fa95f-9ce8-447a-816d-51da2b57ecd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"cleaned_data_baseline.csv\", index=False)\n",
    "print(\" Step 1 Complete: Cleaned dataset saved.\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "dd6e4a9d-b159-4880-9d2a-9ad28594f5f4",
   "metadata": {},
   "source": [
    "# Target variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ea46c7-ce76-4f28-9eaa-27b33c562d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from fancyimpute import IterativeImputer\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822895b2-d0b1-420a-a390-033db2fa9314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the cleaned dataset from Step 1\n",
    "df = pd.read_csv(\"cleaned_data_baseline.csv\")\n",
    "\n",
    "# Quick check\n",
    "print(df.shape)\n",
    "print(df.dtypes.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c27ae4-cfe3-4ba7-9a9a-f2fd7b63f3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate Numerical and Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9e1b8d-adda-4f10-8cb7-e65d17ae31a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify numerical and categorical columns\n",
    "numerical_cols = df.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "print(f\"Numerical Columns: {len(numerical_cols)}\")\n",
    "print(f\"Categorical Columns: {len(categorical_cols)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0580f30-66e5-49c6-94a6-7d15a9d5b573",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715a4034-509e-4e7d-973e-ee50bbb33fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fully prepared data\n",
    "df.to_csv(\"final_prepared_data.csv\", index=False)\n",
    "print(\" Step 2 Complete: Final prepared dataset saved as 'final_prepared_data.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad70e2d4-5b39-4d75-9165-77a7f6b99979",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.experimental import enable_iterative_imputer  # noqa\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "# ========== CONFIG ==========\n",
    "save_folder = \"imputed_data\"\n",
    "os.makedirs(save_folder, exist_ok=True)\n",
    "n_imputations = 5\n",
    "\n",
    "# ========== LOAD ==========\n",
    "# Ensure df is already defined\n",
    "assert 'df' in globals(), \"Please load the original DataFrame as `df` before running this script.\"\n",
    "\n",
    "# ========== IDENTIFY NUMERIC COLUMNS ==========\n",
    "numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "\n",
    "# ========== STEP 1: MICE IMPUTATION ==========\n",
    "print(\"=\" * 50)\n",
    "print(\"STEP 1: MICE IMPUTATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "imputed_dfs = []\n",
    "for i in range(1, n_imputations + 1):\n",
    "    print(f\"\\n=== Running MICE Imputation: Dataset {i} ===\")\n",
    "    #  NEW instance with different seed AND sample_posterior=True for randomness\n",
    "    mice_imputer = IterativeImputer(\n",
    "        max_iter=10, \n",
    "        random_state=42+i,  # Different base to avoid low numbers\n",
    "        sample_posterior=True,  #  KEY: This adds randomness!\n",
    "        n_nearest_features=None,\n",
    "        initial_strategy='mean'\n",
    "    )\n",
    "    # Fit-transform on numeric columns\n",
    "    imputed_array = mice_imputer.fit_transform(df[numeric_cols])\n",
    "    # Replace numeric columns in a copy of the original df\n",
    "    df_imputed = df.copy()\n",
    "    df_imputed[numeric_cols] = pd.DataFrame(imputed_array, columns=numeric_cols, index=df.index)\n",
    "    # Append to list\n",
    "    imputed_dfs.append(df_imputed)\n",
    "    print(f\" Completed imputation {i}\")\n",
    "\n",
    "# ========== STEP 2: ROUNDING ==========\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"STEP 2: ROUNDING NUMERIC COLUMNS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "def round_all_numeric_columns_all_imputations(imputed_dfs, decimals=0, verbose=True):\n",
    "    rounded_dfs = []\n",
    "    for i, df in enumerate(imputed_dfs):\n",
    "        df_copy = df.copy()\n",
    "        numeric_cols = df_copy.select_dtypes(include=[np.number]).columns\n",
    "        df_copy[numeric_cols] = df_copy[numeric_cols].round(decimals)\n",
    "        rounded_dfs.append(df_copy)\n",
    "        if verbose:\n",
    "            print(f\" Imputation {i+1}: Rounded {len(numeric_cols)} numeric columns to {decimals} decimal place(s).\")\n",
    "    return rounded_dfs\n",
    "\n",
    "# Apply rounding to all imputed datasets\n",
    "imputed_dfs = round_all_numeric_columns_all_imputations(imputed_dfs)\n",
    "\n",
    "# ========== STEP 3: SAVE FINAL DATASETS ==========\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"STEP 3: SAVING FINAL DATASETS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for i, df_imputed in enumerate(imputed_dfs, 1):\n",
    "    # Save outputs\n",
    "    pkl_path = f\"{save_folder}/df_imputed_final_imp{i}.pkl\"\n",
    "    csv_path = f\"{save_folder}/df_imputed_final_imp{i}.csv\"\n",
    "    excel_path = f\"{save_folder}/df_imputed_final_imp{i}.xlsx\"\n",
    "    \n",
    "    df_imputed.to_pickle(pkl_path)\n",
    "    df_imputed.to_csv(csv_path, index=False)\n",
    "    df_imputed.to_excel(excel_path, index=False)\n",
    "    \n",
    "    print(f\" Saved files for imputation {i}:\")\n",
    "    print(f\"   → {pkl_path}\")\n",
    "    print(f\"   → {csv_path}\")\n",
    "    print(f\"   → {excel_path}\")\n",
    "\n",
    "# ========== STEP 4: VERIFY DATASETS ARE DIFFERENT ==========\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"STEP 4: VERIFYING DATASET DIFFERENCES\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "def check_imputation_differences(imputed_dfs, verbose=True):\n",
    "    \"\"\"Check if imputed datasets are actually different from each other\"\"\"\n",
    "    if len(imputed_dfs) < 2:\n",
    "        print(\"  Only one dataset - cannot check differences\")\n",
    "        return\n",
    "    \n",
    "    # Get numeric columns that had missing values originally\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    missing_cols = [col for col in numeric_cols if df[col].isnull().any()]\n",
    "    \n",
    "    if not missing_cols:\n",
    "        print(\"  No missing values found in original data\")\n",
    "        return\n",
    "    \n",
    "    print(f\" Checking differences in {len(missing_cols)} columns that had missing values...\")\n",
    "    \n",
    "    differences_found = False\n",
    "    \n",
    "    for col in missing_cols[:3]:  # Check first 3 columns with missing values\n",
    "        # Compare first two datasets for this column\n",
    "        values_1 = imputed_dfs[0][col].values\n",
    "        values_2 = imputed_dfs[1][col].values\n",
    "        \n",
    "        if not np.array_equal(values_1, values_2):\n",
    "            differences_found = True\n",
    "            # Count how many values are different\n",
    "            diff_count = np.sum(values_1 != values_2)\n",
    "            print(f\" Column '{col}': {diff_count} different values between datasets 1 & 2\")\n",
    "        else:\n",
    "            print(f\" Column '{col}': IDENTICAL values between datasets 1 & 2\")\n",
    "    \n",
    "    if differences_found:\n",
    "        print(f\"\\n SUCCESS: Datasets show proper variability!\")\n",
    "    else:\n",
    "        print(f\"\\n  WARNING: Datasets appear identical - check random_state implementation\")\n",
    "    \n",
    "    return differences_found\n",
    "\n",
    "# Run the check\n",
    "check_imputation_differences(imputed_dfs)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\" MICE IMPUTATION COMPLETE!\")\n",
    "print(\"=\" * 50)\n",
    "print(f\" Created {n_imputations} imputed datasets\")\n",
    "print(f\" Applied rounding to all numeric columns\")\n",
    "print(f\" Saved files in: {save_folder}/\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b53723b-bc90-45e1-b949-260ede598f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ========== METHOD 1: QUICK CHECK - Compare first 2 datasets ==========\n",
    "def quick_difference_check(imputed_dfs):\n",
    "    \"\"\"Quick check to see if first two datasets are different\"\"\"\n",
    "    if len(imputed_dfs) < 2:\n",
    "        print(\"Need at least 2 datasets to compare\")\n",
    "        return\n",
    "    \n",
    "    df1 = imputed_dfs[0]\n",
    "    df2 = imputed_dfs[1]\n",
    "    \n",
    "    # Check if dataframes are identical\n",
    "    are_identical = df1.equals(df2)\n",
    "    print(f\"Dataset 1 vs Dataset 2: {'IDENTICAL ❌' if are_identical else 'DIFFERENT ✅'}\")\n",
    "    \n",
    "    if not are_identical:\n",
    "        # Count different values\n",
    "        numeric_cols = df1.select_dtypes(include=[np.number]).columns\n",
    "        total_diff = 0\n",
    "        for col in numeric_cols:\n",
    "            diff_count = np.sum(df1[col] != df2[col])\n",
    "            if diff_count > 0:\n",
    "                total_diff += diff_count\n",
    "                print(f\"  '{col}': {diff_count} different values\")\n",
    "        print(f\"  Total different values: {total_diff}\")\n",
    "\n",
    "# ========== METHOD 2: DETAILED CHECK - All pairwise comparisons ==========\n",
    "def detailed_difference_check(imputed_dfs):\n",
    "    \"\"\"Check differences between all pairs of datasets\"\"\"\n",
    "    n_datasets = len(imputed_dfs)\n",
    "    print(f\"\\n=== Checking all {n_datasets} datasets ===\")\n",
    "    \n",
    "    numeric_cols = imputed_dfs[0].select_dtypes(include=[np.number]).columns\n",
    "    \n",
    "    for i in range(n_datasets):\n",
    "        for j in range(i+1, n_datasets):\n",
    "            are_identical = imputed_dfs[i].equals(imputed_dfs[j])\n",
    "            print(f\"Dataset {i+1} vs Dataset {j+1}: {'IDENTICAL ❌' if are_identical else 'DIFFERENT ✅'}\")\n",
    "\n",
    "# ========== METHOD 3: FOCUS ON ORIGINALLY MISSING VALUES ==========\n",
    "def check_missing_value_differences(original_df, imputed_dfs):\n",
    "    \"\"\"Check differences only in originally missing positions\"\"\"\n",
    "    print(f\"\\n=== Checking differences in originally missing positions ===\")\n",
    "    \n",
    "    numeric_cols = original_df.select_dtypes(include=[np.number]).columns\n",
    "    \n",
    "    differences_found = False\n",
    "    for col in numeric_cols:\n",
    "        if original_df[col].isnull().any():\n",
    "            missing_mask = original_df[col].isnull()\n",
    "            print(f\"\\nColumn '{col}' ({missing_mask.sum()} missing values):\")\n",
    "            \n",
    "            # Compare imputed values at missing positions\n",
    "            for i in range(len(imputed_dfs)-1):\n",
    "                imp1_values = imputed_dfs[i].loc[missing_mask, col]\n",
    "                imp2_values = imputed_dfs[i+1].loc[missing_mask, col]\n",
    "                \n",
    "                are_same = np.array_equal(imp1_values.values, imp2_values.values)\n",
    "                if not are_same:\n",
    "                    differences_found = True\n",
    "                    diff_count = np.sum(imp1_values.values != imp2_values.values)\n",
    "                    print(f\"  Dataset {i+1} vs {i+2}: {diff_count}/{len(imp1_values)} different imputed values ✅\")\n",
    "                else:\n",
    "                    print(f\"  Dataset {i+1} vs {i+2}: IDENTICAL imputed values ❌\")\n",
    "    \n",
    "    return differences_found\n",
    "\n",
    "# ========== METHOD 4: SAMPLE VALUES FROM EACH DATASET ==========\n",
    "def show_sample_imputed_values(original_df, imputed_dfs, n_samples=5):\n",
    "    \"\"\"Show sample imputed values from each dataset\"\"\"\n",
    "    print(f\"\\n=== Sample imputed values (first {n_samples} missing positions) ===\")\n",
    "    \n",
    "    numeric_cols = original_df.select_dtypes(include=[np.number]).columns\n",
    "    \n",
    "    for col in numeric_cols:\n",
    "        if original_df[col].isnull().any():\n",
    "            missing_positions = original_df[original_df[col].isnull()].index[:n_samples]\n",
    "            \n",
    "            print(f\"\\nColumn '{col}' at positions {list(missing_positions)}:\")\n",
    "            for i, df_imp in enumerate(imputed_dfs):\n",
    "                values = df_imp.loc[missing_positions, col].values\n",
    "                print(f\"  Dataset {i+1}: {values}\")\n",
    "\n",
    "# ========== RUN ALL CHECKS ==========\n",
    "print(\"=\" * 60)\n",
    "print(\"CHECKING IMPUTATION DIFFERENCES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Method 1: Quick check\n",
    "quick_difference_check(imputed_dfs)\n",
    "\n",
    "# Method 2: All pairwise comparisons  \n",
    "detailed_difference_check(imputed_dfs)\n",
    "\n",
    "# Method 3: Focus on originally missing values (assumes 'df' is your original dataframe)\n",
    "if 'df' in globals():\n",
    "    differences_found = check_missing_value_differences(df, imputed_dfs)\n",
    "    if differences_found:\n",
    "        print(f\"\\n🎉 SUCCESS: Found differences in imputed values!\")\n",
    "    else:\n",
    "        print(f\"\\n⚠️ WARNING: No differences found in imputed values!\")\n",
    "\n",
    "# Method 4: Show sample values\n",
    "if 'df' in globals():\n",
    "    show_sample_imputed_values(df, imputed_dfs, n_samples=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711fb48b-df58-47c5-ab5a-1bf448defe1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_folder = \"imputed_data\"\n",
    "n_imputations = 5\n",
    "\n",
    "# Lists to hold DataFrames and Y vectors\n",
    "imputed_dfs = []\n",
    "Y_list = []\n",
    "\n",
    "for i in range(1, n_imputations + 1):\n",
    "    file_path = f\"{imputed_folder}/df_imputed_final_imp{i}.pkl\"\n",
    "    \n",
    "    # Load imputed DataFrame\n",
    "    df_imp = pd.read_pickle(file_path)\n",
    "    imputed_dfs.append(df_imp)\n",
    "\n",
    "    # Define Y for this imputation\n",
    "    Y = df_imp[\"caps5_change_baseline\"]\n",
    "    Y_list.append(Y)\n",
    "\n",
    "    print(f\"Y for imputation {i} defined. Sample values:\")\n",
    "    print(Y.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867d7aa7-2678-4d61-b04d-49183d36f6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load imputed DataFrames from saved files\n",
    "imputed_folder = \"imputed_data\"\n",
    "n_imputations = 5\n",
    "\n",
    "for i in range(1, n_imputations + 1):\n",
    "    print(f\"\\n=== Imputed Dataset {i} ===\")\n",
    "\n",
    "    # Load each imputed dataset\n",
    "    df_imp = pd.read_pickle(f\"{imputed_folder}/df_imputed_final_imp{i}.pkl\")\n",
    "\n",
    "    # Get all CAT_* columns\n",
    "    cat_columns = [col for col in df_imp.columns if col.startswith('CAT_')]\n",
    "\n",
    "    print(\"Medication Groups:\")\n",
    "    print(cat_columns)\n",
    "    print(\"Total Medication Groups Found:\", len(cat_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17e7f0c-2653-4a13-9d3f-d96b63ee9397",
   "metadata": {},
   "outputs": [],
   "source": [
    "covariates_CAT_ADHD = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline', 'CAT_Anti_epileptica', 'CAT_Antidepressiva', 'CAT_Antipsychotica',\n",
    "    'CAT_Benzodiazepine', 'CAT_Opioden',\n",
    "    'CAT_Z_drugs', 'DIAGNOSIS_CHILDHOOD_TRAUMA', 'DIAGNOSIS_CPTSD',\n",
    "    'DIAGNOSIS_SEXUAL_TRAUMA', 'DIAGNOSIS_SMOKING', 'DIAGNOSIS_SUICIDALITY',\n",
    "    'age',\n",
    "    'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"EB_NON_TF_THERAPY\", \"OTHER_TREATM_APPROACH\", \"Bipolar_and_Mood_disorder\", \"ethnicity_Dutch\", \"ethnicity_other\"\n",
    "]\n",
    "\n",
    "\n",
    "covariates_CAT_Aceetanilidederivaten = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline', 'CAT_Anti_epileptica', 'CAT_Antidepressiva', 'CAT_Antipsychotica',\n",
    "    'CAT_Benzodiazepine', 'CAT_Opioden', 'CAT_Z_drugs',\n",
    "    'DIAGNOSIS_CHILDHOOD_TRAUMA', 'DIAGNOSIS_CPTSD', 'DIAGNOSIS_SEXUAL_TRAUMA',\n",
    "    'DIAGNOSIS_SMOKING', 'DIAGNOSIS_SUICIDALITY', 'age',\n",
    "    'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"EB_NON_TF_THERAPY\", \"OTHER_TREATM_APPROACH\", \"Bipolar_and_Mood_disorder\", \"ethnicity_Dutch\", \"ethnicity_other\"\n",
    "]\n",
    "\n",
    "\n",
    "covariates_CAT_Z_drugs = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline', 'CAT_Anti_epileptica', 'CAT_Antidepressiva', 'CAT_Antipsychotica',\n",
    "    'CAT_Benzodiazepine', 'CAT_Opioden',\n",
    "    'DIAGNOSIS_CHILDHOOD_TRAUMA', 'DIAGNOSIS_CPTSD', 'DIAGNOSIS_SEXUAL_TRAUMA',\n",
    "    'DIAGNOSIS_SMOKING', 'DIAGNOSIS_SUICIDALITY', 'age',\n",
    "    'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"EB_NON_TF_THERAPY\", \"OTHER_TREATM_APPROACH\", \"Bipolar_and_Mood_disorder\", \"ethnicity_Dutch\", \"ethnicity_other\"\n",
    "]\n",
    "\n",
    "\n",
    "covariates_CAT_Opioden = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline', 'CAT_Anti_epileptica',\n",
    "    'CAT_Antidepressiva',\n",
    "    'CAT_Antipsychotica', 'CAT_Benzodiazepine', 'CAT_Z_drugs',\n",
    "    'DIAGNOSIS_CHILDHOOD_TRAUMA', 'DIAGNOSIS_CPTSD', 'DIAGNOSIS_SEXUAL_TRAUMA',\n",
    "    'DIAGNOSIS_SMOKING', 'DIAGNOSIS_SUICIDALITY', 'age',\n",
    "    'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"EB_NON_TF_THERAPY\", \"OTHER_TREATM_APPROACH\", \"Bipolar_and_Mood_disorder\", \"ethnicity_Dutch\", \"ethnicity_other\"\n",
    "]\n",
    "\n",
    "\n",
    "covariates_CAT_NSAIDs = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline', 'CAT_Anti_epileptica',\n",
    "    'CAT_Antidepressiva',\n",
    "    'CAT_Antipsychotica', 'CAT_Benzodiazepine', 'CAT_Opioden', 'CAT_Z_drugs',\n",
    "    'DIAGNOSIS_CHILDHOOD_TRAUMA', 'DIAGNOSIS_CPTSD', 'DIAGNOSIS_SEXUAL_TRAUMA',\n",
    "    'DIAGNOSIS_SMOKING', 'DIAGNOSIS_SUICIDALITY', 'age',\n",
    "    'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"EB_NON_TF_THERAPY\", \"OTHER_TREATM_APPROACH\", \"Bipolar_and_Mood_disorder\", \"ethnicity_Dutch\", \"ethnicity_other\"\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "covariates_CAT_Benzodiazepine = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline', 'CAT_Anti_epileptica',\n",
    "    'CAT_Antidepressiva',\n",
    "    'CAT_Antipsychotica', 'CAT_Opioden', 'CAT_Z_drugs',\n",
    "    'DIAGNOSIS_CHILDHOOD_TRAUMA', 'DIAGNOSIS_CPTSD', 'DIAGNOSIS_SEXUAL_TRAUMA',\n",
    "    'DIAGNOSIS_SMOKING', 'DIAGNOSIS_SUICIDALITY', 'age',\n",
    "    'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"EB_NON_TF_THERAPY\", \"OTHER_TREATM_APPROACH\", \"Bipolar_and_Mood_disorder\", \"ethnicity_Dutch\", \"ethnicity_other\"\n",
    "]\n",
    "\n",
    "\n",
    "covariates_CAT_Antihypertensiva = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline', 'CAT_Anti_epileptica',\n",
    "    'CAT_Antidepressiva', 'CAT_Antipsychotica',\n",
    "    'CAT_Benzodiazepine', 'CAT_Opioden', 'CAT_Z_drugs',\n",
    "    'DIAGNOSIS_CHILDHOOD_TRAUMA', 'DIAGNOSIS_CPTSD', 'DIAGNOSIS_SEXUAL_TRAUMA',\n",
    "    'DIAGNOSIS_SMOKING', 'DIAGNOSIS_SUICIDALITY', 'age',\n",
    "    'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"EB_NON_TF_THERAPY\", \"OTHER_TREATM_APPROACH\", \"Bipolar_and_Mood_disorder\", \"ethnicity_Dutch\", \"ethnicity_other\"\n",
    "]\n",
    "\n",
    "\n",
    "covariates_CAT_Antihistaminica = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline', 'CAT_Anti_epileptica',\n",
    "    'CAT_Antidepressiva', 'CAT_Antipsychotica',\n",
    "    'CAT_Benzodiazepine', 'CAT_Opioden', 'CAT_Z_drugs',\n",
    "    'DIAGNOSIS_CHILDHOOD_TRAUMA', 'DIAGNOSIS_CPTSD', 'DIAGNOSIS_SEXUAL_TRAUMA',\n",
    "    'DIAGNOSIS_SMOKING', 'DIAGNOSIS_SUICIDALITY', 'age',\n",
    "    'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"EB_NON_TF_THERAPY\", \"OTHER_TREATM_APPROACH\", \"Bipolar_and_Mood_disorder\"\n",
    "]\n",
    "\n",
    "\n",
    "covariates_CAT_Anti_epileptica = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline', 'CAT_Antidepressiva', 'CAT_Antipsychotica',\n",
    "    'CAT_Benzodiazepine', 'CAT_Opioden', 'CAT_Z_drugs',\n",
    "    'DIAGNOSIS_CHILDHOOD_TRAUMA', 'DIAGNOSIS_CPTSD', 'DIAGNOSIS_SEXUAL_TRAUMA',\n",
    "    'DIAGNOSIS_SMOKING', 'DIAGNOSIS_SUICIDALITY', 'age',\n",
    "    'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"EB_NON_TF_THERAPY\", \"OTHER_TREATM_APPROACH\", \"Bipolar_and_Mood_disorder\", \"ethnicity_Dutch\", \"ethnicity_other\"\n",
    "]\n",
    "\n",
    "\n",
    "covariates_CAT_Antidepressiva = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline', 'CAT_Anti_epileptica', 'CAT_Antipsychotica',\n",
    "    'CAT_Benzodiazepine', 'CAT_Opioden', 'CAT_Z_drugs',\n",
    "    'DIAGNOSIS_CHILDHOOD_TRAUMA', 'DIAGNOSIS_CPTSD', 'DIAGNOSIS_SEXUAL_TRAUMA',\n",
    "    'DIAGNOSIS_SMOKING', 'DIAGNOSIS_SUICIDALITY', 'age',\n",
    "    'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"EB_NON_TF_THERAPY\", \"OTHER_TREATM_APPROACH\", \"Bipolar_and_Mood_disorder\", \"ethnicity_Dutch\", \"ethnicity_other\"\n",
    "]\n",
    "\n",
    "\n",
    "covariates_CAT_Antipsychotica = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline', 'CAT_Anti_epileptica',\n",
    "    'CAT_Antidepressiva',\n",
    "    'CAT_Benzodiazepine', 'CAT_Opioden', 'CAT_Z_drugs',\n",
    "    'DIAGNOSIS_CHILDHOOD_TRAUMA', 'DIAGNOSIS_CPTSD', 'DIAGNOSIS_SEXUAL_TRAUMA',\n",
    "    'DIAGNOSIS_SMOKING', 'DIAGNOSIS_SUICIDALITY', 'age',\n",
    "    'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"EB_NON_TF_THERAPY\", \"OTHER_TREATM_APPROACH\", \"Bipolar_and_Mood_disorder\", \"ethnicity_Dutch\", \"ethnicity_other\"\n",
    "]\n",
    "\n",
    "covariates_CAT_ALL_PSYCHOTROPICS_EXCL_BENZO = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline',\n",
    "    'CAT_Benzodiazepine', 'CAT_Anticonceptiva',\n",
    "    'DIAGNOSIS_CHILDHOOD_TRAUMA', 'DIAGNOSIS_CPTSD', 'DIAGNOSIS_SEXUAL_TRAUMA',\n",
    "    'DIAGNOSIS_SMOKING', 'DIAGNOSIS_SUICIDALITY', 'age',\n",
    "    'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"EB_NON_TF_THERAPY\", \"OTHER_TREATM_APPROACH\", \"Bipolar_and_Mood_disorder\", \"ethnicity_Dutch\", \"ethnicity_other\"\n",
    "]\n",
    "\n",
    "covariates_CAT_ALL_PSYCHOTROPICS_EXCL_SEDATIVES_HYPNOTICS = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline',\n",
    "    'CAT_Benzodiazepine', 'CAT_Z_drugs', 'CAT_Anticonceptiva',\n",
    "    'DIAGNOSIS_CHILDHOOD_TRAUMA', 'DIAGNOSIS_CPTSD', 'DIAGNOSIS_SEXUAL_TRAUMA',\n",
    "    'DIAGNOSIS_SMOKING', 'DIAGNOSIS_SUICIDALITY', 'age',\n",
    "    'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"EB_NON_TF_THERAPY\", \"OTHER_TREATM_APPROACH\", \"Bipolar_and_Mood_disorder\", \"ethnicity_Dutch\", \"ethnicity_other\"\n",
    "]\n",
    "\n",
    "\n",
    "covariates_CAT_ALL_PSYCHOTROPICS = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline',\n",
    "    'DIAGNOSIS_CHILDHOOD_TRAUMA', 'DIAGNOSIS_CPTSD', 'DIAGNOSIS_SEXUAL_TRAUMA',\n",
    "    'DIAGNOSIS_SMOKING', 'DIAGNOSIS_SUICIDALITY', 'age',\n",
    "    'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"EB_NON_TF_THERAPY\", \"OTHER_TREATM_APPROACH\", \"Bipolar_and_Mood_disorder\", \"ethnicity_Dutch\", \"ethnicity_other\"\n",
    "]\n",
    "\n",
    "\n",
    "covariates_CAT_ALL = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline',\n",
    "    'DIAGNOSIS_CHILDHOOD_TRAUMA',\n",
    "    'DIAGNOSIS_CPTSD', 'DIAGNOSIS_SEXUAL_TRAUMA',\n",
    "    'DIAGNOSIS_SMOKING', 'DIAGNOSIS_SUICIDALITY',\n",
    "    'age', 'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"EB_NON_TF_THERAPY\", \"OTHER_TREATM_APPROACH\", \"Bipolar_and_Mood_disorder\", \"ethnicity_Dutch\", \"ethnicity_other\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8659b20-ab67-4388-bd8f-11c2dedcb4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# This finds all variables that start with covariates_CAT_ or covariates_cat_\n",
    "final_covariates_map = defaultdict(list)\n",
    "final_covariates_map.update({\n",
    "    var.replace(\"covariates_\", \"\"): val\n",
    "    for var, val in globals().items()\n",
    "    if var.lower().startswith(\"covariates_cat_\") and isinstance(val, list)\n",
    "})\n",
    "\n",
    "# Show detected group names\n",
    "print(\"Groups found:\", list(final_covariates_map.keys()))\n",
    "medication_groups = list(final_covariates_map.keys())\n",
    "print(medication_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8c8ff1-c3d0-48d3-9fbf-98a6762e001e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "def run_all_CAT_group_models(imputed_dfs):\n",
    "    \"\"\"\n",
    "    Runs downstream analysis for each CAT medication group using imputed datasets.\n",
    "\n",
    "    Parameters:\n",
    "    - imputed_dfs: list of 5 imputed DataFrames (from df_imputed_final_imp1.pkl ... imp5.pkl)\n",
    "    \n",
    "    Notes:\n",
    "    - Covariate lists must be defined as global variables: covariates_cat_<group>\n",
    "    - Outputs are saved in: outputs/CAT_<GROUP>/\n",
    "    \"\"\"\n",
    "\n",
    "    print(\" Starting analysis for all CAT groups\")\n",
    "\n",
    "    for var_name in globals():\n",
    "        if var_name.lower().startswith(\"covariates_cat_\") and isinstance(globals()[var_name], list):\n",
    "            group_name = var_name.replace(\"covariates_\", \"\")\n",
    "            group_name = group_name.replace(\"_\", \" \").title().replace(\" \", \"_\")  # e.g., cat_z_drugs → Cat_Z_Drugs\n",
    "            group_name = group_name.replace(\"Cat_\", \"CAT_\")  # force prefix to uppercase\n",
    "\n",
    "            covariates = globals()[var_name]\n",
    "            output_dir = f\"outputs/{group_name}\"\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "            print(f\"\\n Processing {group_name}...\")\n",
    "\n",
    "            for k, df_imp in enumerate(imputed_dfs):\n",
    "                print(f\"  → Using imputation {k+1}\")\n",
    "\n",
    "                # Define X and Y\n",
    "                X = df_imp[covariates]\n",
    "                Y = df_imp[\"caps5_change_baseline\"]\n",
    "\n",
    "                # === Save X and Y as placeholder (replace with modeling later)\n",
    "                X.to_csv(f\"{output_dir}/X_imp{k+1}.csv\", index=False)\n",
    "                Y.to_frame(name=\"Y\").to_csv(f\"{output_dir}/Y_imp{k+1}.csv\", index=False)\n",
    "\n",
    "            print(f\" Done: {group_name}\")\n",
    "\n",
    "    print(\"\\n All CAT group analyses complete.\")\n",
    "\n",
    "# ========= STEP 4: Execute ========= #\n",
    "run_all_CAT_group_models(imputed_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa157356-6578-40bc-a6c3-3f9664d73f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "for treatment_var in medication_groups:\n",
    "    print(f\"\\n {treatment_var}\")\n",
    "    \n",
    "    for i, df in enumerate(imputed_dfs):\n",
    "        if treatment_var not in df.columns:\n",
    "            print(f\"  Imp {i+1}:  Not found in columns.\")\n",
    "            continue\n",
    "\n",
    "        treated = (df[treatment_var] == 1).sum()\n",
    "        control = (df[treatment_var] == 0).sum()\n",
    "        missing = df[treatment_var].isna().sum()\n",
    "\n",
    "        print(f\"  Imp {i+1}: Treated = {treated}, Control = {control}, Missing = {missing}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45518bb5-2213-4061-a9f4-d9dc5a771895",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from collections import defaultdict\n",
    "\n",
    "# ✅ VIF computation function\n",
    "def compute_vif(X):\n",
    "    X = sm.add_constant(X, has_constant='add')\n",
    "    vif_df = pd.DataFrame()\n",
    "    vif_df[\"variable\"] = X.columns\n",
    "    vif_df[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "    return vif_df\n",
    "\n",
    "# ✅ Process each group\n",
    "for group in medication_groups:\n",
    "    print(f\"\\n🔍 Processing VIF for {group}\")\n",
    "\n",
    "    if group not in final_covariates_map:\n",
    "        print(f\" ⚠️ No covariates found for {group}. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    covariates = final_covariates_map[group]\n",
    "    vif_list = []\n",
    "\n",
    "    for i, df_imp in enumerate(imputed_dfs):\n",
    "        try:\n",
    "            X = df_imp[covariates].copy()\n",
    "            vif_df = compute_vif(X)\n",
    "            vif_df[\"imputation\"] = i + 1\n",
    "            vif_list.append(vif_df)\n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ Failed on imputation {i+1} for {group}: {e}\")\n",
    "\n",
    "    if vif_list:\n",
    "        all_vif = pd.concat(vif_list)\n",
    "        pooled_vif = all_vif.groupby(\"variable\")[\"VIF\"].mean().reset_index()\n",
    "        pooled_vif = pooled_vif.sort_values(by=\"VIF\", ascending=False)\n",
    "\n",
    "        output_folder = os.path.join(\"outputs\", group)\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "        pooled_vif.to_csv(os.path.join(output_folder, \"pooled_vif.csv\"), index=False)\n",
    "\n",
    "        print(f\" ✅ Saved: {output_folder}/pooled_vif.csv\")\n",
    "    else:\n",
    "        print(f\" ⚠️ Skipped {group}: No valid imputations.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a75c56-8246-4cf6-87ef-07269bd1ce01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ---------- PS Estimation Function ----------\n",
    "def run_xgboost_ps_modeling(imputed_dfs, medication_groups, final_covariates_map):\n",
    "    for group in medication_groups:\n",
    "        print(f\" Running PS estimation for {group}\")\n",
    "\n",
    "        if group not in final_covariates_map:\n",
    "            print(f\" No covariates found for {group}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        output_folder = os.path.join(\"outputs\", group)\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "        covariates = final_covariates_map[group]\n",
    "        ps_matrix = pd.DataFrame()\n",
    "        auc_list = []\n",
    "\n",
    "        for i, df_imp in enumerate(imputed_dfs):\n",
    "            if group not in df_imp.columns:\n",
    "                print(f\" {group} not found in imputed dataset {i+1}. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            X = df_imp[covariates].copy()\n",
    "            T = df_imp[group]\n",
    "\n",
    "            # Drop missing treatment rows\n",
    "            valid_idx = T.dropna().index\n",
    "            X = X.loc[valid_idx]\n",
    "            T = T.loc[valid_idx]\n",
    "\n",
    "            # Train-test split for ROC\n",
    "            X_train, X_test, T_train, T_test = train_test_split(\n",
    "                X, T, stratify=T, test_size=0.3, random_state=42\n",
    "            )\n",
    "\n",
    "            try:\n",
    "                model = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "                model.fit(X_train, T_train)\n",
    "\n",
    "                ps_scores = model.predict_proba(X)[:, 1]\n",
    "                ps_matrix[f\"ps_imp{i+1}\"] = pd.Series(ps_scores, index=valid_idx)\n",
    "\n",
    "                # ROC & AUC\n",
    "                auc = roc_auc_score(T_test, model.predict_proba(X_test)[:, 1])\n",
    "                auc_list.append(auc)\n",
    "\n",
    "                fpr, tpr, _ = roc_curve(T_test, model.predict_proba(X_test)[:, 1])\n",
    "                plt.figure()\n",
    "                plt.plot(fpr, tpr, label=f\"AUC = {auc:.3f}\")\n",
    "                plt.plot([0, 1], [0, 1], 'k--')\n",
    "                plt.xlabel(\"False Positive Rate\")\n",
    "                plt.ylabel(\"True Positive Rate\")\n",
    "                plt.title(f\"ROC Curve - {group} (Imp {i+1})\")\n",
    "                plt.legend()\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(os.path.join(output_folder, f\"roc_curve_imp{i+1}.png\"))\n",
    "                plt.close()\n",
    "                print(f\"   Imp {i+1}: AUC = {auc:.3f}, ROC saved.\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"   Error in {group} (imp {i+1}): {e}\")\n",
    "\n",
    "        # Save AUCs and Composite PS\n",
    "        if not ps_matrix.empty:\n",
    "            # Fill NaN rows (from dropped subjects in some imputations) with mean\n",
    "            ps_matrix[\"composite_ps\"] = ps_matrix.mean(axis=1)\n",
    "            ps_matrix.to_excel(os.path.join(output_folder, \"propensity_scores.xlsx\"))\n",
    "\n",
    "            auc_df = pd.DataFrame({\n",
    "                \"imputation\": [f\"imp{i+1}\" for i in range(len(auc_list))],\n",
    "                \"AUC\": auc_list\n",
    "            })\n",
    "            auc_df.loc[len(auc_df.index)] = [\"mean\", np.mean(auc_list) if auc_list else np.nan]\n",
    "            auc_df.to_excel(os.path.join(output_folder, \"auc_scores.xlsx\"), index=False)\n",
    "\n",
    "            print(f\" Composite PS + AUC saved for {group}\")\n",
    "        else:\n",
    "            print(f\" No valid PS scores generated for {group}\")\n",
    "\n",
    "# ---------- Run ----------\n",
    "run_xgboost_ps_modeling(imputed_dfs, medication_groups, final_covariates_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a181c2a-5bec-4510-9d28-a7637b4615e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "def compute_feature_importance(imputed_dfs, medication_groups, final_covariates_map):\n",
    "    for group in medication_groups:\n",
    "        print(f\"\\n Computing feature importance for {group}\")\n",
    "\n",
    "        if group not in final_covariates_map:\n",
    "            print(f\" No covariates found for {group}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        covariates = final_covariates_map[group]\n",
    "        output_folder = os.path.join(\"outputs\", group)\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "        importance_df_list = []\n",
    "\n",
    "        for i, df_imp in enumerate(imputed_dfs):\n",
    "            if group not in df_imp.columns:\n",
    "                print(f\" {group} not in imputed dataset {i+1}. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            X = df_imp[covariates].copy()\n",
    "            T = df_imp[group]\n",
    "\n",
    "            # Drop NaNs in treatment\n",
    "            valid_idx = T.dropna().index\n",
    "            X = X.loc[valid_idx]\n",
    "            T = T.loc[valid_idx]\n",
    "\n",
    "            try:\n",
    "                model = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "                model.fit(X, T)\n",
    "\n",
    "                # Get feature importance\n",
    "                importances = model.get_booster().get_score(importance_type='gain')\n",
    "                df_feat = pd.DataFrame.from_dict(importances, orient='index', columns=[f\"imp{i+1}\"])\n",
    "                df_feat.index.name = 'feature'\n",
    "                importance_df_list.append(df_feat)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"   Error during modeling: {e}\")\n",
    "\n",
    "        if importance_df_list:\n",
    "            # Combine and average\n",
    "            all_feat = pd.concat(importance_df_list, axis=1).fillna(0)\n",
    "            all_feat[\"mean_importance\"] = all_feat.mean(axis=1)\n",
    "\n",
    "            # Filter top 30 non-zero\n",
    "            non_zero = all_feat[all_feat[\"mean_importance\"] > 0]\n",
    "            top30 = non_zero.sort_values(by=\"mean_importance\", ascending=False).head(30)\n",
    "\n",
    "            # Save to CSV\n",
    "            top30.to_csv(os.path.join(output_folder, \"feature_importance.csv\"))\n",
    "\n",
    "            # Plot\n",
    "            plt.figure(figsize=(10, 8))\n",
    "            plt.barh(top30.index[::-1], top30[\"mean_importance\"][::-1])  # plot top → bottom\n",
    "            plt.xlabel(\"Mean Gain Importance\")\n",
    "            plt.title(f\"Top 30 Feature Importance - {group}\")\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(output_folder, \"feature_importance_top30.png\"))\n",
    "            plt.close()\n",
    "\n",
    "            print(f\" Saved feature importance plot and CSV for {group}\")\n",
    "        else:\n",
    "            print(f\" No valid models for {group}\")\n",
    "\n",
    "#  Run\n",
    "compute_feature_importance(imputed_dfs, medication_groups, final_covariates_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df86de76-a391-45d6-a5e2-06d6390c6a1a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def compute_trimmed_clipped_iptw(ps_df, treatment, lower=0.05, upper=0.95, clip_max=10):\n",
    "    weights = []\n",
    "    keep_mask = (ps_df > lower) & (ps_df < upper)\n",
    "\n",
    "    for i in range(ps_df.shape[1]):\n",
    "        ps = ps_df.iloc[:, i].clip(lower=1e-6, upper=1 - 1e-6)  # avoid div by zero\n",
    "        mask = keep_mask.iloc[:, i]\n",
    "        w = pd.Series(np.nan, index=ps.index)\n",
    "\n",
    "        w[mask & (treatment == 1)] = 1 / ps[mask & (treatment == 1)]\n",
    "        w[mask & (treatment == 0)] = 1 / (1 - ps[mask & (treatment == 0)])\n",
    "        w = w.clip(upper=clip_max)\n",
    "        weights.append(w)\n",
    "\n",
    "    return pd.concat(weights, axis=1)\n",
    "\n",
    "\n",
    "def apply_rubins_rule_to_iptw(iptw_matrix):\n",
    "    \"\"\"\n",
    "    Given an IPTW matrix (n rows × M imputations), return Rubin’s rule pooled mean, SD, SE.\n",
    "    \"\"\"\n",
    "    M = iptw_matrix.shape[1]\n",
    "    q_bar = iptw_matrix.mean(axis=1)\n",
    "    u_bar = iptw_matrix.var(axis=1, ddof=1)\n",
    "    B = iptw_matrix.apply(lambda x: x.mean(), axis=1).var(ddof=1)\n",
    "    total_var = u_bar + (1 + 1/M) * B\n",
    "    total_se = np.sqrt(total_var)\n",
    "    return q_bar, u_bar.pow(0.5), total_se\n",
    "\n",
    "\n",
    "def run_trim_clip_save_all(imputed_dfs, medication_groups, final_covariates_map):\n",
    "    for group in medication_groups:\n",
    "        print(f\"\\n🔍 Processing IPTW + trimming + clipping for {group}\")\n",
    "        output_folder = os.path.join(\"outputs\", group)\n",
    "        ps_path = os.path.join(output_folder, \"propensity_scores.xlsx\")\n",
    "\n",
    "        if not os.path.exists(ps_path):\n",
    "            print(f\"⚠️ Missing PS file: {ps_path}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            ps_all = pd.read_excel(ps_path, index_col=0)\n",
    "            ps_cols = [col for col in ps_all.columns if col.startswith(\"ps_imp\")]\n",
    "            composite_index = ps_all.index\n",
    "\n",
    "            # Get treatment from one imputed dataset\n",
    "            T_full = None\n",
    "            for df in imputed_dfs:\n",
    "                if group in df.columns:\n",
    "                    T_full = df.loc[composite_index, group]\n",
    "                    break\n",
    "\n",
    "            if T_full is None:\n",
    "                print(f\"❌ Treatment column {group} not found in any imputed dataset.\")\n",
    "                continue\n",
    "\n",
    "            # Compute IPTW matrix (shape: n × M)\n",
    "            iptw_matrix = compute_trimmed_clipped_iptw(ps_all[ps_cols], T_full)\n",
    "            iptw_matrix.columns = [f\"iptw_imp{i+1}\" for i in range(iptw_matrix.shape[1])]\n",
    "\n",
    "            # Apply Rubin’s Rule for mean, SD, SE\n",
    "            iptw_matrix[\"iptw_mean\"], iptw_matrix[\"iptw_sd\"], iptw_matrix[\"iptw_se\"] = apply_rubins_rule_to_iptw(\n",
    "                iptw_matrix[[f\"iptw_imp{i+1}\" for i in range(iptw_matrix.shape[1])]]\n",
    "            )\n",
    "\n",
    "            # Save IPTW matrix separately\n",
    "            iptw_matrix.to_excel(os.path.join(output_folder, \"iptw_weights.xlsx\"))\n",
    "            print(f\"✅ Saved IPTW weights for {group}\")\n",
    "\n",
    "            # Save trimmed & clipped imputed datasets with IPTW\n",
    "            for i in range(5):\n",
    "                df = imputed_dfs[i].copy()\n",
    "                if group not in df.columns:\n",
    "                    continue\n",
    "\n",
    "                trimmed_idx = iptw_matrix.index.intersection(df.index)\n",
    "                needed_cols = final_covariates_map[group] + [group, \"caps5_change_baseline\"]\n",
    "\n",
    "                # Select only necessary columns\n",
    "                df_trimmed = df.loc[trimmed_idx, needed_cols].copy()\n",
    "                df_trimmed[\"iptw\"] = iptw_matrix[f\"iptw_imp{i+1}\"].loc[trimmed_idx]\n",
    "\n",
    "                # ✅ DROP rows with missing IPTW values\n",
    "                before = len(df_trimmed)\n",
    "                df_trimmed = df_trimmed.dropna(subset=[\"iptw\"])\n",
    "                after = len(df_trimmed)\n",
    "                print(f\"    ℹ️ Retained {after}/{before} rows after IPTW NaN drop.\")\n",
    "\n",
    "                # Save to .pkl\n",
    "                df_trimmed.to_pickle(os.path.join(output_folder, f\"trimmed_data_imp{i+1}.pkl\"))\n",
    "                print(f\"  💾 Saved trimmed dataset: {output_folder}/trimmed_data_imp{i+1}.*\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error in {group}: {e}\")\n",
    "\n",
    "\n",
    "run_trim_clip_save_all(imputed_dfs, medication_groups, final_covariates_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed308b6-fe68-43f9-aa75-565caf5adf0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_ps_overlap_all_groups(medication_groups):\n",
    "    for group in medication_groups:\n",
    "        print(f\"\\n📊 Plotting PS overlap for {group}\")\n",
    "\n",
    "        folder = os.path.join(\"outputs\", group)\n",
    "        ps_file = os.path.join(folder, \"propensity_scores.xlsx\")\n",
    "        iptw_file = os.path.join(folder, \"iptw_weights.xlsx\")\n",
    "        trimmed_file = os.path.join(folder, \"trimmed_data_imp1.pkl\")\n",
    "\n",
    "        if not all(os.path.exists(f) for f in [ps_file, iptw_file, trimmed_file]):\n",
    "            print(f\"⚠️ Missing required files for {group}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            ps_df = pd.read_excel(ps_file, index_col=0)\n",
    "            iptw_df = pd.read_excel(iptw_file, index_col=0)\n",
    "            trimmed_df = pd.read_pickle(trimmed_file)\n",
    "\n",
    "            # Extract\n",
    "            ps = ps_df[\"composite_ps\"].reindex(trimmed_df.index)\n",
    "            w = iptw_df[\"iptw_mean\"].reindex(trimmed_df.index)\n",
    "            T = trimmed_df[group]\n",
    "\n",
    "            # Masks to remove NaNs\n",
    "            treated_mask = (T == 1) & ps.notna() & w.notna()\n",
    "            control_mask = (T == 0) & ps.notna() & w.notna()\n",
    "\n",
    "            treated = ps[treated_mask]\n",
    "            treated_w = w[treated_mask]\n",
    "\n",
    "            control = ps[control_mask]\n",
    "            control_w = w[control_mask]\n",
    "\n",
    "            # === Unweighted Plot ===\n",
    "            plt.figure(figsize=(8, 5))\n",
    "            plt.hist([treated, control], bins=25, label=[\"Treated\", \"Control\"], alpha=0.6)\n",
    "            plt.title(f\"Unweighted PS Overlap - {group}\")\n",
    "            plt.xlabel(\"Composite Propensity Score\")\n",
    "            plt.ylabel(\"Count\")\n",
    "            plt.legend()\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(folder, \"ps_overlap_unweighted.png\"))\n",
    "            plt.close()\n",
    "\n",
    "            # === Weighted Plot ===\n",
    "            plt.figure(figsize=(8, 5))\n",
    "            plt.hist([treated, control], bins=25, weights=[treated_w, control_w], label=[\"Treated\", \"Control\"], alpha=0.6)\n",
    "            plt.title(f\"Weighted PS Overlap - {group}\")\n",
    "            plt.xlabel(\"Composite Propensity Score\")\n",
    "            plt.ylabel(\"Weighted Count\")\n",
    "            plt.legend()\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(folder, \"ps_overlap_weighted.png\"))\n",
    "            plt.close()\n",
    "\n",
    "            print(f\"✅ Saved unweighted and weighted PS plots for {group}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error processing {group}: {e}\")\n",
    "\n",
    "# 🔁 Run\n",
    "plot_ps_overlap_all_groups(medication_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ea4dea-63df-45a3-96a0-4fa1500801cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set up base output folder\n",
    "output_base = \"outputs\"\n",
    "ps_file = \"propensity_scores.xlsx\"\n",
    "iptw_file = \"iptw_weights.xlsx\"\n",
    "trimmed_data_file = \"trimmed_data_imp1.pkl\"\n",
    "\n",
    "# Collect all treatment group folders\n",
    "groups = [g for g in medication_groups if os.path.isdir(os.path.join(output_base, g))]\n",
    "\n",
    "# Generate 4-panel overlap plots\n",
    "for group in groups:\n",
    "    group_path = os.path.join(output_base, group)\n",
    "    try:\n",
    "        # Load trimmed treatment info\n",
    "        trimmed_df = pd.read_pickle(os.path.join(group_path, trimmed_data_file))\n",
    "        index = trimmed_df.index\n",
    "\n",
    "        # Fix: case-insensitive match for treatment variable\n",
    "        possible_cols = [col for col in trimmed_df.columns if col.upper() == group.upper()]\n",
    "        if not possible_cols:\n",
    "            print(f\" Treatment variable {group} not found in {group}, skipping.\")\n",
    "            continue\n",
    "        treatment_var = possible_cols[0]\n",
    "        T = trimmed_df[treatment_var]\n",
    "\n",
    "        # Load composite PS (aligned to trimmed_df index)\n",
    "        ps_df = pd.read_excel(os.path.join(group_path, ps_file), index_col=0)\n",
    "        if 'composite_ps' not in ps_df.columns:\n",
    "            print(f\" Composite column missing in {ps_file}, skipping {group}.\")\n",
    "            continue\n",
    "        ps = ps_df.loc[index, 'composite_ps']\n",
    "\n",
    "        # Load IPTW weights (aligned to trimmed_df index)\n",
    "        weights_df = pd.read_excel(os.path.join(group_path, iptw_file), index_col=0)\n",
    "        if 'iptw_mean' not in weights_df.columns:\n",
    "            print(f\" IPTW weight column missing in {iptw_file}, skipping {group}.\")\n",
    "            continue\n",
    "        weights = weights_df.loc[index, 'iptw_mean']\n",
    "\n",
    "        # Prepare 4 datasets\n",
    "        raw_treated = ps[T == 1]\n",
    "        raw_control = ps[T == 0]\n",
    "        weighted_treated = (ps[T == 1], weights[T == 1])\n",
    "        weighted_control = (ps[T == 0], weights[T == 0])\n",
    "\n",
    "        # Create plot\n",
    "        fig, axs = plt.subplots(2, 2, figsize=(12, 8))\n",
    "        fig.suptitle(f\"Propensity Score Distribution - {group}\", fontsize=14)\n",
    "\n",
    "        axs[0, 0].hist(raw_treated, bins=20, alpha=0.7, color='blue')\n",
    "        axs[0, 0].set_title(\"Raw Treated\")\n",
    "\n",
    "        axs[0, 1].hist(raw_control, bins=20, alpha=0.7, color='green')\n",
    "        axs[0, 1].set_title(\"Raw Control\")\n",
    "\n",
    "        axs[1, 0].hist(weighted_treated[0], bins=20, weights=weighted_treated[1], alpha=0.7, color='blue')\n",
    "        axs[1, 0].set_title(\"Weighted Treated\")\n",
    "\n",
    "        axs[1, 1].hist(weighted_control[0], bins=20, weights=weighted_control[1], alpha=0.7, color='green')\n",
    "        axs[1, 1].set_title(\"Weighted Control\")\n",
    "\n",
    "        for ax in axs.flat:\n",
    "            ax.set_xlim(0, 1)\n",
    "            ax.set_xlabel(\"Propensity Score\")\n",
    "            ax.set_ylabel(\"Count\")\n",
    "\n",
    "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "\n",
    "        # Save figure\n",
    "        plot_path = os.path.join(group_path, f\"four_panel_overlap_{group}.png\")\n",
    "        plt.savefig(plot_path)\n",
    "        plt.close()\n",
    "        print(f\" ✅ Saved: {plot_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\" ❌ Error in {group}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84753833-c355-4cd9-91f1-f0d0f66b7705",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ATT calculation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac32062-ff51-4b7d-9ac8-94e71a1e7534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weighted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be14728-16ba-4ae3-8077-ec1c895d8b10",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import KFold\n",
    "from scipy.stats import t, probplot\n",
    "import xgboost as xgb\n",
    "\n",
    "# -----------------------------\n",
    "# Configuration\n",
    "# -----------------------------\n",
    "n_repeats = 1\n",
    "seeds = list(range(1, 11))\n",
    "imputations = 5\n",
    "output_folder = \"outputs\"\n",
    "\n",
    "# -----------------------------\n",
    "# Diagnostic Plotting Function\n",
    "# -----------------------------\n",
    "def create_diagnostic_plots(residuals_data, fitted_data, group_name):\n",
    "    \"\"\"Create 4 diagnostic plots for model validation\"\"\"\n",
    "    plots_dir = os.path.join(output_folder, \"plots\")\n",
    "    os.makedirs(plots_dir, exist_ok=True)\n",
    "    \n",
    "    # Flatten the collected data\n",
    "    all_residuals = np.concatenate(residuals_data)\n",
    "    all_fitted = np.concatenate(fitted_data)\n",
    "    \n",
    "    # Create figure with 2x2 subplots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    fig.suptitle(f'Diagnostic Plots - {group_name}', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Residuals vs Fitted\n",
    "    axes[0,0].scatter(all_fitted, all_residuals, alpha=0.6, s=20)\n",
    "    axes[0,0].axhline(y=0, color='red', linestyle='--', alpha=0.8)\n",
    "    axes[0,0].set_xlabel('Fitted Values')\n",
    "    axes[0,0].set_ylabel('Residuals')\n",
    "    axes[0,0].set_title('Residuals vs Fitted')\n",
    "    axes[0,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. QQ Plot\n",
    "    probplot(all_residuals, dist=\"norm\", plot=axes[0,1])\n",
    "    axes[0,1].set_title('Q-Q Plot (Normal)')\n",
    "    axes[0,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Residual Histogram\n",
    "    axes[1,0].hist(all_residuals, bins=30, density=True, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    axes[1,0].set_xlabel('Residuals')\n",
    "    axes[1,0].set_ylabel('Density')\n",
    "    axes[1,0].set_title('Residual Distribution')\n",
    "    axes[1,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Scale-Location Plot\n",
    "    sqrt_abs_residuals = np.sqrt(np.abs(all_residuals))\n",
    "    axes[1,1].scatter(all_fitted, sqrt_abs_residuals, alpha=0.6, s=20)\n",
    "    axes[1,1].set_xlabel('Fitted Values')\n",
    "    axes[1,1].set_ylabel('√|Residuals|')\n",
    "    axes[1,1].set_title('Scale-Location Plot')\n",
    "    axes[1,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save plot\n",
    "    plot_filename = os.path.join(plots_dir, f'{group_name}.png')\n",
    "    plt.savefig(plot_filename, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"📊 Diagnostic plots saved for {group_name}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Rubin's Rule\n",
    "# -----------------------------\n",
    "def rubins_pool(estimates, ses):\n",
    "    m = len(estimates)\n",
    "    q_bar = np.mean(estimates)\n",
    "    u_bar = np.mean(np.square(ses))\n",
    "    b_m = np.var(estimates, ddof=1)\n",
    "    total_var = u_bar + ((1 + 1/m) * b_m)\n",
    "    total_se = np.sqrt(total_var)\n",
    "    ci_lower = q_bar - 1.96 * total_se\n",
    "    ci_upper = q_bar + 1.96 * total_se\n",
    "    p_value = 2 * (1 - t.cdf(np.abs(q_bar / total_se), df=m-1))\n",
    "    rounded_p = round(p_value, 5)\n",
    "    formatted_p = \"< 0.00001\" if rounded_p <= 0.00001 else f\"{rounded_p:.5f}\"\n",
    "    return q_bar, total_se, ci_lower, ci_upper, formatted_p\n",
    "\n",
    "# -----------------------------\n",
    "# SMD + Variance Ratio\n",
    "# -----------------------------\n",
    "def calculate_smd_vr(X, T, weights):\n",
    "    treated = X[T == 1]\n",
    "    control = X[T == 0]\n",
    "    w_treated = weights[T == 1]\n",
    "    w_control = weights[T == 0]\n",
    "    smd, vr = [], []\n",
    "    for col in X.columns:\n",
    "        try:\n",
    "            m1, m0 = np.average(treated[col], weights=w_treated), np.average(control[col], weights=w_control)\n",
    "            s1 = np.sqrt(np.average((treated[col] - m1) ** 2, weights=w_treated))\n",
    "            s0 = np.sqrt(np.average((control[col] - m0) ** 2, weights=w_control))\n",
    "            pooled_sd = np.sqrt((s1 ** 2 + s0 ** 2) / 2)\n",
    "            smd.append((m1 - m0) / pooled_sd if pooled_sd > 0 else 0)\n",
    "            vr.append(s1**2 / s0**2 if s0**2 > 0 else 0)\n",
    "        except Exception:\n",
    "            smd.append(np.nan)\n",
    "            vr.append(np.nan)\n",
    "    return np.nanmean(smd), np.nanmean(vr)\n",
    "\n",
    "# -----------------------------\n",
    "# XGBoost Main Loop (No DML)\n",
    "# -----------------------------\n",
    "def run_xgboost_with_trimmed_data(final_covariates_map):\n",
    "    att_results = []\n",
    "    balance_results = []\n",
    "\n",
    "    for group, covariates in final_covariates_map.items():\n",
    "        print(f\"\\n🚀 Running XGBoost for {group}\")\n",
    "        group_dir = os.path.join(output_folder, group)\n",
    "        os.makedirs(group_dir, exist_ok=True)\n",
    "\n",
    "        best_result = None\n",
    "        best_se = float(\"inf\")\n",
    "        \n",
    "        # Initialize lists to collect residuals and fitted values for diagnostic plots\n",
    "        group_residuals = []\n",
    "        group_fitted = []\n",
    "\n",
    "        for seed in seeds:\n",
    "            att_list, se_list, r2_list, rmse_list, smd_list, vr_list = [], [], [], [], [], []\n",
    "\n",
    "            for imp in range(1, imputations + 1):\n",
    "                file_path = os.path.join(group_dir, f\"trimmed_data_imp{imp}.pkl\")\n",
    "                if not os.path.exists(file_path):\n",
    "                    continue\n",
    "\n",
    "                df = pd.read_pickle(file_path)\n",
    "                if group not in df.columns or \"iptw\" not in df.columns:\n",
    "                    continue\n",
    "\n",
    "                X = df[covariates].copy()\n",
    "                T = df[group]\n",
    "                Y = df[\"caps5_change_baseline\"]\n",
    "                W = df[\"iptw\"]\n",
    "\n",
    "                for repeat in range(n_repeats):\n",
    "                    kf = KFold(n_splits=5, shuffle=True, random_state=seed + repeat)\n",
    "                    for train_idx, test_idx in kf.split(X):\n",
    "                        try:\n",
    "                            X_train, T_train, Y_train, W_train = (\n",
    "                                X.iloc[train_idx],\n",
    "                                T.iloc[train_idx],\n",
    "                                Y.iloc[train_idx],\n",
    "                                W.iloc[train_idx],\n",
    "                            )\n",
    "\n",
    "                            # XGBoost regression model\n",
    "                            model = xgb.XGBRegressor(n_estimators=100, max_depth=3, learning_rate=0.1, n_jobs=1, random_state=seed)\n",
    "                            \n",
    "                            # Add treatment variable to features\n",
    "                            X_train_with_T = X_train.copy()\n",
    "                            X_train_with_T[group] = T_train\n",
    "                            \n",
    "                            # Fit model\n",
    "                            model.fit(X_train_with_T, Y_train, sample_weight=W_train)\n",
    "                            \n",
    "                            # Predict outcomes for treated and control groups\n",
    "                            X_treated = X_train.copy()\n",
    "                            X_treated[group] = 1\n",
    "                            X_control = X_train.copy()\n",
    "                            X_control[group] = 0\n",
    "                            \n",
    "                            Y_pred_treated = model.predict(X_treated)\n",
    "                            Y_pred_control = model.predict(X_control)\n",
    "                            \n",
    "                            # Calculate ATT (Average Treatment Effect on Treated)\n",
    "                            treated_mask = T_train == 1\n",
    "                            if np.any(treated_mask):\n",
    "                                att = np.average(Y_pred_treated[treated_mask] - Y_pred_control[treated_mask], \n",
    "                                               weights=W_train[treated_mask])\n",
    "                                \n",
    "                                # Calculate standard error (approximate)\n",
    "                                treatment_effects = Y_pred_treated[treated_mask] - Y_pred_control[treated_mask]\n",
    "                                residual = treatment_effects - att\n",
    "                                se = np.sqrt(np.sum((W_train[treated_mask] * residual) ** 2)) / np.sum(W_train[treated_mask])\n",
    "\n",
    "                                \n",
    "                                att_list.append(att)\n",
    "                                se_list.append(se)\n",
    "\n",
    "                            # Model performance metrics\n",
    "                            Y_pred = model.predict(X_train_with_T)\n",
    "                            residuals = Y_train - Y_pred\n",
    "                            rmse = mean_squared_error(Y_train, Y_pred, squared=False)\n",
    "                            r2 = r2_score(Y_train, Y_pred)\n",
    "                            r2_list.append(r2)\n",
    "                            rmse_list.append(rmse)\n",
    "                            \n",
    "                            # Collect residuals and fitted values for diagnostic plots\n",
    "                            group_residuals.append(residuals.values)\n",
    "                            group_fitted.append(Y_pred)\n",
    "\n",
    "                            smd, vr = calculate_smd_vr(X_train, T_train, W_train)\n",
    "                            smd_list.append(smd)\n",
    "                            vr_list.append(vr)\n",
    "                        except Exception as e:\n",
    "                            print(f\"⚠️ Error in {group}, seed {seed}, imp {imp}, rep {repeat}: {e}\")\n",
    "\n",
    "            if att_list:\n",
    "                att, se, ci_l, ci_u, p_val = rubins_pool(att_list, se_list)\n",
    "                avg_r2 = np.mean(r2_list)\n",
    "                avg_rmse = np.mean(rmse_list)\n",
    "                avg_smd = np.mean(smd_list)\n",
    "                avg_vr = np.mean(vr_list)\n",
    "\n",
    "                balance_results.append({\n",
    "                    \"group\": group, \"seed\": seed, \"smd\": avg_smd, \"vr\": avg_vr\n",
    "                })\n",
    "\n",
    "                if se < best_se:\n",
    "                    best_se = se\n",
    "                    best_result = {\n",
    "                        \"group\": group, \"seed\": seed, \"att\": att, \"se\": se,\n",
    "                        \"ci_lower\": ci_l, \"ci_upper\": ci_u, \"p_value\": p_val,\n",
    "                        \"r2\": avg_r2, \"rmse\": avg_rmse\n",
    "                    }\n",
    "\n",
    "                print(f\"✅ {group} | Seed {seed}: ATT = {att:.4f}, SE = {se:.4f}, p = {p_val}\")\n",
    "            else:\n",
    "                print(f\"⚠️ No valid results for {group} | Seed {seed}\")\n",
    "\n",
    "        # Create diagnostic plots for this group\n",
    "        if group_residuals and group_fitted:\n",
    "            create_diagnostic_plots(group_residuals, group_fitted, group)\n",
    "\n",
    "        if best_result:\n",
    "            att_results.append(best_result)\n",
    "            print(f\"🏆 Best result for {group} → Seed {best_result['seed']} | SE = {best_result['se']:.4f}\")\n",
    "\n",
    "    # Save final output\n",
    "    pd.DataFrame(att_results).to_excel(\"xgb_rubin_summary_cats.xlsx\", index=False)\n",
    "    pd.DataFrame(balance_results).to_excel(\"smd_vr_summary_cats.xlsx\", index=False)\n",
    "    print(\"\\n🎯 All summary files saved.\")\n",
    "\n",
    "run_xgboost_with_trimmed_data(final_covariates_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accbc7d0-83ff-4a42-a304-f9cd50f5031b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unweighted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2031c63f-4f0b-4ead-acae-9a083dbea569",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import KFold\n",
    "from scipy.stats import t, probplot\n",
    "import xgboost as xgb\n",
    "\n",
    "# -----------------------------\n",
    "# Configuration\n",
    "# -----------------------------\n",
    "n_repeats = 1\n",
    "seeds = list(range(1, 11))\n",
    "imputations = 5\n",
    "output_folder = \"outputs\"\n",
    "\n",
    "# -----------------------------\n",
    "# Diagnostic Plotting Function\n",
    "# -----------------------------\n",
    "def create_diagnostic_plots(residuals_data, fitted_data, group_name):\n",
    "    \"\"\"Create 4 diagnostic plots for model validation\"\"\"\n",
    "    plots_dir = os.path.join(output_folder, \"plots\")\n",
    "    os.makedirs(plots_dir, exist_ok=True)\n",
    "    \n",
    "    # Flatten the collected data\n",
    "    all_residuals = np.concatenate(residuals_data)\n",
    "    all_fitted = np.concatenate(fitted_data)\n",
    "    \n",
    "    # Create figure with 2x2 subplots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    fig.suptitle(f'Diagnostic Plots - {group_name}', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Residuals vs Fitted\n",
    "    axes[0,0].scatter(all_fitted, all_residuals, alpha=0.6, s=20)\n",
    "    axes[0,0].axhline(y=0, color='red', linestyle='--', alpha=0.8)\n",
    "    axes[0,0].set_xlabel('Fitted Values')\n",
    "    axes[0,0].set_ylabel('Residuals')\n",
    "    axes[0,0].set_title('Residuals vs Fitted')\n",
    "    axes[0,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. QQ Plot\n",
    "    probplot(all_residuals, dist=\"norm\", plot=axes[0,1])\n",
    "    axes[0,1].set_title('Q-Q Plot (Normal)')\n",
    "    axes[0,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Residual Histogram\n",
    "    axes[1,0].hist(all_residuals, bins=30, density=True, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    axes[1,0].set_xlabel('Residuals')\n",
    "    axes[1,0].set_ylabel('Density')\n",
    "    axes[1,0].set_title('Residual Distribution')\n",
    "    axes[1,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Scale-Location Plot\n",
    "    sqrt_abs_residuals = np.sqrt(np.abs(all_residuals))\n",
    "    axes[1,1].scatter(all_fitted, sqrt_abs_residuals, alpha=0.6, s=20)\n",
    "    axes[1,1].set_xlabel('Fitted Values')\n",
    "    axes[1,1].set_ylabel('√|Residuals|')\n",
    "    axes[1,1].set_title('Scale-Location Plot')\n",
    "    axes[1,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save plot\n",
    "    plot_filename = os.path.join(plots_dir, f'{group_name}_unweighted.png')\n",
    "    plt.savefig(plot_filename, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"📊 Diagnostic plots saved for {group_name}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Rubin's Rule\n",
    "# -----------------------------\n",
    "def rubins_pool(estimates, ses):\n",
    "    m = len(estimates)\n",
    "    q_bar = np.mean(estimates)\n",
    "    u_bar = np.mean(np.square(ses))\n",
    "    b_m = np.var(estimates, ddof=1)\n",
    "    total_var = u_bar + ((1 + 1/m) * b_m)\n",
    "    total_se = np.sqrt(total_var)\n",
    "    ci_lower = q_bar - 1.96 * total_se\n",
    "    ci_upper = q_bar + 1.96 * total_se\n",
    "    p_value = 2 * (1 - t.cdf(np.abs(q_bar / total_se), df=m-1))\n",
    "    rounded_p = round(p_value, 5)\n",
    "    formatted_p = \"< 0.00001\" if rounded_p <= 0.00001 else f\"{rounded_p:.5f}\"\n",
    "    return q_bar, total_se, ci_lower, ci_upper, formatted_p\n",
    "\n",
    "# -----------------------------\n",
    "# SMD + Variance Ratio\n",
    "# -----------------------------\n",
    "def calculate_smd_vr(X, T):\n",
    "    treated = X[T == 1]\n",
    "    control = X[T == 0]\n",
    "    smd, vr = [], []\n",
    "    for col in X.columns:\n",
    "        try:\n",
    "            m1, m0 = np.mean(treated[col]), np.mean(control[col])\n",
    "            s1 = np.std(treated[col])\n",
    "            s0 = np.std(control[col])\n",
    "            pooled_sd = np.sqrt((s1 ** 2 + s0 ** 2) / 2)\n",
    "            smd.append((m1 - m0) / pooled_sd if pooled_sd > 0 else 0)\n",
    "            vr.append(s1**2 / s0**2 if s0**2 > 0 else 0)\n",
    "        except Exception:\n",
    "            smd.append(np.nan)\n",
    "            vr.append(np.nan)\n",
    "    return np.nanmean(smd), np.nanmean(vr)\n",
    "\n",
    "# -----------------------------\n",
    "# XGBoost Main Loop (No DML)\n",
    "# -----------------------------\n",
    "def run_xgboost_with_trimmed_data(final_covariates_map):\n",
    "    att_results = []\n",
    "    balance_results = []\n",
    "\n",
    "    for group, covariates in final_covariates_map.items():\n",
    "        print(f\"\\n🚀 Running XGBoost for {group}\")\n",
    "        group_dir = os.path.join(output_folder, group)\n",
    "        os.makedirs(group_dir, exist_ok=True)\n",
    "\n",
    "        best_result = None\n",
    "        best_se = float(\"inf\")\n",
    "        \n",
    "        # Initialize lists to collect residuals and fitted values for diagnostic plots\n",
    "        group_residuals = []\n",
    "        group_fitted = []\n",
    "\n",
    "        for seed in seeds:\n",
    "            att_list, se_list, r2_list, rmse_list, smd_list, vr_list = [], [], [], [], [], []\n",
    "\n",
    "            for imp in range(1, imputations + 1):\n",
    "                file_path = os.path.join(group_dir, f\"trimmed_data_imp{imp}.pkl\")\n",
    "                if not os.path.exists(file_path):\n",
    "                    continue\n",
    "\n",
    "                df = pd.read_pickle(file_path)\n",
    "                if group not in df.columns or \"iptw\" not in df.columns:\n",
    "                    continue\n",
    "\n",
    "                X = df[covariates].copy()\n",
    "                T = df[group]\n",
    "                Y = df[\"caps5_change_baseline\"]\n",
    "                #W = df[\"iptw\"]\n",
    "\n",
    "                for repeat in range(n_repeats):\n",
    "                    kf = KFold(n_splits=5, shuffle=True, random_state=seed + repeat)\n",
    "                    for train_idx, test_idx in kf.split(X):\n",
    "                        try:\n",
    "                            X_train = X.iloc[train_idx]\n",
    "                            T_train = T.iloc[train_idx]\n",
    "                            Y_train = Y.iloc[train_idx]\n",
    "                        \n",
    "\n",
    "                            # XGBoost regression model\n",
    "                            model = xgb.XGBRegressor(n_estimators=100, max_depth=3, learning_rate=0.1, n_jobs=1, random_state=seed)\n",
    "                            \n",
    "                            # Add treatment variable to features\n",
    "                            X_train_with_T = X_train.copy()\n",
    "                            X_train_with_T[group] = T_train\n",
    "                            \n",
    "                            # Fit model\n",
    "                            model.fit(X_train_with_T, Y_train)\n",
    "                            \n",
    "                            # Predict outcomes for treated and control groups\n",
    "                            X_treated = X_train.copy()\n",
    "                            X_treated[group] = 1\n",
    "                            X_control = X_train.copy()\n",
    "                            X_control[group] = 0\n",
    "                            \n",
    "                            Y_pred_treated = model.predict(X_treated)\n",
    "                            Y_pred_control = model.predict(X_control)\n",
    "                            \n",
    "                            # Calculate ATT (Average Treatment Effect on Treated)\n",
    "                            treated_mask = T_train == 1\n",
    "                            if np.any(treated_mask):\n",
    "                                att = np.mean(Y_pred_treated[treated_mask] - Y_pred_control[treated_mask])\n",
    "                                \n",
    "                                # Calculate standard error (approximate)\n",
    "                                treatment_effects = Y_pred_treated[treated_mask] - Y_pred_control[treated_mask]\n",
    "                                residual = treatment_effects - att\n",
    "                                se = np.sqrt(np.sum((residual) ** 2)) / np.sum(treated_mask)\n",
    "                                att_list.append(att)\n",
    "                                se_list.append(se)\n",
    "\n",
    "                            # Model performance metrics\n",
    "                            Y_pred = model.predict(X_train_with_T)\n",
    "                            residuals = Y_train - Y_pred\n",
    "                            rmse = mean_squared_error(Y_train, Y_pred, squared=False)\n",
    "                            r2 = r2_score(Y_train, Y_pred)\n",
    "                            r2_list.append(r2)\n",
    "                            rmse_list.append(rmse)\n",
    "                            \n",
    "                            # Collect residuals and fitted values for diagnostic plots\n",
    "                            group_residuals.append(residuals.values)\n",
    "                            group_fitted.append(Y_pred)\n",
    "\n",
    "                            smd, vr = calculate_smd_vr(X_train, T_train)\n",
    "                            smd_list.append(smd)\n",
    "                            vr_list.append(vr)\n",
    "                        except Exception as e:\n",
    "                            print(f\"⚠️ Error in {group}, seed {seed}, imp {imp}, rep {repeat}: {e}\")\n",
    "\n",
    "            if att_list:\n",
    "                att, se, ci_l, ci_u, p_val = rubins_pool(att_list, se_list)\n",
    "                avg_r2 = np.mean(r2_list)\n",
    "                avg_rmse = np.mean(rmse_list)\n",
    "                avg_smd = np.mean(smd_list)\n",
    "                avg_vr = np.mean(vr_list)\n",
    "\n",
    "                balance_results.append({\n",
    "                    \"group\": group, \"seed\": seed, \"smd\": avg_smd, \"vr\": avg_vr\n",
    "                })\n",
    "\n",
    "                if se < best_se:\n",
    "                    best_se = se\n",
    "                    best_result = {\n",
    "                        \"group\": group, \"seed\": seed, \"att\": att, \"se\": se,\n",
    "                        \"ci_lower\": ci_l, \"ci_upper\": ci_u, \"p_value\": p_val,\n",
    "                        \"r2\": avg_r2, \"rmse\": avg_rmse\n",
    "                    }\n",
    "\n",
    "                print(f\"✅ {group} | Seed {seed}: ATT = {att:.4f}, SE = {se:.4f}, p = {p_val}\")\n",
    "            else:\n",
    "                print(f\"⚠️ No valid results for {group} | Seed {seed}\")\n",
    "\n",
    "        # Create diagnostic plots for this group\n",
    "        if group_residuals and group_fitted:\n",
    "            create_diagnostic_plots(group_residuals, group_fitted, group)\n",
    "\n",
    "        if best_result:\n",
    "            att_results.append(best_result)\n",
    "            print(f\"🏆 Best result for {group} → Seed {best_result['seed']} | SE = {best_result['se']:.4f}\")\n",
    "\n",
    "    # Save final output\n",
    "    pd.DataFrame(att_results).to_excel(\"xgb_rubin_summary_cats_unweighted.xlsx\", index=False)\n",
    "    pd.DataFrame(balance_results).to_excel(\"smd_vr_summary_cats_unweighted.xlsx\", index=False)\n",
    "    print(\"\\n🎯 All summary files saved.\")\n",
    "\n",
    "run_xgboost_with_trimmed_data(final_covariates_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f4c13c-ac14-4dfc-aa7a-e7ec6d3a5f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import sem, ttest_ind\n",
    "\n",
    "# ----------------------------------\n",
    "# File paths\n",
    "# ----------------------------------\n",
    "output_base = \"outputs\"\n",
    "att_file = \"xgb_rubin_summary_cats.xlsx\"\n",
    "trimmed_file = \"trimmed_data_imp1.pkl\"\n",
    "auc_file = \"auc_scores.xlsx\"  # NEW\n",
    "\n",
    "# ----------------------------------\n",
    "# Load ATT Summary\n",
    "# ----------------------------------\n",
    "if os.path.exists(att_file):\n",
    "    att_df = pd.read_excel(att_file)\n",
    "else:\n",
    "    raise FileNotFoundError(\"❌ ATT summary file not found: xgb_rubin_summary_cats.xlsx\")\n",
    "\n",
    "summary_rows = []\n",
    "\n",
    "# ----------------------------------\n",
    "# Loop over medication groups\n",
    "# ----------------------------------\n",
    "groups = [g for g in medication_groups if os.path.isdir(os.path.join(output_base, g))]\n",
    "\n",
    "for med in groups:\n",
    "    try:\n",
    "        group_path = os.path.join(output_base, med)\n",
    "\n",
    "        # Load trimmed data\n",
    "        df = pd.read_pickle(os.path.join(group_path, trimmed_file))\n",
    "\n",
    "        # Detect treatment column\n",
    "        treatment_cols = [col for col in df.columns if col.upper() == med.upper()]\n",
    "        if not treatment_cols:\n",
    "            print(f\"⚠️ Treatment column {med} not found in trimmed data. Skipping.\")\n",
    "            continue\n",
    "        treatment_var = treatment_cols[0]\n",
    "\n",
    "        # Extract treatment and outcome\n",
    "        T = df[treatment_var]\n",
    "        Y = df[\"caps5_change_baseline\"]\n",
    "\n",
    "        # Treated and control stats\n",
    "        treated = Y[T == 1]\n",
    "        control = Y[T == 0]\n",
    "\n",
    "        mean_treat = treated.mean()\n",
    "        se_treat = sem(treated) if len(treated) > 1 else np.nan\n",
    "\n",
    "        mean_ctrl = control.mean()\n",
    "        se_ctrl = sem(control) if len(control) > 1 else np.nan\n",
    "\n",
    "        # Cohen's d (unadjusted)\n",
    "        pooled_sd = np.sqrt(((treated.std() ** 2) + (control.std() ** 2)) / 2)\n",
    "        cohen_d = (mean_treat - mean_ctrl) / pooled_sd if pooled_sd > 0 else np.nan\n",
    "\n",
    "        # E-value (unadjusted)\n",
    "        delta = mean_treat - mean_ctrl\n",
    "        E = delta / abs(mean_ctrl) * 100 if mean_ctrl != 0 else np.nan\n",
    "\n",
    "        # Unadjusted p-value\n",
    "        try:\n",
    "            t_stat, p_val = ttest_ind(treated, control, equal_var=False, nan_policy=\"omit\")\n",
    "            rounded_p = round(p_val, 5)\n",
    "            formatted_p = \"< 0.00001\" if rounded_p < 0.00001 else rounded_p\n",
    "        except Exception:\n",
    "            formatted_p = np.nan\n",
    "\n",
    "        # AUC from new auc_scores.xlsx file\n",
    "        auc_val = np.nan\n",
    "        auc_path = os.path.join(group_path, auc_file)\n",
    "        if os.path.exists(auc_path):\n",
    "            auc_df = pd.read_excel(auc_path)\n",
    "            if \"AUC\" in auc_df.columns:\n",
    "                auc_val = auc_df[\"AUC\"].dropna().mean()\n",
    "\n",
    "        # Adjusted stats from Rubin summary\n",
    "        att_row = att_df[att_df[\"group\"].str.strip().str.upper() == med.strip().upper()]\n",
    "        if not att_row.empty:\n",
    "            att = att_row.iloc[0][\"att\"]\n",
    "            att_se = att_row.iloc[0][\"se\"]\n",
    "            att_p_val = att_row.iloc[0][\"p_value\"]\n",
    "            r2 = att_row.iloc[0][\"r2\"]\n",
    "            rmse = att_row.iloc[0][\"rmse\"]\n",
    "\n",
    "            try:\n",
    "                rounded_att_p = round(float(att_p_val), 5)\n",
    "                formatted_att_p = \"< 0.00001\" if rounded_att_p < 0.00001 else rounded_att_p\n",
    "            except:\n",
    "                formatted_att_p = att_p_val\n",
    "        else:\n",
    "            att, att_se, formatted_att_p, r2, rmse = np.nan, np.nan, np.nan, np.nan, np.nan\n",
    "\n",
    "        # Append full row\n",
    "        summary_rows.append({\n",
    "            'Medication Group': med,\n",
    "            'Mean Treated': mean_treat,\n",
    "            'SE Treated': se_treat,\n",
    "            'Mean Control': mean_ctrl,\n",
    "            'SE Control': se_ctrl,\n",
    "            'Cohen d': cohen_d,\n",
    "            'E (Unadjusted)': E,\n",
    "            'n Treated': len(treated),\n",
    "            'n Control': len(control),\n",
    "            #'Unadjusted p-value': formatted_p,\n",
    "            'ATT Estimate': att,\n",
    "            'ATT SE (Robust)': att_se,\n",
    "            'ATT p-value': formatted_att_p,\n",
    "            'R²': r2,\n",
    "            'RMSE': rmse,\n",
    "            'AUC': auc_val\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error in {med}: {e}\")\n",
    "\n",
    "# ----------------------------------\n",
    "# Save final summary\n",
    "# ----------------------------------\n",
    "summary_df = pd.DataFrame(summary_rows)\n",
    "summary_df = summary_df.sort_values(\"Medication Group\")\n",
    "summary_df.to_excel(\"Final_ATT_Summary_Cat.xlsx\", index=False)\n",
    "print(\"✅ Final_ATT_Summary_Cat saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2add3f46-01e8-4fac-b63c-cb913efd7755",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "# ✅ Load the final summary table\n",
    "final_df = pd.read_excel(\"Final_ATT_Summary_Cat.xlsx\")\n",
    "\n",
    "# ✅ Parse DML p-values (handle \"< 0.00001\")\n",
    "def parse_pval(p):\n",
    "    try:\n",
    "        if isinstance(p, str) and \"<\" in p:\n",
    "            return 0.000001\n",
    "        return float(p)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "final_df['ATT p-value'] = final_df['ATT p-value'].apply(parse_pval)\n",
    "\n",
    "# ✅ Plot settings\n",
    "width = 0.35\n",
    "\n",
    "# ✅ Plotting function for a single medication group\n",
    "def plot_single_group(row):\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    bars1 = ax.bar(-width/2, row['Mean Control'], width, \n",
    "                   yerr=row['SE Control'], label='Control', hatch='//', color='gray', capsize=5)\n",
    "    bars2 = ax.bar(+width/2, row['Mean Treated'], width, \n",
    "                   yerr=row['SE Treated'], label='Treated', color='steelblue', capsize=5)\n",
    "\n",
    "    label = (\n",
    "        f\"ATT = {row['ATT Estimate']:.2f}\\n\"\n",
    "        f\"d = {row['Cohen d']:.2f}, p = {row['ATT p-value']:.3f}\\n\"\n",
    "        f\"nT = {row['n Treated']}, nC = {row['n Control']}\\n\"\n",
    "        f\"E = {row['E (Unadjusted)']:.1f}%\"\n",
    "    )\n",
    "    max_y = max(row['Mean Control'], row['Mean Treated']) + 1.5\n",
    "    ax.text(0, max_y, label, ha='center', va='bottom', fontsize=9, color='#FFD700')\n",
    "\n",
    "    ax.axhline(0, color='black', linewidth=0.8)\n",
    "    ax.set_xticks([-width/2, +width/2])\n",
    "    ax.set_xticklabels(['Control', 'Treated'])\n",
    "    ax.set_title(f\"Group: {row['Medication Group']}\", fontsize=12, weight='bold')\n",
    "    ax.set_ylabel(\"CAPS5 Change Score\")\n",
    "    ax.set_ylim(bottom=0, top=max_y + 2)\n",
    "    ax.legend()\n",
    "    fig.tight_layout()\n",
    "    return fig\n",
    "\n",
    "# ✅ Generate and save all plots into a multi-page PDF\n",
    "with PdfPages(\"xgb_att_barplot_cat.pdf\") as pdf:\n",
    "    for idx, row in final_df.iterrows():\n",
    "        fig = plot_single_group(row)\n",
    "        pdf.savefig(fig, dpi=300, bbox_inches='tight')\n",
    "        plt.close(fig)\n",
    "print(\"✅ xgb_att_barplot_cat saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7dd2a6-210a-47e3-adc8-15a9d93cfd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Love plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5ab6a2-6ea7-4e7c-bb94-cf40cd9c330d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "# ----------------------------------------\n",
    "# Functions to calculate balance\n",
    "# ----------------------------------------\n",
    "def calculate_smd(x1, x2, w1=None, w2=None):\n",
    "    def weighted_mean(x, w): return np.average(x, weights=w)\n",
    "    def weighted_var(x, w):\n",
    "        m = np.average(x, weights=w)\n",
    "        return np.average((x - m) ** 2, weights=w)\n",
    "    \n",
    "    m1 = weighted_mean(x1, w1) if w1 is not None else np.mean(x1)\n",
    "    m2 = weighted_mean(x2, w2) if w2 is not None else np.mean(x2)\n",
    "    v1 = weighted_var(x1, w1) if w1 is not None else np.var(x1, ddof=1)\n",
    "    v2 = weighted_var(x2, w2) if w2 is not None else np.var(x2, ddof=1)\n",
    "    \n",
    "    pooled_sd = np.sqrt((v1 + v2) / 2)\n",
    "    return np.abs(m1 - m2) / pooled_sd if pooled_sd > 0 else 0\n",
    "\n",
    "def variance_ratio(x1, x2, w1=None, w2=None):\n",
    "    def weighted_var(x, w):\n",
    "        m = np.average(x, weights=w)\n",
    "        return np.average((x - m) ** 2, weights=w)\n",
    "    \n",
    "    v1 = weighted_var(x1, w1) if w1 is not None else np.var(x1, ddof=1)\n",
    "    v2 = weighted_var(x2, w2) if w2 is not None else np.var(x2, ddof=1)\n",
    "    \n",
    "    return max(v1 / v2, v2 / v1) if v1 > 0 and v2 > 0 else 1\n",
    "\n",
    "# ----------------------------------------\n",
    "# Setup\n",
    "# ----------------------------------------\n",
    "output_base = \"outputs\"\n",
    "groups = [g for g in os.listdir(output_base) if os.path.isdir(os.path.join(output_base, g))]\n",
    "\n",
    "# Create a case-insensitive mapping\n",
    "final_covariates_map_lower = {k.lower(): v for k, v in final_covariates_map.items()}\n",
    "\n",
    "# ----------------------------------------\n",
    "# Main Loop\n",
    "# ----------------------------------------\n",
    "for group in groups:\n",
    "    if group.lower() not in final_covariates_map_lower:\n",
    "        continue\n",
    "\n",
    "    print(f\"\\n🔍 Processing {group}...\")\n",
    "\n",
    "    try:\n",
    "        group_path = os.path.join(output_base, group)\n",
    "        covariates = final_covariates_map_lower[group.lower()]\n",
    "        \n",
    "        column_name = None\n",
    "        for col in pd.read_pickle(os.path.join(group_path, \"trimmed_data_imp1.pkl\")).columns:\n",
    "            if col.lower() == group.lower():\n",
    "                column_name = col\n",
    "                break\n",
    "        if column_name is None:\n",
    "            print(f\"⚠️ Column not found for {group}, skipping.\")\n",
    "            continue\n",
    "\n",
    "        smd_unw_all, smd_w_all = [], []\n",
    "        vr_unw_all, vr_w_all = [], []\n",
    "\n",
    "        for i in range(1, 6):\n",
    "            df_path = os.path.join(group_path, f\"trimmed_data_imp{i}.pkl\")\n",
    "            iptw_path = os.path.join(group_path, \"iptw_weights.xlsx\")\n",
    "\n",
    "            if not os.path.exists(df_path) or not os.path.exists(iptw_path):\n",
    "                print(f\"⚠️ Missing data for {group} imp{i}, skipping.\")\n",
    "                continue\n",
    "\n",
    "            df = pd.read_pickle(df_path)\n",
    "            iptw_df = pd.read_excel(iptw_path, index_col=0)\n",
    "            T = df[column_name]\n",
    "            W = iptw_df.loc[df.index, \"iptw_mean\"]\n",
    "\n",
    "            smd_unw_i, smd_w_i, vr_unw_i, vr_w_i = [], [], [], []\n",
    "\n",
    "            for cov in covariates:\n",
    "                x1, x0 = df.loc[T == 1, cov], df.loc[T == 0, cov]\n",
    "                w1, w0 = W[T == 1], W[T == 0]\n",
    "\n",
    "                su = calculate_smd(x1, x0)\n",
    "                sw = calculate_smd(x1, x0, w1, w0)\n",
    "\n",
    "                vu = variance_ratio(x1, x0) if cov in ['treatmentdurationdays', 'CAPS5score_baseline', 'age'] else np.nan\n",
    "                vw = variance_ratio(x1, x0, w1, w0) if cov in ['treatmentdurationdays', 'CAPS5score_baseline', 'age'] else np.nan\n",
    "\n",
    "                smd_unw_i.append(su)\n",
    "                smd_w_i.append(sw)\n",
    "                vr_unw_i.append(vu)\n",
    "                vr_w_i.append(vw)\n",
    "\n",
    "            smd_unw_all.append(smd_unw_i)\n",
    "            smd_w_all.append(smd_w_i)\n",
    "            vr_unw_all.append(vr_unw_i)\n",
    "            vr_w_all.append(vr_w_i)\n",
    "\n",
    "        smd_unw = np.mean(smd_unw_all, axis=0)\n",
    "        smd_w = np.mean(smd_w_all, axis=0)\n",
    "        vr_unw = np.nanmean(vr_unw_all, axis=0)\n",
    "        vr_w = np.nanmean(vr_w_all, axis=0)\n",
    "\n",
    "        severity = []\n",
    "        for sw in smd_w:\n",
    "            if sw <= 0.1:\n",
    "                severity.append(\"Good\")\n",
    "            elif sw <= 0.2:\n",
    "                severity.append(\"Moderate\")\n",
    "            else:\n",
    "                severity.append(\"Poor\")\n",
    "\n",
    "        covariate_names = covariates\n",
    "        numeric_df = pd.DataFrame({\n",
    "            \"Covariate\": covariate_names,\n",
    "            \"SMD_Unweighted\": smd_unw,\n",
    "            \"SMD_Weighted\": smd_w,\n",
    "            \"Imbalance_Severity\": severity,\n",
    "            \"VR_Unweighted\": vr_unw,\n",
    "            \"VR_Weighted\": vr_w\n",
    "        })\n",
    "\n",
    "        numeric_path = os.path.join(group_path, f\"covariate_balance_table_{group}.xlsx\")\n",
    "        numeric_df.to_excel(numeric_path, index=False)\n",
    "        print(f\"📊 Exported numeric summary to: {numeric_path}\")\n",
    "\n",
    "        # -------------------------\n",
    "        # Plot\n",
    "        # -------------------------\n",
    "        labels = covariates\n",
    "        y_pos = np.arange(len(labels))\n",
    "\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(18, len(labels) * 0.45))\n",
    "\n",
    "        axes[0].scatter(smd_unw, y_pos, color='red', label=\"Unweighted\")\n",
    "        axes[0].scatter(smd_w, y_pos, color='blue', label=\"Weighted\")\n",
    "        axes[0].axvline(0.1, color='gray', linestyle='--', label=\"Threshold 0.1\")\n",
    "        axes[0].axvline(0.2, color='black', linestyle='--', label=\"Threshold 0.2\")\n",
    "        axes[0].set_xlim(0, max(max(smd_unw), max(smd_w), 0.25) + 0.05)\n",
    "        axes[0].set_yticks(y_pos)\n",
    "        axes[0].set_yticklabels(labels)\n",
    "        axes[0].invert_yaxis()\n",
    "        axes[0].set_title(\"Standardized Mean Differences (SMD)\")\n",
    "        axes[0].legend(loc=\"upper right\")\n",
    "        axes[0].grid(True)\n",
    "\n",
    "        vr_mask = [cov in ['treatmentdurationdays', 'CAPS5score_baseline', 'age'] for cov in covariates]\n",
    "        filtered_y = [i for i, b in enumerate(vr_mask) if b]\n",
    "        filtered_labels = [labels[i] for i in filtered_y]\n",
    "        filtered_vr_unw = [vr_unw[i] for i in filtered_y]\n",
    "        filtered_vr_w = [vr_w[i] for i in filtered_y]\n",
    "\n",
    "        axes[1].scatter(filtered_vr_unw, filtered_y, color='blue', marker='o', label=\"Unweighted\")\n",
    "        axes[1].scatter(filtered_vr_w, filtered_y, color='red', marker='x', label=\"Weighted\")\n",
    "        axes[1].axvline(2, color='gray', linestyle='--')\n",
    "        axes[1].axvline(0.5, color='gray', linestyle='--')\n",
    "        axes[1].set_xlim(0, max(filtered_vr_unw + filtered_vr_w + [2.5]) + 0.5)\n",
    "        axes[1].set_yticks(filtered_y)\n",
    "        axes[1].set_yticklabels(filtered_labels)\n",
    "        axes[1].invert_yaxis()\n",
    "        axes[1].set_title(\"Variance Ratio (VR)\")\n",
    "        axes[1].legend()\n",
    "        axes[1].grid(True)\n",
    "\n",
    "        fig.suptitle(f\"Covariate Balance for {group.replace('CAT_', '')}\", fontsize=14, weight='bold')\n",
    "        fig.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "        plot_path = os.path.join(group_path, f\"love_plot_{group}.pdf\")\n",
    "        fig.savefig(plot_path, dpi=300)\n",
    "        plt.close()\n",
    "        print(f\"✅ Saved love plot: {plot_path}\")\n",
    "        print(f\"📏 Max weighted SMD for {group}: {np.max(smd_w):.3f}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error in {group}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cedeed7-7c10-422b-b58e-81e03e632ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99056fbb-3bd6-44b0-beed-753f79181a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "#-----------------------------\n",
    "# Generate heatmaps\n",
    "# -------------------------------\n",
    "for treatment_var in medication_groups:\n",
    "    print(f\"\\n========== Creating Heatmap for {treatment_var} ==========\")\n",
    "\n",
    "    try:\n",
    "        output_folder = os.path.join('outputs', treatment_var)\n",
    "        balance_path = os.path.join(output_folder, f'covariate_balance_table_{treatment_var}.xlsx')\n",
    "\n",
    "        if not os.path.exists(balance_path):\n",
    "            print(f\"❌ Balance file not found: {balance_path}\")\n",
    "            continue\n",
    "\n",
    "        balance_df = pd.read_excel(balance_path)\n",
    "\n",
    "        # ✅ Use finalized covariates + 'Propensity Score'\n",
    "        covariates = final_covariates_map[treatment_var] + ['Propensity Score']\n",
    "        balance_df = balance_df[balance_df['Covariate'].isin(covariates)]\n",
    "\n",
    "        # ✅ Check for CAPS5score_baseline\n",
    "        highlight_caps = 'CAPS5score_baseline' in balance_df['Covariate'].values\n",
    "\n",
    "        # ✅ Format for heatmap\n",
    "        heatmap_df = balance_df[['Covariate', 'SMD_Unweighted', 'SMD_Weighted']].copy()\n",
    "        heatmap_df.columns = ['Covariate', 'Unweighted', 'Weighted']\n",
    "        heatmap_df = heatmap_df.set_index('Covariate')\n",
    "        heatmap_df = heatmap_df.sort_values(by='Unweighted', ascending=False)\n",
    "\n",
    "        # ✅ Plot\n",
    "        plt.figure(figsize=(12, max(10, len(heatmap_df) * 0.35)))\n",
    "        ax = sns.heatmap(\n",
    "            heatmap_df,\n",
    "            cmap=\"coolwarm\",\n",
    "            annot=True,\n",
    "            fmt=\".2f\",\n",
    "            linewidths=0.6,\n",
    "            linecolor='gray',\n",
    "            cbar_kws={\"label\": \"Standardized Mean Difference\"}\n",
    "        )\n",
    "\n",
    "        plt.title(f\"Covariate Balance Heatmap (Rubin IPTW)\\n{treatment_var}\", fontsize=15, weight='bold')\n",
    "        plt.xlabel(\"Condition\")\n",
    "        plt.ylabel(\"Covariate\")\n",
    "\n",
    "        # ✅ Bold CAPS5score_baseline if present\n",
    "        if highlight_caps:\n",
    "            ylabels = [label.get_text() for label in ax.get_yticklabels()]\n",
    "            ax.set_yticklabels([\n",
    "                f\"{label} ←\" if label == 'CAPS5score_baseline' else label for label in ylabels\n",
    "            ])\n",
    "\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # ✅ Save image\n",
    "        save_path = os.path.join(output_folder, f'heatmap_smd_{treatment_var}.png')\n",
    "        plt.savefig(save_path, dpi=300)\n",
    "        plt.close()\n",
    "\n",
    "        print(f\"✅ Heatmap saved: {save_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error processing {treatment_var}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ecb610-b6bb-4639-9b1b-ff658d37153c",
   "metadata": {},
   "source": [
    "## Subcat analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e55b3a-f46c-4df7-bc09-7d346c53e2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "covariates_SUBCAT_Antipsychotica_atypisch = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline',\n",
    "    'CAT_Antidepressiva', 'CAT_Antipsychotica',\n",
    "    'DIAGNOSIS_CHILDHOOD_TRAUMA', 'DIAGNOSIS_CPTSD', 'DIAGNOSIS_SEXUAL_TRAUMA',\n",
    "    'DIAGNOSIS_SUICIDALITY', 'age',\n",
    "    'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"Bipolar_and_Mood_disorder\", \"ethnicity_Dutch\", \"ethnicity_other\"\n",
    "]\n",
    "\n",
    "covariates_SUBCAT_TCA = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline',\n",
    "    'CAT_Antipsychotica',\n",
    "    'DIAGNOSIS_CHILDHOOD_TRAUMA', 'DIAGNOSIS_CPTSD', 'DIAGNOSIS_SEXUAL_TRAUMA',\n",
    "    'DIAGNOSIS_SUICIDALITY', 'age',\n",
    "    'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"Bipolar_and_Mood_disorder\", \"ethnicity_Dutch\", \"ethnicity_other\"\n",
    "]\n",
    "\n",
    "covariates_SUBCAT_SSRI = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline',\n",
    "    'CAT_Antipsychotica',\n",
    "    'DIAGNOSIS_CHILDHOOD_TRAUMA', 'DIAGNOSIS_CPTSD', 'DIAGNOSIS_SEXUAL_TRAUMA',\n",
    "    'DIAGNOSIS_SUICIDALITY', 'age',\n",
    "    'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"Bipolar_and_Mood_disorder\", \"ethnicity_Dutch\", \"ethnicity_other\"\n",
    "]\n",
    "\n",
    "covariates_SUBCAT_SNRI = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline',\n",
    "    'CAT_Antipsychotica',\n",
    "    'DIAGNOSIS_CHILDHOOD_TRAUMA', 'DIAGNOSIS_CPTSD', 'DIAGNOSIS_SEXUAL_TRAUMA',\n",
    "    'DIAGNOSIS_SUICIDALITY', 'age',\n",
    "    'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"Bipolar_and_Mood_disorder\", \"ethnicity_Dutch\", \"ethnicity_other\"\n",
    "]\n",
    "\n",
    "\n",
    "covariates_SUBCAT_Tetracyclische_antidepressiva = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline',\n",
    "    'CAT_Antipsychotica',\n",
    "    'DIAGNOSIS_CHILDHOOD_TRAUMA', 'DIAGNOSIS_CPTSD', 'DIAGNOSIS_SEXUAL_TRAUMA',\n",
    "    'DIAGNOSIS_SUICIDALITY', 'age',\n",
    "    'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"Bipolar_and_Mood_disorder\", \"ethnicity_Dutch\", \"ethnicity_other\"\n",
    "]\n",
    "\n",
    "covariates_SUBCAT_Antidepressiva_overige = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline',\n",
    "    'CAT_Antipsychotica',\n",
    "    'DIAGNOSIS_CHILDHOOD_TRAUMA', 'DIAGNOSIS_CPTSD', 'DIAGNOSIS_SEXUAL_TRAUMA',\n",
    "    'DIAGNOSIS_SUICIDALITY', 'age',\n",
    "    'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"Bipolar_and_Mood_disorder\", \"ethnicity_Dutch\", \"ethnicity_other\"\n",
    "]\n",
    "\n",
    "covariates_SUBCAT_Systemische_antihistaminica = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline',\n",
    "    'CAT_Antidepressiva', 'CAT_Antipsychotica',\n",
    "    'DIAGNOSIS_CHILDHOOD_TRAUMA', 'DIAGNOSIS_CPTSD', 'DIAGNOSIS_SEXUAL_TRAUMA',\n",
    "    'DIAGNOSIS_SUICIDALITY', 'age',\n",
    "    'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"Bipolar_and_Mood_disorder\", \"ethnicity_Dutch\", \"ethnicity_other\"\n",
    "]\n",
    "\n",
    "\n",
    "covariates_SUBCAT_anxiolytica_Benzodiazepine = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline',\n",
    "    'CAT_Antidepressiva',\n",
    "    'CAT_Antipsychotica',\n",
    "    'DIAGNOSIS_CHILDHOOD_TRAUMA', 'DIAGNOSIS_CPTSD', 'DIAGNOSIS_SEXUAL_TRAUMA',\n",
    "    'DIAGNOSIS_SUICIDALITY', 'age',\n",
    "    'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"Bipolar_and_Mood_disorder\", \"ethnicity_Dutch\", \"ethnicity_other\"\n",
    "]\n",
    "\n",
    "covariates_SUBCAT_hypnotica_Benzodiazepine = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline',\n",
    "    'CAT_Antidepressiva',\n",
    "    'CAT_Antipsychotica',\n",
    "    'DIAGNOSIS_CHILDHOOD_TRAUMA', 'DIAGNOSIS_CPTSD', 'DIAGNOSIS_SEXUAL_TRAUMA',\n",
    "    'DIAGNOSIS_SUICIDALITY', 'age',\n",
    "    'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"Bipolar_and_Mood_disorder\", \"ethnicity_Dutch\", \"ethnicity_other\"\n",
    "]\n",
    "\n",
    "\n",
    "covariates_SUBCAT_Amfetaminen = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline', 'CAT_Antidepressiva', 'CAT_Antipsychotica',\n",
    "    'DIAGNOSIS_CHILDHOOD_TRAUMA', 'DIAGNOSIS_CPTSD',\n",
    "    'DIAGNOSIS_SEXUAL_TRAUMA', 'DIAGNOSIS_SUICIDALITY',\n",
    "    'age',\n",
    "    'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"Bipolar_and_Mood_disorder\", \"ethnicity_Dutch\", \"ethnicity_other\"\n",
    "]\n",
    "\n",
    "\n",
    "covariates_SUBCAT_Systemische_betablokkers = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline',\n",
    "    'CAT_Antidepressiva', 'CAT_Antipsychotica',\n",
    "    'DIAGNOSIS_CHILDHOOD_TRAUMA', 'DIAGNOSIS_CPTSD', 'DIAGNOSIS_SEXUAL_TRAUMA',\n",
    "    'DIAGNOSIS_SUICIDALITY', 'age',\n",
    "    'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"Bipolar_and_Mood_disorder\", \"ethnicity_Dutch\", \"ethnicity_other\"\n",
    "]\n",
    "\n",
    "\n",
    "covariates_SUBCAT_Paracetamol_mono = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline',\n",
    "    'CAT_Antidepressiva', 'CAT_Antipsychotica',\n",
    "    'DIAGNOSIS_CHILDHOOD_TRAUMA', 'DIAGNOSIS_CPTSD', 'DIAGNOSIS_SEXUAL_TRAUMA',\n",
    "    'DIAGNOSIS_SUICIDALITY', 'age',\n",
    "    'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"Bipolar_and_Mood_disorder\", \"ethnicity_Dutch\", \"ethnicity_other\"\n",
    "]\n",
    "\n",
    "\n",
    "covariates_SUBCAT_Anti_epileptica_stemmingsstabilisatoren = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline',\n",
    "    'CAT_Antidepressiva',\n",
    "    'CAT_Antipsychotica',\n",
    "    'DIAGNOSIS_CHILDHOOD_TRAUMA', 'DIAGNOSIS_CPTSD', 'DIAGNOSIS_SEXUAL_TRAUMA',\n",
    "    'DIAGNOSIS_SUICIDALITY', 'age', \n",
    "    'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"Bipolar_and_Mood_disorder\", \"ethnicity_Dutch\", \"ethnicity_other\"\n",
    "]\n",
    "\n",
    "covariates_SUBCAT_Opioden = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline',\n",
    "    'CAT_Antidepressiva',\n",
    "    'CAT_Antipsychotica',\n",
    "    'DIAGNOSIS_CHILDHOOD_TRAUMA', 'DIAGNOSIS_CPTSD', 'DIAGNOSIS_SEXUAL_TRAUMA',\n",
    "    'DIAGNOSIS_SUICIDALITY', 'age',\n",
    "    'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"Bipolar_and_Mood_disorder\", \"ethnicity_Dutch\", \"ethnicity_other\"\n",
    "]\n",
    "\n",
    "covariates_SUBCAT_Z_drugs = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline',\n",
    "    'CAT_Antidepressiva', 'CAT_Antipsychotica',\n",
    "    'DIAGNOSIS_CHILDHOOD_TRAUMA', 'DIAGNOSIS_CPTSD', 'DIAGNOSIS_SEXUAL_TRAUMA',\n",
    "    'DIAGNOSIS_SUICIDALITY', 'age',\n",
    "    'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"Bipolar_and_Mood_disorder\", \"ethnicity_Dutch\", \"ethnicity_other\"\n",
    "]\n",
    "\n",
    "covariates_SUBCAT_NSAIDs = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline',\n",
    "    'CAT_Antidepressiva',\n",
    "    'CAT_Antipsychotica',\n",
    "    'DIAGNOSIS_CHILDHOOD_TRAUMA', 'DIAGNOSIS_CPTSD', 'DIAGNOSIS_SEXUAL_TRAUMA',\n",
    "    'DIAGNOSIS_SUICIDALITY', 'age',\n",
    "    'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"Bipolar_and_Mood_disorder\", \"ethnicity_Dutch\", \"ethnicity_other\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3ef609-2a60-4e10-8983-dadb6790f337",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# This finds all variables that start with covariates_SUBCAT_\n",
    "final_covariates_map = defaultdict(list)\n",
    "final_covariates_map.update({\n",
    "    var.replace(\"covariates_\", \"\"): val\n",
    "    for var, val in globals().items()\n",
    "    if var.lower().startswith(\"covariates_subcat_\") and isinstance(val, list)\n",
    "})\n",
    "\n",
    "# Show detected group names\n",
    "print(\"Groups found:\", list(final_covariates_map.keys()))\n",
    "medication_groups = list(final_covariates_map.keys())\n",
    "print(medication_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8784a01-10be-41b1-aa30-a516788adbaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "def run_all_SUBCAT_group_models(imputed_dfs):\n",
    "    \"\"\"\n",
    "    Runs downstream analysis for each SUBCAT medisubcation group using imputed datasets.\n",
    "\n",
    "    Parameters:\n",
    "    - imputed_dfs: list of 5 imputed DataFrames (from df_imputed_final_imp1.pkl ... imp5.pkl)\n",
    "    \n",
    "    Notes:\n",
    "    - Covariate lists must be defined as global variables: covariates_subcat_<group>\n",
    "    - Outputs are saved in: outputs/SUBCAT_<GROUP>/\n",
    "    \"\"\"\n",
    "\n",
    "    print(\" Starting analysis for all SUBCAT groups\")\n",
    "\n",
    "    for var_name in globals():\n",
    "        if var_name.lower().startswith(\"covariates_subcat_\") and isinstance(globals()[var_name], list):\n",
    "            group_name = var_name.replace(\"covariates_\", \"\")\n",
    "            group_name = group_name.replace(\"_\", \" \").title().replace(\" \", \"_\")  # e.g., subcat_z_drugs → Subcat_Z_Drugs\n",
    "            group_name = group_name.replace(\"Subcat_\", \"SUBCAT_\")  # force prefix to uppercase\n",
    "\n",
    "            covariates = globals()[var_name]\n",
    "            output_dir = f\"outputs/{group_name}\"\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "            print(f\"\\n Processing {group_name}...\")\n",
    "\n",
    "            for k, df_imp in enumerate(imputed_dfs):\n",
    "                print(f\"  → Using imputation {k+1}\")\n",
    "\n",
    "                # Define X and Y\n",
    "                X = df_imp[covariates]\n",
    "                Y = df_imp[\"caps5_change_baseline\"]\n",
    "\n",
    "                # === Save X and Y as placeholder (replace with modeling later)\n",
    "                X.to_csv(f\"{output_dir}/X_imp{k+1}.csv\", index=False)\n",
    "                Y.to_frame(name=\"Y\").to_csv(f\"{output_dir}/Y_imp{k+1}.csv\", index=False)\n",
    "\n",
    "            print(f\" Done: {group_name}\")\n",
    "\n",
    "    print(\"\\n All SUBCAT group analyses complete.\")\n",
    "\n",
    "# ========= STEP 4: Execute ========= #\n",
    "run_all_SUBCAT_group_models(imputed_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6669c45-1195-4268-9001-4ddc425cc580",
   "metadata": {},
   "outputs": [],
   "source": [
    "for treatment_var in medication_groups:\n",
    "    print(f\"\\n {treatment_var}\")\n",
    "    \n",
    "    for i, df in enumerate(imputed_dfs):\n",
    "        if treatment_var not in df.columns:\n",
    "            print(f\"  Imp {i+1}:  Not found in columns.\")\n",
    "            continue\n",
    "\n",
    "        treated = (df[treatment_var] == 1).sum()\n",
    "        control = (df[treatment_var] == 0).sum()\n",
    "        missing = df[treatment_var].isna().sum()\n",
    "\n",
    "        print(f\"  Imp {i+1}: Treated = {treated}, Control = {control}, Missing = {missing}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62728b25-f7b4-4303-b783-3813f0bc25f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from collections import defaultdict\n",
    "\n",
    "# ✅ VIF computation function\n",
    "def compute_vif(X):\n",
    "    X = sm.add_constant(X, has_constant='add')\n",
    "    vif_df = pd.DataFrame()\n",
    "    vif_df[\"variable\"] = X.columns\n",
    "    vif_df[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "    return vif_df\n",
    "\n",
    "# ✅ Process each group\n",
    "for group in medication_groups:\n",
    "    print(f\"\\n🔍 Processing VIF for {group}\")\n",
    "\n",
    "    if group not in final_covariates_map:\n",
    "        print(f\" ⚠️ No covariates found for {group}. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    covariates = final_covariates_map[group]\n",
    "    vif_list = []\n",
    "\n",
    "    for i, df_imp in enumerate(imputed_dfs):\n",
    "        try:\n",
    "            X = df_imp[covariates].copy()\n",
    "            vif_df = compute_vif(X)\n",
    "            vif_df[\"imputation\"] = i + 1\n",
    "            vif_list.append(vif_df)\n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ Failed on imputation {i+1} for {group}: {e}\")\n",
    "\n",
    "    if vif_list:\n",
    "        all_vif = pd.concat(vif_list)\n",
    "        pooled_vif = all_vif.groupby(\"variable\")[\"VIF\"].mean().reset_index()\n",
    "        pooled_vif = pooled_vif.sort_values(by=\"VIF\", ascending=False)\n",
    "\n",
    "        output_folder = os.path.join(\"outputs\", group)\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "        pooled_vif.to_csv(os.path.join(output_folder, \"pooled_vif.csv\"), index=False)\n",
    "\n",
    "        print(f\" ✅ Saved: {output_folder}/pooled_vif.csv\")\n",
    "    else:\n",
    "        print(f\" ⚠️ Skipped {group}: No valid imputations.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a3aec7-90b0-4160-b2eb-4e97e633bfb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ---------- PS Estimation Function ----------\n",
    "def run_xgboost_ps_modeling(imputed_dfs, medication_groups, final_covariates_map):\n",
    "    for group in medication_groups:\n",
    "        print(f\" Running PS estimation for {group}\")\n",
    "\n",
    "        if group not in final_covariates_map:\n",
    "            print(f\" No covariates found for {group}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        output_folder = os.path.join(\"outputs\", group)\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "        covariates = final_covariates_map[group]\n",
    "        ps_matrix = pd.DataFrame()\n",
    "        auc_list = []\n",
    "\n",
    "        for i, df_imp in enumerate(imputed_dfs):\n",
    "            if group not in df_imp.columns:\n",
    "                print(f\" {group} not found in imputed dataset {i+1}. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            X = df_imp[covariates].copy()\n",
    "            T = df_imp[group]\n",
    "\n",
    "            # Drop missing treatment rows\n",
    "            valid_idx = T.dropna().index\n",
    "            X = X.loc[valid_idx]\n",
    "            T = T.loc[valid_idx]\n",
    "\n",
    "            # Train-test split for ROC\n",
    "            X_train, X_test, T_train, T_test = train_test_split(\n",
    "                X, T, stratify=T, test_size=0.3, random_state=42\n",
    "            )\n",
    "\n",
    "            try:\n",
    "                model = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "                model.fit(X_train, T_train)\n",
    "\n",
    "                ps_scores = model.predict_proba(X)[:, 1]\n",
    "                ps_matrix[f\"ps_imp{i+1}\"] = pd.Series(ps_scores, index=valid_idx)\n",
    "\n",
    "                # ROC & AUC\n",
    "                auc = roc_auc_score(T_test, model.predict_proba(X_test)[:, 1])\n",
    "                auc_list.append(auc)\n",
    "\n",
    "                fpr, tpr, _ = roc_curve(T_test, model.predict_proba(X_test)[:, 1])\n",
    "                plt.figure()\n",
    "                plt.plot(fpr, tpr, label=f\"AUC = {auc:.3f}\")\n",
    "                plt.plot([0, 1], [0, 1], 'k--')\n",
    "                plt.xlabel(\"False Positive Rate\")\n",
    "                plt.ylabel(\"True Positive Rate\")\n",
    "                plt.title(f\"ROC Curve - {group} (Imp {i+1})\")\n",
    "                plt.legend()\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(os.path.join(output_folder, f\"roc_curve_imp{i+1}.png\"))\n",
    "                plt.close()\n",
    "                print(f\"   Imp {i+1}: AUC = {auc:.3f}, ROC saved.\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"   Error in {group} (imp {i+1}): {e}\")\n",
    "\n",
    "        # Save AUCs and Composite PS\n",
    "        if not ps_matrix.empty:\n",
    "            # Fill NaN rows (from dropped subjects in some imputations) with mean\n",
    "            ps_matrix[\"composite_ps\"] = ps_matrix.mean(axis=1)\n",
    "            ps_matrix.to_excel(os.path.join(output_folder, \"propensity_scores.xlsx\"))\n",
    "\n",
    "            auc_df = pd.DataFrame({\n",
    "                \"imputation\": [f\"imp{i+1}\" for i in range(len(auc_list))],\n",
    "                \"AUC\": auc_list\n",
    "            })\n",
    "            auc_df.loc[len(auc_df.index)] = [\"mean\", np.mean(auc_list) if auc_list else np.nan]\n",
    "            auc_df.to_excel(os.path.join(output_folder, \"auc_scores.xlsx\"), index=False)\n",
    "\n",
    "            print(f\" Composite PS + AUC saved for {group}\")\n",
    "        else:\n",
    "            print(f\" No valid PS scores generated for {group}\")\n",
    "\n",
    "# ---------- Run ----------\n",
    "run_xgboost_ps_modeling(imputed_dfs, medication_groups, final_covariates_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c350b8e-70f1-4c4d-bfa4-8963d6f2e7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "def compute_feature_importance(imputed_dfs, medication_groups, final_covariates_map):\n",
    "    for group in medication_groups:\n",
    "        print(f\"\\n Computing feature importance for {group}\")\n",
    "\n",
    "        if group not in final_covariates_map:\n",
    "            print(f\" No covariates found for {group}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        covariates = final_covariates_map[group]\n",
    "        output_folder = os.path.join(\"outputs\", group)\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "        importance_df_list = []\n",
    "\n",
    "        for i, df_imp in enumerate(imputed_dfs):\n",
    "            if group not in df_imp.columns:\n",
    "                print(f\" {group} not in imputed dataset {i+1}. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            X = df_imp[covariates].copy()\n",
    "            T = df_imp[group]\n",
    "\n",
    "            # Drop NaNs in treatment\n",
    "            valid_idx = T.dropna().index\n",
    "            X = X.loc[valid_idx]\n",
    "            T = T.loc[valid_idx]\n",
    "\n",
    "            try:\n",
    "                model = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "                model.fit(X, T)\n",
    "\n",
    "                # Get feature importance\n",
    "                importances = model.get_booster().get_score(importance_type='gain')\n",
    "                df_feat = pd.DataFrame.from_dict(importances, orient='index', columns=[f\"imp{i+1}\"])\n",
    "                df_feat.index.name = 'feature'\n",
    "                importance_df_list.append(df_feat)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"   Error during modeling: {e}\")\n",
    "\n",
    "        if importance_df_list:\n",
    "            # Combine and average\n",
    "            all_feat = pd.concat(importance_df_list, axis=1).fillna(0)\n",
    "            all_feat[\"mean_importance\"] = all_feat.mean(axis=1)\n",
    "\n",
    "            # Filter top 30 non-zero\n",
    "            non_zero = all_feat[all_feat[\"mean_importance\"] > 0]\n",
    "            top30 = non_zero.sort_values(by=\"mean_importance\", ascending=False).head(30)\n",
    "\n",
    "            # Save to CSV\n",
    "            top30.to_csv(os.path.join(output_folder, \"feature_importance.csv\"))\n",
    "\n",
    "            # Plot\n",
    "            plt.figure(figsize=(10, 8))\n",
    "            plt.barh(top30.index[::-1], top30[\"mean_importance\"][::-1])  # plot top → bottom\n",
    "            plt.xlabel(\"Mean Gain Importance\")\n",
    "            plt.title(f\"Top 30 Feature Importance - {group}\")\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(output_folder, \"feature_importance_top30.png\"))\n",
    "            plt.close()\n",
    "\n",
    "            print(f\" Saved feature importance plot and CSV for {group}\")\n",
    "        else:\n",
    "            print(f\" No valid models for {group}\")\n",
    "\n",
    "#  Run\n",
    "compute_feature_importance(imputed_dfs, medication_groups, final_covariates_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b8ab3d-0e32-41f1-8ac1-e8c308947d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def compute_trimmed_clipped_iptw(ps_df, treatment, lower=0.05, upper=0.95, clip_max=10):\n",
    "    weights = []\n",
    "    keep_mask = (ps_df > lower) & (ps_df < upper)\n",
    "\n",
    "    for i in range(ps_df.shape[1]):\n",
    "        ps = ps_df.iloc[:, i].clip(lower=1e-6, upper=1 - 1e-6)  # avoid div by zero\n",
    "        mask = keep_mask.iloc[:, i]\n",
    "        w = pd.Series(np.nan, index=ps.index)\n",
    "\n",
    "        w[mask & (treatment == 1)] = 1 / ps[mask & (treatment == 1)]\n",
    "        w[mask & (treatment == 0)] = 1 / (1 - ps[mask & (treatment == 0)])\n",
    "        w = w.clip(upper=clip_max)\n",
    "        weights.append(w)\n",
    "\n",
    "    return pd.concat(weights, axis=1)\n",
    "\n",
    "\n",
    "def apply_rubins_rule_to_iptw(iptw_matrix):\n",
    "    \"\"\"\n",
    "    Given an IPTW matrix (n rows × M imputations), return Rubin’s rule pooled mean, SD, SE.\n",
    "    \"\"\"\n",
    "    M = iptw_matrix.shape[1]\n",
    "    q_bar = iptw_matrix.mean(axis=1)\n",
    "    u_bar = iptw_matrix.var(axis=1, ddof=1)\n",
    "    B = iptw_matrix.apply(lambda x: x.mean(), axis=1).var(ddof=1)\n",
    "    total_var = u_bar + (1 + 1/M) * B\n",
    "    total_se = np.sqrt(total_var)\n",
    "    return q_bar, u_bar.pow(0.5), total_se\n",
    "\n",
    "\n",
    "def run_trim_clip_save_all(imputed_dfs, medication_groups, final_covariates_map):\n",
    "    for group in medication_groups:\n",
    "        print(f\"\\n🔍 Processing IPTW + trimming + clipping for {group}\")\n",
    "        output_folder = os.path.join(\"outputs\", group)\n",
    "        ps_path = os.path.join(output_folder, \"propensity_scores.xlsx\")\n",
    "\n",
    "        if not os.path.exists(ps_path):\n",
    "            print(f\"⚠️ Missing PS file: {ps_path}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            ps_all = pd.read_excel(ps_path, index_col=0)\n",
    "            ps_cols = [col for col in ps_all.columns if col.startswith(\"ps_imp\")]\n",
    "            composite_index = ps_all.index\n",
    "\n",
    "            # Get treatment from one imputed dataset\n",
    "            T_full = None\n",
    "            for df in imputed_dfs:\n",
    "                if group in df.columns:\n",
    "                    T_full = df.loc[composite_index, group]\n",
    "                    break\n",
    "\n",
    "            if T_full is None:\n",
    "                print(f\"❌ Treatment column {group} not found in any imputed dataset.\")\n",
    "                continue\n",
    "\n",
    "            # Compute IPTW matrix (shape: n × M)\n",
    "            iptw_matrix = compute_trimmed_clipped_iptw(ps_all[ps_cols], T_full)\n",
    "            iptw_matrix.columns = [f\"iptw_imp{i+1}\" for i in range(iptw_matrix.shape[1])]\n",
    "\n",
    "            # Apply Rubin’s Rule for mean, SD, SE\n",
    "            iptw_matrix[\"iptw_mean\"], iptw_matrix[\"iptw_sd\"], iptw_matrix[\"iptw_se\"] = apply_rubins_rule_to_iptw(\n",
    "                iptw_matrix[[f\"iptw_imp{i+1}\" for i in range(iptw_matrix.shape[1])]]\n",
    "            )\n",
    "\n",
    "            # Save IPTW matrix separately\n",
    "            iptw_matrix.to_excel(os.path.join(output_folder, \"iptw_weights.xlsx\"))\n",
    "            print(f\"✅ Saved IPTW weights for {group}\")\n",
    "\n",
    "            # Save trimmed & clipped imputed datasets with IPTW\n",
    "            for i in range(5):\n",
    "                df = imputed_dfs[i].copy()\n",
    "                if group not in df.columns:\n",
    "                    continue\n",
    "\n",
    "                trimmed_idx = iptw_matrix.index.intersection(df.index)\n",
    "                needed_cols = final_covariates_map[group] + [group, \"caps5_change_baseline\"]\n",
    "\n",
    "                # Select only necessary columns\n",
    "                df_trimmed = df.loc[trimmed_idx, needed_cols].copy()\n",
    "                df_trimmed[\"iptw\"] = iptw_matrix[f\"iptw_imp{i+1}\"].loc[trimmed_idx]\n",
    "\n",
    "                # ✅ DROP rows with missing IPTW values\n",
    "                before = len(df_trimmed)\n",
    "                df_trimmed = df_trimmed.dropna(subset=[\"iptw\"])\n",
    "                after = len(df_trimmed)\n",
    "                print(f\"    ℹ️ Retained {after}/{before} rows after IPTW NaN drop.\")\n",
    "\n",
    "                # Save to .pkl\n",
    "                df_trimmed.to_pickle(os.path.join(output_folder, f\"trimmed_data_imp{i+1}.pkl\"))\n",
    "                print(f\"  💾 Saved trimmed dataset: {output_folder}/trimmed_data_imp{i+1}.*\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error in {group}: {e}\")\n",
    "\n",
    "\n",
    "run_trim_clip_save_all(imputed_dfs, medication_groups, final_covariates_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c885c8-0ce8-4666-83e8-5926e92c2ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_ps_overlap_all_groups(medication_groups):\n",
    "    for group in medication_groups:\n",
    "        print(f\"\\n📊 Plotting PS overlap for {group}\")\n",
    "\n",
    "        folder = os.path.join(\"outputs\", group)\n",
    "        ps_file = os.path.join(folder, \"propensity_scores.xlsx\")\n",
    "        iptw_file = os.path.join(folder, \"iptw_weights.xlsx\")\n",
    "        trimmed_file = os.path.join(folder, \"trimmed_data_imp1.pkl\")\n",
    "\n",
    "        if not all(os.path.exists(f) for f in [ps_file, iptw_file, trimmed_file]):\n",
    "            print(f\"⚠️ Missing required files for {group}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            ps_df = pd.read_excel(ps_file, index_col=0)\n",
    "            iptw_df = pd.read_excel(iptw_file, index_col=0)\n",
    "            trimmed_df = pd.read_pickle(trimmed_file)\n",
    "\n",
    "            # Extract\n",
    "            ps = ps_df[\"composite_ps\"].reindex(trimmed_df.index)\n",
    "            w = iptw_df[\"iptw_mean\"].reindex(trimmed_df.index)\n",
    "            T = trimmed_df[group]\n",
    "\n",
    "            # Masks to remove NaNs\n",
    "            treated_mask = (T == 1) & ps.notna() & w.notna()\n",
    "            control_mask = (T == 0) & ps.notna() & w.notna()\n",
    "\n",
    "            treated = ps[treated_mask]\n",
    "            treated_w = w[treated_mask]\n",
    "\n",
    "            control = ps[control_mask]\n",
    "            control_w = w[control_mask]\n",
    "\n",
    "            # === Unweighted Plot ===\n",
    "            plt.figure(figsize=(8, 5))\n",
    "            plt.hist([treated, control], bins=25, label=[\"Treated\", \"Control\"], alpha=0.6)\n",
    "            plt.title(f\"Unweighted PS Overlap - {group}\")\n",
    "            plt.xlabel(\"Composite Propensity Score\")\n",
    "            plt.ylabel(\"Count\")\n",
    "            plt.legend()\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(folder, \"ps_overlap_unweighted.png\"))\n",
    "            plt.close()\n",
    "\n",
    "            # === Weighted Plot ===\n",
    "            plt.figure(figsize=(8, 5))\n",
    "            plt.hist([treated, control], bins=25, weights=[treated_w, control_w], label=[\"Treated\", \"Control\"], alpha=0.6)\n",
    "            plt.title(f\"Weighted PS Overlap - {group}\")\n",
    "            plt.xlabel(\"Composite Propensity Score\")\n",
    "            plt.ylabel(\"Weighted Count\")\n",
    "            plt.legend()\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(folder, \"ps_overlap_weighted.png\"))\n",
    "            plt.close()\n",
    "\n",
    "            print(f\"✅ Saved unweighted and weighted PS plots for {group}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error processing {group}: {e}\")\n",
    "\n",
    "# 🔁 Run\n",
    "plot_ps_overlap_all_groups(medication_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9416ccc5-be17-41b6-b800-1388807f0170",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set up base output folder\n",
    "output_base = \"outputs\"\n",
    "ps_file = \"propensity_scores.xlsx\"\n",
    "iptw_file = \"iptw_weights.xlsx\"\n",
    "trimmed_data_file = \"trimmed_data_imp1.pkl\"\n",
    "\n",
    "# Collect all treatment group folders\n",
    "groups = [g for g in medication_groups if os.path.isdir(os.path.join(output_base, g))]\n",
    "\n",
    "\n",
    "# Generate 4-panel overlap plots\n",
    "for group in groups:\n",
    "    group_path = os.path.join(output_base, group)\n",
    "    try:\n",
    "        # Load trimmed treatment info\n",
    "        trimmed_df = pd.read_pickle(os.path.join(group_path, trimmed_data_file))\n",
    "        index = trimmed_df.index\n",
    "\n",
    "        # Fix: case-insensitive match for treatment variable\n",
    "        possible_cols = [col for col in trimmed_df.columns if col.upper() == group.upper()]\n",
    "        if not possible_cols:\n",
    "            print(f\" Treatment variable {group} not found in {group}, skipping.\")\n",
    "            continue\n",
    "        treatment_var = possible_cols[0]\n",
    "        T = trimmed_df[treatment_var]\n",
    "\n",
    "        # Load composite PS (aligned to trimmed_df index)\n",
    "        ps_df = pd.read_excel(os.path.join(group_path, ps_file), index_col=0)\n",
    "        if 'composite_ps' not in ps_df.columns:\n",
    "            print(f\" Composite column missing in {ps_file}, skipping {group}.\")\n",
    "            continue\n",
    "        ps = ps_df.loc[index, 'composite_ps']\n",
    "\n",
    "        # Load IPTW weights (aligned to trimmed_df index)\n",
    "        weights_df = pd.read_excel(os.path.join(group_path, iptw_file), index_col=0)\n",
    "        if 'iptw_mean' not in weights_df.columns:\n",
    "            print(f\" IPTW weight column missing in {iptw_file}, skipping {group}.\")\n",
    "            continue\n",
    "        weights = weights_df.loc[index, 'iptw_mean']\n",
    "\n",
    "        # Prepare 4 datasets\n",
    "        raw_treated = ps[T == 1]\n",
    "        raw_control = ps[T == 0]\n",
    "        weighted_treated = (ps[T == 1], weights[T == 1])\n",
    "        weighted_control = (ps[T == 0], weights[T == 0])\n",
    "\n",
    "        # Create plot\n",
    "        fig, axs = plt.subplots(2, 2, figsize=(12, 8))\n",
    "        fig.suptitle(f\"Propensity Score Distribution - {group}\", fontsize=14)\n",
    "\n",
    "        axs[0, 0].hist(raw_treated, bins=20, alpha=0.7, color='blue')\n",
    "        axs[0, 0].set_title(\"Raw Treated\")\n",
    "\n",
    "        axs[0, 1].hist(raw_control, bins=20, alpha=0.7, color='green')\n",
    "        axs[0, 1].set_title(\"Raw Control\")\n",
    "\n",
    "        axs[1, 0].hist(weighted_treated[0], bins=20, weights=weighted_treated[1], alpha=0.7, color='blue')\n",
    "        axs[1, 0].set_title(\"Weighted Treated\")\n",
    "\n",
    "        axs[1, 1].hist(weighted_control[0], bins=20, weights=weighted_control[1], alpha=0.7, color='green')\n",
    "        axs[1, 1].set_title(\"Weighted Control\")\n",
    "\n",
    "        for ax in axs.flat:\n",
    "            ax.set_xlim(0, 1)\n",
    "            ax.set_xlabel(\"Propensity Score\")\n",
    "            ax.set_ylabel(\"Count\")\n",
    "\n",
    "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "\n",
    "        # Save figure\n",
    "        plot_path = os.path.join(group_path, f\"four_panel_overlap_{group}.png\")\n",
    "        plt.savefig(plot_path)\n",
    "        plt.close()\n",
    "        print(f\" ✅ Saved: {plot_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\" ❌ Error in {group}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ffe0fe-e79d-49bb-9f2b-7e652c1e72ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ATT calculation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37018865-e063-4f96-a285-b905dc33fabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weighted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f04f14-2d87-4c64-ace6-45cb7c97414e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import KFold\n",
    "from scipy.stats import t, probplot\n",
    "import xgboost as xgb\n",
    "\n",
    "# -----------------------------\n",
    "# Configuration\n",
    "# -----------------------------\n",
    "n_repeats = 1\n",
    "seeds = list(range(1, 11))\n",
    "imputations = 5\n",
    "output_folder = \"outputs\"\n",
    "\n",
    "# -----------------------------\n",
    "# Diagnostic Plotting Function\n",
    "# -----------------------------\n",
    "def create_diagnostic_plots(residuals_data, fitted_data, group_name):\n",
    "    \"\"\"Create 4 diagnostic plots for model validation\"\"\"\n",
    "    plots_dir = os.path.join(output_folder, \"plots\")\n",
    "    os.makedirs(plots_dir, exist_ok=True)\n",
    "    \n",
    "    # Flatten the collected data\n",
    "    all_residuals = np.concatenate(residuals_data)\n",
    "    all_fitted = np.concatenate(fitted_data)\n",
    "    \n",
    "    # Create figure with 2x2 subplots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    fig.suptitle(f'Diagnostic Plots - {group_name}', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Residuals vs Fitted\n",
    "    axes[0,0].scatter(all_fitted, all_residuals, alpha=0.6, s=20)\n",
    "    axes[0,0].axhline(y=0, color='red', linestyle='--', alpha=0.8)\n",
    "    axes[0,0].set_xlabel('Fitted Values')\n",
    "    axes[0,0].set_ylabel('Residuals')\n",
    "    axes[0,0].set_title('Residuals vs Fitted')\n",
    "    axes[0,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. QQ Plot\n",
    "    probplot(all_residuals, dist=\"norm\", plot=axes[0,1])\n",
    "    axes[0,1].set_title('Q-Q Plot (Normal)')\n",
    "    axes[0,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Residual Histogram\n",
    "    axes[1,0].hist(all_residuals, bins=30, density=True, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    axes[1,0].set_xlabel('Residuals')\n",
    "    axes[1,0].set_ylabel('Density')\n",
    "    axes[1,0].set_title('Residual Distribution')\n",
    "    axes[1,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Scale-Location Plot\n",
    "    sqrt_abs_residuals = np.sqrt(np.abs(all_residuals))\n",
    "    axes[1,1].scatter(all_fitted, sqrt_abs_residuals, alpha=0.6, s=20)\n",
    "    axes[1,1].set_xlabel('Fitted Values')\n",
    "    axes[1,1].set_ylabel('√|Residuals|')\n",
    "    axes[1,1].set_title('Scale-Location Plot')\n",
    "    axes[1,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save plot\n",
    "    plot_filename = os.path.join(plots_dir, f'{group_name}.png')\n",
    "    plt.savefig(plot_filename, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"📊 Diagnostic plots saved for {group_name}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Rubin's Rule\n",
    "# -----------------------------\n",
    "def rubins_pool(estimates, ses):\n",
    "    m = len(estimates)\n",
    "    q_bar = np.mean(estimates)\n",
    "    u_bar = np.mean(np.square(ses))\n",
    "    b_m = np.var(estimates, ddof=1)\n",
    "    total_var = u_bar + ((1 + 1/m) * b_m)\n",
    "    total_se = np.sqrt(total_var)\n",
    "    ci_lower = q_bar - 1.96 * total_se\n",
    "    ci_upper = q_bar + 1.96 * total_se\n",
    "    p_value = 2 * (1 - t.cdf(np.abs(q_bar / total_se), df=m-1))\n",
    "    rounded_p = round(p_value, 5)\n",
    "    formatted_p = \"< 0.00001\" if rounded_p <= 0.00001 else f\"{rounded_p:.5f}\"\n",
    "    return q_bar, total_se, ci_lower, ci_upper, formatted_p\n",
    "\n",
    "# -----------------------------\n",
    "# SMD + Variance Ratio\n",
    "# -----------------------------\n",
    "def calculate_smd_vr(X, T, weights):\n",
    "    treated = X[T == 1]\n",
    "    control = X[T == 0]\n",
    "    w_treated = weights[T == 1]\n",
    "    w_control = weights[T == 0]\n",
    "    smd, vr = [], []\n",
    "    for col in X.columns:\n",
    "        try:\n",
    "            m1, m0 = np.average(treated[col], weights=w_treated), np.average(control[col], weights=w_control)\n",
    "            s1 = np.sqrt(np.average((treated[col] - m1) ** 2, weights=w_treated))\n",
    "            s0 = np.sqrt(np.average((control[col] - m0) ** 2, weights=w_control))\n",
    "            pooled_sd = np.sqrt((s1 ** 2 + s0 ** 2) / 2)\n",
    "            smd.append((m1 - m0) / pooled_sd if pooled_sd > 0 else 0)\n",
    "            vr.append(s1**2 / s0**2 if s0**2 > 0 else 0)\n",
    "        except Exception:\n",
    "            smd.append(np.nan)\n",
    "            vr.append(np.nan)\n",
    "    return np.nanmean(smd), np.nanmean(vr)\n",
    "\n",
    "# -----------------------------\n",
    "# XGBoost Main Loop (No DML)\n",
    "# -----------------------------\n",
    "def run_xgboost_with_trimmed_data(final_covariates_map):\n",
    "    att_results = []\n",
    "    balance_results = []\n",
    "\n",
    "    for group, covariates in final_covariates_map.items():\n",
    "        print(f\"\\n🚀 Running XGBoost for {group}\")\n",
    "        group_dir = os.path.join(output_folder, group)\n",
    "        os.makedirs(group_dir, exist_ok=True)\n",
    "\n",
    "        best_result = None\n",
    "        best_se = float(\"inf\")\n",
    "        \n",
    "        # Initialize lists to collect residuals and fitted values for diagnostic plots\n",
    "        group_residuals = []\n",
    "        group_fitted = []\n",
    "\n",
    "        for seed in seeds:\n",
    "            att_list, se_list, r2_list, rmse_list, smd_list, vr_list = [], [], [], [], [], []\n",
    "\n",
    "            for imp in range(1, imputations + 1):\n",
    "                file_path = os.path.join(group_dir, f\"trimmed_data_imp{imp}.pkl\")\n",
    "                if not os.path.exists(file_path):\n",
    "                    continue\n",
    "\n",
    "                df = pd.read_pickle(file_path)\n",
    "                if group not in df.columns or \"iptw\" not in df.columns:\n",
    "                    continue\n",
    "\n",
    "                X = df[covariates].copy()\n",
    "                T = df[group]\n",
    "                Y = df[\"caps5_change_baseline\"]\n",
    "                W = df[\"iptw\"]\n",
    "\n",
    "                for repeat in range(n_repeats):\n",
    "                    kf = KFold(n_splits=5, shuffle=True, random_state=seed + repeat)\n",
    "                    for train_idx, test_idx in kf.split(X):\n",
    "                        try:\n",
    "                            X_train, T_train, Y_train, W_train = (\n",
    "                                X.iloc[train_idx],\n",
    "                                T.iloc[train_idx],\n",
    "                                Y.iloc[train_idx],\n",
    "                                W.iloc[train_idx],\n",
    "                            )\n",
    "\n",
    "                            # XGBoost regression model\n",
    "                            model = xgb.XGBRegressor(n_estimators=100, max_depth=3, learning_rate=0.1, n_jobs=1, random_state=seed)\n",
    "                            \n",
    "                            # Add treatment variable to features\n",
    "                            X_train_with_T = X_train.copy()\n",
    "                            X_train_with_T[group] = T_train\n",
    "                            \n",
    "                            # Fit model\n",
    "                            model.fit(X_train_with_T, Y_train, sample_weight=W_train)\n",
    "                            \n",
    "                            # Predict outcomes for treated and control groups\n",
    "                            X_treated = X_train.copy()\n",
    "                            X_treated[group] = 1\n",
    "                            X_control = X_train.copy()\n",
    "                            X_control[group] = 0\n",
    "                            \n",
    "                            Y_pred_treated = model.predict(X_treated)\n",
    "                            Y_pred_control = model.predict(X_control)\n",
    "                            \n",
    "                            # Calculate ATT (Average Treatment Effect on Treated)\n",
    "                            treated_mask = T_train == 1\n",
    "                            if np.any(treated_mask):\n",
    "                                att = np.average(Y_pred_treated[treated_mask] - Y_pred_control[treated_mask], \n",
    "                                               weights=W_train[treated_mask])\n",
    "                                \n",
    "                                # Calculate standard error (approximate)\n",
    "                                treatment_effects = Y_pred_treated[treated_mask] - Y_pred_control[treated_mask]\n",
    "                                residual = treatment_effects - att\n",
    "                                se = np.sqrt(np.sum((W_train[treated_mask] * residual) ** 2)) / np.sum(W_train[treated_mask])\n",
    "\n",
    "                                \n",
    "                                att_list.append(att)\n",
    "                                se_list.append(se)\n",
    "\n",
    "                            # Model performance metrics\n",
    "                            Y_pred = model.predict(X_train_with_T)\n",
    "                            residuals = Y_train - Y_pred\n",
    "                            rmse = mean_squared_error(Y_train, Y_pred, squared=False)\n",
    "                            r2 = r2_score(Y_train, Y_pred)\n",
    "                            r2_list.append(r2)\n",
    "                            rmse_list.append(rmse)\n",
    "                            \n",
    "                            # Collect residuals and fitted values for diagnostic plots\n",
    "                            group_residuals.append(residuals.values)\n",
    "                            group_fitted.append(Y_pred)\n",
    "\n",
    "                            smd, vr = calculate_smd_vr(X_train, T_train, W_train)\n",
    "                            smd_list.append(smd)\n",
    "                            vr_list.append(vr)\n",
    "                        except Exception as e:\n",
    "                            print(f\"⚠️ Error in {group}, seed {seed}, imp {imp}, rep {repeat}: {e}\")\n",
    "\n",
    "            if att_list:\n",
    "                att, se, ci_l, ci_u, p_val = rubins_pool(att_list, se_list)\n",
    "                avg_r2 = np.mean(r2_list)\n",
    "                avg_rmse = np.mean(rmse_list)\n",
    "                avg_smd = np.mean(smd_list)\n",
    "                avg_vr = np.mean(vr_list)\n",
    "\n",
    "                balance_results.append({\n",
    "                    \"group\": group, \"seed\": seed, \"smd\": avg_smd, \"vr\": avg_vr\n",
    "                })\n",
    "\n",
    "                if se < best_se:\n",
    "                    best_se = se\n",
    "                    best_result = {\n",
    "                        \"group\": group, \"seed\": seed, \"att\": att, \"se\": se,\n",
    "                        \"ci_lower\": ci_l, \"ci_upper\": ci_u, \"p_value\": p_val,\n",
    "                        \"r2\": avg_r2, \"rmse\": avg_rmse\n",
    "                    }\n",
    "\n",
    "                print(f\"✅ {group} | Seed {seed}: ATT = {att:.4f}, SE = {se:.4f}, p = {p_val}\")\n",
    "            else:\n",
    "                print(f\"⚠️ No valid results for {group} | Seed {seed}\")\n",
    "\n",
    "        # Create diagnostic plots for this group\n",
    "        if group_residuals and group_fitted:\n",
    "            create_diagnostic_plots(group_residuals, group_fitted, group)\n",
    "\n",
    "        if best_result:\n",
    "            att_results.append(best_result)\n",
    "            print(f\"🏆 Best result for {group} → Seed {best_result['seed']} | SE = {best_result['se']:.4f}\")\n",
    "\n",
    "    # Save final output\n",
    "    pd.DataFrame(att_results).to_excel(\"xgb_rubin_summary_subcats.xlsx\", index=False)\n",
    "    pd.DataFrame(balance_results).to_excel(\"smd_vr_summary_subcats.xlsx\", index=False)\n",
    "    print(\"\\n🎯 All summary files saved.\")\n",
    "\n",
    "run_xgboost_with_trimmed_data(final_covariates_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e354a90-c1e2-491d-9a6a-28dc4ac2cf12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unweighted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e72bfe5-59ae-4ce7-bda2-80909a87b90f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import KFold\n",
    "from scipy.stats import t, probplot\n",
    "import xgboost as xgb\n",
    "\n",
    "# -----------------------------\n",
    "# Configuration\n",
    "# -----------------------------\n",
    "n_repeats = 1\n",
    "seeds = list(range(1, 11))\n",
    "imputations = 5\n",
    "output_folder = \"outputs\"\n",
    "\n",
    "# -----------------------------\n",
    "# Diagnostic Plotting Function\n",
    "# -----------------------------\n",
    "def create_diagnostic_plots(residuals_data, fitted_data, group_name):\n",
    "    \"\"\"Create 4 diagnostic plots for model validation\"\"\"\n",
    "    plots_dir = os.path.join(output_folder, \"plots\")\n",
    "    os.makedirs(plots_dir, exist_ok=True)\n",
    "    \n",
    "    # Flatten the collected data\n",
    "    all_residuals = np.concatenate(residuals_data)\n",
    "    all_fitted = np.concatenate(fitted_data)\n",
    "    \n",
    "    # Create figure with 2x2 subplots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    fig.suptitle(f'Diagnostic Plots - {group_name}', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Residuals vs Fitted\n",
    "    axes[0,0].scatter(all_fitted, all_residuals, alpha=0.6, s=20)\n",
    "    axes[0,0].axhline(y=0, color='red', linestyle='--', alpha=0.8)\n",
    "    axes[0,0].set_xlabel('Fitted Values')\n",
    "    axes[0,0].set_ylabel('Residuals')\n",
    "    axes[0,0].set_title('Residuals vs Fitted')\n",
    "    axes[0,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. QQ Plot\n",
    "    probplot(all_residuals, dist=\"norm\", plot=axes[0,1])\n",
    "    axes[0,1].set_title('Q-Q Plot (Normal)')\n",
    "    axes[0,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Residual Histogram\n",
    "    axes[1,0].hist(all_residuals, bins=30, density=True, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    axes[1,0].set_xlabel('Residuals')\n",
    "    axes[1,0].set_ylabel('Density')\n",
    "    axes[1,0].set_title('Residual Distribution')\n",
    "    axes[1,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Scale-Location Plot\n",
    "    sqrt_abs_residuals = np.sqrt(np.abs(all_residuals))\n",
    "    axes[1,1].scatter(all_fitted, sqrt_abs_residuals, alpha=0.6, s=20)\n",
    "    axes[1,1].set_xlabel('Fitted Values')\n",
    "    axes[1,1].set_ylabel('√|Residuals|')\n",
    "    axes[1,1].set_title('Scale-Location Plot')\n",
    "    axes[1,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save plot\n",
    "    plot_filename = os.path.join(plots_dir, f'{group_name}_unweighted.png')\n",
    "    plt.savefig(plot_filename, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"📊 Diagnostic plots saved for {group_name}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Rubin's Rule\n",
    "# -----------------------------\n",
    "def rubins_pool(estimates, ses):\n",
    "    m = len(estimates)\n",
    "    q_bar = np.mean(estimates)\n",
    "    u_bar = np.mean(np.square(ses))\n",
    "    b_m = np.var(estimates, ddof=1)\n",
    "    total_var = u_bar + ((1 + 1/m) * b_m)\n",
    "    total_se = np.sqrt(total_var)\n",
    "    ci_lower = q_bar - 1.96 * total_se\n",
    "    ci_upper = q_bar + 1.96 * total_se\n",
    "    p_value = 2 * (1 - t.cdf(np.abs(q_bar / total_se), df=m-1))\n",
    "    rounded_p = round(p_value, 5)\n",
    "    formatted_p = \"< 0.00001\" if rounded_p <= 0.00001 else f\"{rounded_p:.5f}\"\n",
    "    return q_bar, total_se, ci_lower, ci_upper, formatted_p\n",
    "\n",
    "# -----------------------------\n",
    "# SMD + Variance Ratio\n",
    "# -----------------------------\n",
    "def calculate_smd_vr(X, T):\n",
    "    treated = X[T == 1]\n",
    "    control = X[T == 0]\n",
    "    smd, vr = [], []\n",
    "    for col in X.columns:\n",
    "        try:\n",
    "            m1, m0 = np.mean(treated[col]), np.mean(control[col])\n",
    "            s1 = np.std(treated[col])\n",
    "            s0 = np.std(control[col])\n",
    "            pooled_sd = np.sqrt((s1 ** 2 + s0 ** 2) / 2)\n",
    "            smd.append((m1 - m0) / pooled_sd if pooled_sd > 0 else 0)\n",
    "            vr.append(s1**2 / s0**2 if s0**2 > 0 else 0)\n",
    "        except Exception:\n",
    "            smd.append(np.nan)\n",
    "            vr.append(np.nan)\n",
    "    return np.nanmean(smd), np.nanmean(vr)\n",
    "\n",
    "# -----------------------------\n",
    "# XGBoost Main Loop (No DML)\n",
    "# -----------------------------\n",
    "def run_xgboost_with_trimmed_data(final_covariates_map):\n",
    "    att_results = []\n",
    "    balance_results = []\n",
    "\n",
    "    for group, covariates in final_covariates_map.items():\n",
    "        print(f\"\\n🚀 Running XGBoost for {group}\")\n",
    "        group_dir = os.path.join(output_folder, group)\n",
    "        os.makedirs(group_dir, exist_ok=True)\n",
    "\n",
    "        best_result = None\n",
    "        best_se = float(\"inf\")\n",
    "        \n",
    "        # Initialize lists to collect residuals and fitted values for diagnostic plots\n",
    "        group_residuals = []\n",
    "        group_fitted = []\n",
    "\n",
    "        for seed in seeds:\n",
    "            att_list, se_list, r2_list, rmse_list, smd_list, vr_list = [], [], [], [], [], []\n",
    "\n",
    "            for imp in range(1, imputations + 1):\n",
    "                file_path = os.path.join(group_dir, f\"trimmed_data_imp{imp}.pkl\")\n",
    "                if not os.path.exists(file_path):\n",
    "                    continue\n",
    "\n",
    "                df = pd.read_pickle(file_path)\n",
    "                if group not in df.columns or \"iptw\" not in df.columns:\n",
    "                    continue\n",
    "\n",
    "                X = df[covariates].copy()\n",
    "                T = df[group]\n",
    "                Y = df[\"caps5_change_baseline\"]\n",
    "                #W = df[\"iptw\"]\n",
    "\n",
    "                for repeat in range(n_repeats):\n",
    "                    kf = KFold(n_splits=5, shuffle=True, random_state=seed + repeat)\n",
    "                    for train_idx, test_idx in kf.split(X):\n",
    "                        try:\n",
    "                            X_train = X.iloc[train_idx]\n",
    "                            T_train = T.iloc[train_idx]\n",
    "                            Y_train = Y.iloc[train_idx]\n",
    "                            \n",
    "\n",
    "                            # XGBoost regression model\n",
    "                            model = xgb.XGBRegressor(n_estimators=100, max_depth=3, learning_rate=0.1, n_jobs=1, random_state=seed)\n",
    "                            \n",
    "                            # Add treatment variable to features\n",
    "                            X_train_with_T = X_train.copy()\n",
    "                            X_train_with_T[group] = T_train\n",
    "                            \n",
    "                            # Fit model\n",
    "                            model.fit(X_train_with_T, Y_train)\n",
    "                            \n",
    "                            # Predict outcomes for treated and control groups\n",
    "                            X_treated = X_train.copy()\n",
    "                            X_treated[group] = 1\n",
    "                            X_control = X_train.copy()\n",
    "                            X_control[group] = 0\n",
    "                            \n",
    "                            Y_pred_treated = model.predict(X_treated)\n",
    "                            Y_pred_control = model.predict(X_control)\n",
    "                            \n",
    "                            # Calculate ATT (Average Treatment Effect on Treated)\n",
    "                            treated_mask = T_train == 1\n",
    "                            if np.any(treated_mask):\n",
    "                                att = np.mean(Y_pred_treated[treated_mask] - Y_pred_control[treated_mask])\n",
    "                                \n",
    "                                # Calculate standard error (approximate)\n",
    "                                treatment_effects = Y_pred_treated[treated_mask] - Y_pred_control[treated_mask]\n",
    "                                residual = treatment_effects - att\n",
    "                                se = np.sqrt(np.sum((residual) ** 2)) / np.sum(treated_mask)\n",
    "                                att_list.append(att)\n",
    "                                se_list.append(se)\n",
    "\n",
    "                            # Model performance metrics\n",
    "                            Y_pred = model.predict(X_train_with_T)\n",
    "                            residuals = Y_train - Y_pred\n",
    "                            rmse = mean_squared_error(Y_train, Y_pred, squared=False)\n",
    "                            r2 = r2_score(Y_train, Y_pred)\n",
    "                            r2_list.append(r2)\n",
    "                            rmse_list.append(rmse)\n",
    "                            \n",
    "                            # Collect residuals and fitted values for diagnostic plots\n",
    "                            group_residuals.append(residuals.values)\n",
    "                            group_fitted.append(Y_pred)\n",
    "\n",
    "                            smd, vr = calculate_smd_vr(X_train, T_train)\n",
    "                            smd_list.append(smd)\n",
    "                            vr_list.append(vr)\n",
    "                        except Exception as e:\n",
    "                            print(f\"⚠️ Error in {group}, seed {seed}, imp {imp}, rep {repeat}: {e}\")\n",
    "\n",
    "            if att_list:\n",
    "                att, se, ci_l, ci_u, p_val = rubins_pool(att_list, se_list)\n",
    "                avg_r2 = np.mean(r2_list)\n",
    "                avg_rmse = np.mean(rmse_list)\n",
    "                avg_smd = np.mean(smd_list)\n",
    "                avg_vr = np.mean(vr_list)\n",
    "\n",
    "                balance_results.append({\n",
    "                    \"group\": group, \"seed\": seed, \"smd\": avg_smd, \"vr\": avg_vr\n",
    "                })\n",
    "\n",
    "                if se < best_se:\n",
    "                    best_se = se\n",
    "                    best_result = {\n",
    "                        \"group\": group, \"seed\": seed, \"att\": att, \"se\": se,\n",
    "                        \"ci_lower\": ci_l, \"ci_upper\": ci_u, \"p_value\": p_val,\n",
    "                        \"r2\": avg_r2, \"rmse\": avg_rmse\n",
    "                    }\n",
    "\n",
    "                print(f\"✅ {group} | Seed {seed}: ATT = {att:.4f}, SE = {se:.4f}, p = {p_val}\")\n",
    "            else:\n",
    "                print(f\"⚠️ No valid results for {group} | Seed {seed}\")\n",
    "\n",
    "        # Create diagnostic plots for this group\n",
    "        if group_residuals and group_fitted:\n",
    "            create_diagnostic_plots(group_residuals, group_fitted, group)\n",
    "\n",
    "        if best_result:\n",
    "            att_results.append(best_result)\n",
    "            print(f\"🏆 Best result for {group} → Seed {best_result['seed']} | SE = {best_result['se']:.4f}\")\n",
    "\n",
    "    # Save final output\n",
    "    pd.DataFrame(att_results).to_excel(\"xgb_rubin_summary_subcats_unweighted.xlsx\", index=False)\n",
    "    pd.DataFrame(balance_results).to_excel(\"smd_vr_summary_subcats_unweighted.xlsx\", index=False)\n",
    "    print(\"\\n🎯 All summary files saved.\")\n",
    "\n",
    "run_xgboost_with_trimmed_data(final_covariates_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d115ca6-36c5-48a5-b13c-4147a3a01119",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import sem, ttest_ind\n",
    "\n",
    "# ----------------------------------\n",
    "# File paths\n",
    "# ----------------------------------\n",
    "output_base = \"outputs\"\n",
    "att_file = \"xgb_rubin_summary_subcats.xlsx\"\n",
    "trimmed_file = \"trimmed_data_imp1.pkl\"\n",
    "auc_file = \"auc_scores.xlsx\"  \n",
    "\n",
    "# ----------------------------------\n",
    "# Load ATT Summary\n",
    "# ----------------------------------\n",
    "if os.path.exists(att_file):\n",
    "    att_df = pd.read_excel(att_file)\n",
    "else:\n",
    "    raise FileNotFoundError(\"❌ ATT summary file not found: xgb_rubin_summary_subcats.xlsx\")\n",
    "\n",
    "summary_rows = []\n",
    "\n",
    "# ----------------------------------\n",
    "# Loop over medication groups\n",
    "# ----------------------------------\n",
    "groups = [g for g in medication_groups if os.path.isdir(os.path.join(output_base, g))]\n",
    "\n",
    "for med in groups:\n",
    "    try:\n",
    "        group_path = os.path.join(output_base, med)\n",
    "\n",
    "        # Load trimmed data\n",
    "        df = pd.read_pickle(os.path.join(group_path, trimmed_file))\n",
    "\n",
    "        # Detect treatment column\n",
    "        treatment_cols = [col for col in df.columns if col.upper() == med.upper()]\n",
    "        if not treatment_cols:\n",
    "            print(f\"⚠️ Treatment column {med} not found in trimmed data. Skipping.\")\n",
    "            continue\n",
    "        treatment_var = treatment_cols[0]\n",
    "\n",
    "        # Extract treatment and outcome\n",
    "        T = df[treatment_var]\n",
    "        Y = df[\"caps5_change_baseline\"]\n",
    "\n",
    "        # Treated and control stats\n",
    "        treated = Y[T == 1]\n",
    "        control = Y[T == 0]\n",
    "\n",
    "        mean_treat = treated.mean()\n",
    "        se_treat = sem(treated) if len(treated) > 1 else np.nan\n",
    "\n",
    "        mean_ctrl = control.mean()\n",
    "        se_ctrl = sem(control) if len(control) > 1 else np.nan\n",
    "\n",
    "        # Cohen's d (unadjusted)\n",
    "        pooled_sd = np.sqrt(((treated.std() ** 2) + (control.std() ** 2)) / 2)\n",
    "        cohen_d = (mean_treat - mean_ctrl) / pooled_sd if pooled_sd > 0 else np.nan\n",
    "\n",
    "        # E-value (unadjusted)\n",
    "        delta = mean_treat - mean_ctrl\n",
    "        E = delta / abs(mean_ctrl) * 100 if mean_ctrl != 0 else np.nan\n",
    "\n",
    "        # Unadjusted p-value\n",
    "        try:\n",
    "            t_stat, p_val = ttest_ind(treated, control, equal_var=False, nan_policy=\"omit\")\n",
    "            rounded_p = round(p_val, 5)\n",
    "            formatted_p = \"< 0.00001\" if rounded_p < 0.00001 else rounded_p\n",
    "        except Exception:\n",
    "            formatted_p = np.nan\n",
    "\n",
    "        # AUC from new auc_scores.xlsx file\n",
    "        auc_val = np.nan\n",
    "        auc_path = os.path.join(group_path, auc_file)\n",
    "        if os.path.exists(auc_path):\n",
    "            auc_df = pd.read_excel(auc_path)\n",
    "            if \"AUC\" in auc_df.columns:\n",
    "                auc_val = auc_df[\"AUC\"].dropna().mean()\n",
    "\n",
    "        # Adjusted stats from Rubin summary\n",
    "        att_row = att_df[att_df[\"group\"].str.strip().str.upper() == med.strip().upper()]\n",
    "        if not att_row.empty:\n",
    "            att = att_row.iloc[0][\"att\"]\n",
    "            att_se = att_row.iloc[0][\"se\"]\n",
    "            att_p_val = att_row.iloc[0][\"p_value\"]\n",
    "            r2 = att_row.iloc[0][\"r2\"]\n",
    "            rmse = att_row.iloc[0][\"rmse\"]\n",
    "\n",
    "            try:\n",
    "                rounded_att_p = round(float(att_p_val), 5)\n",
    "                formatted_att_p = \"< 0.00001\" if rounded_att_p < 0.00001 else rounded_att_p\n",
    "            except:\n",
    "                formatted_att_p = att_p_val\n",
    "        else:\n",
    "            att, att_se, formatted_att_p, r2, rmse = np.nan, np.nan, np.nan, np.nan, np.nan\n",
    "\n",
    "        # Append full row\n",
    "        summary_rows.append({\n",
    "            'Medication Group': med,\n",
    "            'Mean Treated': mean_treat,\n",
    "            'SE Treated': se_treat,\n",
    "            'Mean Control': mean_ctrl,\n",
    "            'SE Control': se_ctrl,\n",
    "            'Cohen d': cohen_d,\n",
    "            'E (Unadjusted)': E,\n",
    "            'n Treated': len(treated),\n",
    "            'n Control': len(control),\n",
    "            #'Unadjusted p-value': formatted_p,\n",
    "            'ATT Estimate': att,\n",
    "            'ATT SE (Robust)': att_se,\n",
    "            'ATT p-value': formatted_att_p,\n",
    "            'R²': r2,\n",
    "            'RMSE': rmse,\n",
    "            'AUC': auc_val\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error in {med}: {e}\")\n",
    "\n",
    "# ----------------------------------\n",
    "# Save final summary\n",
    "# ----------------------------------\n",
    "summary_df = pd.DataFrame(summary_rows)\n",
    "summary_df = summary_df.sort_values(\"Medication Group\")\n",
    "summary_df.to_excel(\"Final_ATT_Summary_SubCat.xlsx\", index=False)\n",
    "print(\"✅ Final_ATT_Summary_SubCat saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630929b1-b3f8-4767-8b90-68918d6543ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "# ✅ Load the final summary table\n",
    "final_df = pd.read_excel(\"Final_ATT_Summary_SubCat.xlsx\")\n",
    "\n",
    "# ✅ Parse DML p-values (handle \"< 0.00001\")\n",
    "def parse_pval(p):\n",
    "    try:\n",
    "        if isinstance(p, str) and \"<\" in p:\n",
    "            return 0.000001\n",
    "        return float(p)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "final_df['ATT p-value'] = final_df['ATT p-value'].apply(parse_pval)\n",
    "\n",
    "# ✅ Plot settings\n",
    "width = 0.35\n",
    "\n",
    "# ✅ Plotting function for a single medication group\n",
    "def plot_single_group(row):\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    bars1 = ax.bar(-width/2, row['Mean Control'], width, \n",
    "                   yerr=row['SE Control'], label='Control', hatch='//', color='gray', capsize=5)\n",
    "    bars2 = ax.bar(+width/2, row['Mean Treated'], width, \n",
    "                   yerr=row['SE Treated'], label='Treated', color='steelblue', capsize=5)\n",
    "\n",
    "    label = (\n",
    "        f\"ATT = {row['ATT Estimate']:.2f}\\n\"\n",
    "        f\"d = {row['Cohen d']:.2f}, p = {row['ATT p-value']:.3f}\\n\"\n",
    "        f\"nT = {row['n Treated']}, nC = {row['n Control']}\\n\"\n",
    "        f\"E = {row['E (Unadjusted)']:.1f}%\"\n",
    "    )\n",
    "    max_y = max(row['Mean Control'], row['Mean Treated']) + 1.5\n",
    "    ax.text(0, max_y, label, ha='center', va='bottom', fontsize=9, color='#FFD700')\n",
    "\n",
    "    ax.axhline(0, color='black', linewidth=0.8)\n",
    "    ax.set_xticks([-width/2, +width/2])\n",
    "    ax.set_xticklabels(['Control', 'Treated'])\n",
    "    ax.set_title(f\"Group: {row['Medication Group']}\", fontsize=12, weight='bold')\n",
    "    ax.set_ylabel(\"CAPS5 Change Score\")\n",
    "    ax.set_ylim(bottom=0, top=max_y + 2)\n",
    "    ax.legend()\n",
    "    fig.tight_layout()\n",
    "    return fig\n",
    "\n",
    "# ✅ Generate and save all plots into a multi-page PDF\n",
    "with PdfPages(\"xgb_att_barplot_subcat.pdf\") as pdf:\n",
    "    for idx, row in final_df.iterrows():\n",
    "        fig = plot_single_group(row)\n",
    "        pdf.savefig(fig, dpi=300, bbox_inches='tight')\n",
    "        plt.close(fig)\n",
    "print(\"✅ xgb_att_barplot_subcat saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cccc7ca-28e3-40b5-a9ea-17419902df6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Love plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0812ab6-4512-4742-9f4d-26503f46c4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "# ----------------------------------------\n",
    "# Functions to calculate balance\n",
    "# ----------------------------------------\n",
    "def calculate_smd(x1, x2, w1=None, w2=None):\n",
    "    def weighted_mean(x, w): return np.average(x, weights=w)\n",
    "    def weighted_var(x, w):\n",
    "        m = np.average(x, weights=w)\n",
    "        return np.average((x - m) ** 2, weights=w)\n",
    "    \n",
    "    m1 = weighted_mean(x1, w1) if w1 is not None else np.mean(x1)\n",
    "    m2 = weighted_mean(x2, w2) if w2 is not None else np.mean(x2)\n",
    "    v1 = weighted_var(x1, w1) if w1 is not None else np.var(x1, ddof=1)\n",
    "    v2 = weighted_var(x2, w2) if w2 is not None else np.var(x2, ddof=1)\n",
    "    \n",
    "    pooled_sd = np.sqrt((v1 + v2) / 2)\n",
    "    return np.abs(m1 - m2) / pooled_sd if pooled_sd > 0 else 0\n",
    "\n",
    "def variance_ratio(x1, x2, w1=None, w2=None):\n",
    "    def weighted_var(x, w):\n",
    "        m = np.average(x, weights=w)\n",
    "        return np.average((x - m) ** 2, weights=w)\n",
    "    \n",
    "    v1 = weighted_var(x1, w1) if w1 is not None else np.var(x1, ddof=1)\n",
    "    v2 = weighted_var(x2, w2) if w2 is not None else np.var(x2, ddof=1)\n",
    "    \n",
    "    return max(v1 / v2, v2 / v1) if v1 > 0 and v2 > 0 else 1\n",
    "\n",
    "# ----------------------------------------\n",
    "# Setup\n",
    "# ----------------------------------------\n",
    "output_base = \"outputs\"\n",
    "groups = [g for g in os.listdir(output_base) if os.path.isdir(os.path.join(output_base, g))]\n",
    "\n",
    "# Create a case-insensitive mapping\n",
    "final_covariates_map_lower = {k.lower(): v for k, v in final_covariates_map.items()}\n",
    "\n",
    "# ----------------------------------------\n",
    "# Main Loop\n",
    "# ----------------------------------------\n",
    "for group in groups:\n",
    "    if group.lower() not in final_covariates_map_lower:\n",
    "        continue\n",
    "\n",
    "    print(f\"\\n🔍 Processing {group}...\")\n",
    "\n",
    "    try:\n",
    "        group_path = os.path.join(output_base, group)\n",
    "        covariates = final_covariates_map_lower[group.lower()]\n",
    "        \n",
    "        column_name = None\n",
    "        for col in pd.read_pickle(os.path.join(group_path, \"trimmed_data_imp1.pkl\")).columns:\n",
    "            if col.lower() == group.lower():\n",
    "                column_name = col\n",
    "                break\n",
    "        if column_name is None:\n",
    "            print(f\"⚠️ Column not found for {group}, skipping.\")\n",
    "            continue\n",
    "\n",
    "        smd_unw_all, smd_w_all = [], []\n",
    "        vr_unw_all, vr_w_all = [], []\n",
    "\n",
    "        for i in range(1, 6):\n",
    "            df_path = os.path.join(group_path, f\"trimmed_data_imp{i}.pkl\")\n",
    "            iptw_path = os.path.join(group_path, \"iptw_weights.xlsx\")\n",
    "\n",
    "            if not os.path.exists(df_path) or not os.path.exists(iptw_path):\n",
    "                print(f\"⚠️ Missing data for {group} imp{i}, skipping.\")\n",
    "                continue\n",
    "\n",
    "            df = pd.read_pickle(df_path)\n",
    "            iptw_df = pd.read_excel(iptw_path, index_col=0)\n",
    "            T = df[column_name]\n",
    "            W = iptw_df.loc[df.index, \"iptw_mean\"]\n",
    "\n",
    "            smd_unw_i, smd_w_i, vr_unw_i, vr_w_i = [], [], [], []\n",
    "\n",
    "            for cov in covariates:\n",
    "                x1, x0 = df.loc[T == 1, cov], df.loc[T == 0, cov]\n",
    "                w1, w0 = W[T == 1], W[T == 0]\n",
    "\n",
    "                su = calculate_smd(x1, x0)\n",
    "                sw = calculate_smd(x1, x0, w1, w0)\n",
    "\n",
    "                vu = variance_ratio(x1, x0) if cov in ['treatmentdurationdays', 'CAPS5score_baseline', 'age'] else np.nan\n",
    "                vw = variance_ratio(x1, x0, w1, w0) if cov in ['treatmentdurationdays', 'CAPS5score_baseline', 'age'] else np.nan\n",
    "\n",
    "                smd_unw_i.append(su)\n",
    "                smd_w_i.append(sw)\n",
    "                vr_unw_i.append(vu)\n",
    "                vr_w_i.append(vw)\n",
    "\n",
    "            smd_unw_all.append(smd_unw_i)\n",
    "            smd_w_all.append(smd_w_i)\n",
    "            vr_unw_all.append(vr_unw_i)\n",
    "            vr_w_all.append(vr_w_i)\n",
    "\n",
    "        smd_unw = np.mean(smd_unw_all, axis=0)\n",
    "        smd_w = np.mean(smd_w_all, axis=0)\n",
    "        vr_unw = np.nanmean(vr_unw_all, axis=0)\n",
    "        vr_w = np.nanmean(vr_w_all, axis=0)\n",
    "\n",
    "        severity = []\n",
    "        for sw in smd_w:\n",
    "            if sw <= 0.1:\n",
    "                severity.append(\"Good\")\n",
    "            elif sw <= 0.2:\n",
    "                severity.append(\"Moderate\")\n",
    "            else:\n",
    "                severity.append(\"Poor\")\n",
    "\n",
    "        covariate_names = covariates\n",
    "        numeric_df = pd.DataFrame({\n",
    "            \"Covariate\": covariate_names,\n",
    "            \"SMD_Unweighted\": smd_unw,\n",
    "            \"SMD_Weighted\": smd_w,\n",
    "            \"Imbalance_Severity\": severity,\n",
    "            \"VR_Unweighted\": vr_unw,\n",
    "            \"VR_Weighted\": vr_w\n",
    "        })\n",
    "\n",
    "        numeric_path = os.path.join(group_path, f\"covariate_balance_table_{group}.xlsx\")\n",
    "        numeric_df.to_excel(numeric_path, index=False)\n",
    "        print(f\"📊 Exported numeric summary to: {numeric_path}\")\n",
    "\n",
    "        # -------------------------\n",
    "        # Plot\n",
    "        # -------------------------\n",
    "        labels = covariates\n",
    "        y_pos = np.arange(len(labels))\n",
    "\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(18, len(labels) * 0.45))\n",
    "\n",
    "        axes[0].scatter(smd_unw, y_pos, color='red', label=\"Unweighted\")\n",
    "        axes[0].scatter(smd_w, y_pos, color='blue', label=\"Weighted\")\n",
    "        axes[0].axvline(0.1, color='gray', linestyle='--', label=\"Threshold 0.1\")\n",
    "        axes[0].axvline(0.2, color='black', linestyle='--', label=\"Threshold 0.2\")\n",
    "        axes[0].set_xlim(0, max(max(smd_unw), max(smd_w), 0.25) + 0.05)\n",
    "        axes[0].set_yticks(y_pos)\n",
    "        axes[0].set_yticklabels(labels)\n",
    "        axes[0].invert_yaxis()\n",
    "        axes[0].set_title(\"Standardized Mean Differences (SMD)\")\n",
    "        axes[0].legend(loc=\"upper right\")\n",
    "        axes[0].grid(True)\n",
    "\n",
    "        vr_mask = [cov in ['treatmentdurationdays', 'CAPS5score_baseline', 'age'] for cov in covariates]\n",
    "        filtered_y = [i for i, b in enumerate(vr_mask) if b]\n",
    "        filtered_labels = [labels[i] for i in filtered_y]\n",
    "        filtered_vr_unw = [vr_unw[i] for i in filtered_y]\n",
    "        filtered_vr_w = [vr_w[i] for i in filtered_y]\n",
    "\n",
    "        axes[1].scatter(filtered_vr_unw, filtered_y, color='blue', marker='o', label=\"Unweighted\")\n",
    "        axes[1].scatter(filtered_vr_w, filtered_y, color='red', marker='x', label=\"Weighted\")\n",
    "        axes[1].axvline(2, color='gray', linestyle='--')\n",
    "        axes[1].axvline(0.5, color='gray', linestyle='--')\n",
    "        axes[1].set_xlim(0, max(filtered_vr_unw + filtered_vr_w + [2.5]) + 0.5)\n",
    "        axes[1].set_yticks(filtered_y)\n",
    "        axes[1].set_yticklabels(filtered_labels)\n",
    "        axes[1].invert_yaxis()\n",
    "        axes[1].set_title(\"Variance Ratio (VR)\")\n",
    "        axes[1].legend()\n",
    "        axes[1].grid(True)\n",
    "\n",
    "        fig.suptitle(f\"Covariate Balance for {group.replace('CAT_', '')}\", fontsize=14, weight='bold')\n",
    "        fig.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "        plot_path = os.path.join(group_path, f\"love_plot_{group}.pdf\")\n",
    "        fig.savefig(plot_path, dpi=300)\n",
    "        plt.close()\n",
    "        print(f\"✅ Saved love plot: {plot_path}\")\n",
    "        print(f\"📏 Max weighted SMD for {group}: {np.max(smd_w):.3f}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error in {group}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae62a1bf-fc01-4e72-b029-43d68b098e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d944cd-c180-4252-8adb-fef061de0ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "#-----------------------------\n",
    "# Generate heatmaps\n",
    "# -------------------------------\n",
    "for treatment_var in medication_groups:\n",
    "    print(f\"\\n========== Creating Heatmap for {treatment_var} ==========\")\n",
    "\n",
    "    try:\n",
    "        output_folder = os.path.join('outputs', treatment_var)\n",
    "        balance_path = os.path.join(output_folder, f'covariate_balance_table_{treatment_var}.xlsx')\n",
    "\n",
    "        if not os.path.exists(balance_path):\n",
    "            print(f\"❌ Balance file not found: {balance_path}\")\n",
    "            continue\n",
    "\n",
    "        balance_df = pd.read_excel(balance_path)\n",
    "\n",
    "        # ✅ Use finalized covariates + 'Propensity Score'\n",
    "        covariates = final_covariates_map[treatment_var] + ['Propensity Score']\n",
    "        balance_df = balance_df[balance_df['Covariate'].isin(covariates)]\n",
    "\n",
    "        # ✅ Check for CAPS5score_baseline\n",
    "        highlight_caps = 'CAPS5score_baseline' in balance_df['Covariate'].values\n",
    "\n",
    "        # ✅ Format for heatmap\n",
    "        heatmap_df = balance_df[['Covariate', 'SMD_Unweighted', 'SMD_Weighted']].copy()\n",
    "        heatmap_df.columns = ['Covariate', 'Unweighted', 'Weighted']\n",
    "        heatmap_df = heatmap_df.set_index('Covariate')\n",
    "        heatmap_df = heatmap_df.sort_values(by='Unweighted', ascending=False)\n",
    "\n",
    "        # ✅ Plot\n",
    "        plt.figure(figsize=(12, max(10, len(heatmap_df) * 0.35)))\n",
    "        ax = sns.heatmap(\n",
    "            heatmap_df,\n",
    "            cmap=\"coolwarm\",\n",
    "            annot=True,\n",
    "            fmt=\".2f\",\n",
    "            linewidths=0.6,\n",
    "            linecolor='gray',\n",
    "            cbar_kws={\"label\": \"Standardized Mean Difference\"}\n",
    "        )\n",
    "\n",
    "        plt.title(f\"Covariate Balance Heatmap (Rubin IPTW)\\n{treatment_var}\", fontsize=15, weight='bold')\n",
    "        plt.xlabel(\"Condition\")\n",
    "        plt.ylabel(\"Covariate\")\n",
    "\n",
    "        # ✅ Bold CAPS5score_baseline if present\n",
    "        if highlight_caps:\n",
    "            ylabels = [label.get_text() for label in ax.get_yticklabels()]\n",
    "            ax.set_yticklabels([\n",
    "                f\"{label} ←\" if label == 'CAPS5score_baseline' else label for label in ylabels\n",
    "            ])\n",
    "\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # ✅ Save image\n",
    "        save_path = os.path.join(output_folder, f'heatmap_smd_{treatment_var}.png')\n",
    "        plt.savefig(save_path, dpi=300)\n",
    "        plt.close()\n",
    "\n",
    "        print(f\"✅ Heatmap saved: {save_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error processing {treatment_var}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711a844e-e3d4-45d5-9781-c324f75d3fbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5fbd63cf-32bb-4417-99c9-b6748d051820",
   "metadata": {},
   "source": [
    "## SubSubCat Analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262ea436-3978-4fab-a149-f0e3d43f04d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "covariates_SubSubCat_Oxazepam = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline',\n",
    "    'CAT_Antidepressiva', 'CAT_Antipsychotica',\n",
    "    'DIAGNOSIS_CHILDHOOD_TRAUMA', 'DIAGNOSIS_CPTSD', 'DIAGNOSIS_SEXUAL_TRAUMA',\n",
    "    'DIAGNOSIS_SUICIDALITY', 'age',\n",
    "    'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"Bipolar_and_Mood_disorder\", \"ethnicity_Dutch\", \"ethnicity_other\"\n",
    "]\n",
    "\n",
    "covariates_SubSubCat_Diazepam = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline',\n",
    "    'CAT_Antidepressiva', 'CAT_Antipsychotica',\n",
    "    'DIAGNOSIS_CHILDHOOD_TRAUMA', 'DIAGNOSIS_CPTSD', 'DIAGNOSIS_SEXUAL_TRAUMA',\n",
    "    'DIAGNOSIS_SUICIDALITY', 'age',\n",
    "    'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"Bipolar_and_Mood_disorder\", \"ethnicity_Dutch\", \"ethnicity_other\"\n",
    "]\n",
    "\n",
    "\n",
    "covariates_SubSubCat_Paracetamol = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline',\n",
    "    'CAT_Antidepressiva',\n",
    "    'CAT_Antipsychotica',\n",
    "    'CAT_Benzodiazepine',\n",
    "    'DIAGNOSIS_CHILDHOOD_TRAUMA', 'DIAGNOSIS_CPTSD', 'DIAGNOSIS_SEXUAL_TRAUMA',\n",
    "    'DIAGNOSIS_SUICIDALITY', 'age',\n",
    "    'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"Bipolar_and_Mood_disorder\", \"ethnicity_Dutch\", \"ethnicity_other\"\n",
    "]\n",
    "\n",
    "covariates_SubSubCat_Lorazepam = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline',\n",
    "    'CAT_Antidepressiva', 'CAT_Antipsychotica',\n",
    "    'DIAGNOSIS_CHILDHOOD_TRAUMA', 'DIAGNOSIS_CPTSD', 'DIAGNOSIS_SEXUAL_TRAUMA',\n",
    "    'DIAGNOSIS_SUICIDALITY', 'age',\n",
    "    'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"Bipolar_and_Mood_disorder\", \"ethnicity_Dutch\", \"ethnicity_other\"\n",
    "]\n",
    "\n",
    "covariates_SubSubCat_Mirtazapine = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline',\n",
    "    'CAT_Antipsychotica',\n",
    "    'CAT_Benzodiazepine',\n",
    "    'DIAGNOSIS_CHILDHOOD_TRAUMA', 'DIAGNOSIS_CPTSD', 'DIAGNOSIS_SEXUAL_TRAUMA',\n",
    "    'DIAGNOSIS_SUICIDALITY', 'age',\n",
    "    'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"Bipolar_and_Mood_disorder\", \"ethnicity_Dutch\", \"ethnicity_other\"\n",
    "]\n",
    "\n",
    "covariates_SubSubCat_Escitalopram = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline',\n",
    "    'CAT_Antipsychotica',\n",
    "    'CAT_Benzodiazepine',\n",
    "    'DIAGNOSIS_CHILDHOOD_TRAUMA', 'DIAGNOSIS_CPTSD', 'DIAGNOSIS_SEXUAL_TRAUMA',\n",
    "    'DIAGNOSIS_SUICIDALITY', 'age',\n",
    "    'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"Bipolar_and_Mood_disorder\", \"ethnicity_Dutch\", \"ethnicity_other\"\n",
    "]\n",
    "\n",
    "\n",
    "covariates_SubSubCat_Sertraline = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline',\n",
    "    'CAT_Antipsychotica',\n",
    "    'CAT_Benzodiazepine',\n",
    "    'DIAGNOSIS_CHILDHOOD_TRAUMA', 'DIAGNOSIS_CPTSD', 'DIAGNOSIS_SEXUAL_TRAUMA',\n",
    "    'DIAGNOSIS_SUICIDALITY', 'age',\n",
    "    'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"Bipolar_and_Mood_disorder\", \"ethnicity_Dutch\", \"ethnicity_other\"\n",
    "]\n",
    "\n",
    "covariates_SubSubCat_Temazepam = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline',\n",
    "    'CAT_Antidepressiva', 'CAT_Antipsychotica',\n",
    "    'DIAGNOSIS_CHILDHOOD_TRAUMA', 'DIAGNOSIS_CPTSD', 'DIAGNOSIS_SEXUAL_TRAUMA',\n",
    "    'DIAGNOSIS_SUICIDALITY', 'age',\n",
    "    'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"Bipolar_and_Mood_disorder\", \"ethnicity_Dutch\", \"ethnicity_other\"\n",
    "]\n",
    "\n",
    "covariates_SubSubCat_Citalopram = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline',\n",
    "    'CAT_Antipsychotica',\n",
    "    'CAT_Benzodiazepine',\n",
    "    'DIAGNOSIS_CHILDHOOD_TRAUMA', 'DIAGNOSIS_CPTSD', 'DIAGNOSIS_SEXUAL_TRAUMA',\n",
    "    'DIAGNOSIS_SUICIDALITY', 'age',\n",
    "    'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"Bipolar_and_Mood_disorder\", \"ethnicity_Dutch\", \"ethnicity_other\"\n",
    "]\n",
    "\n",
    "covariates_SubSubCat_Quetiapine = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline',\n",
    "    'CAT_Antidepressiva',\n",
    "    'CAT_Benzodiazepine',\n",
    "    'DIAGNOSIS_CHILDHOOD_TRAUMA', 'DIAGNOSIS_CPTSD', 'DIAGNOSIS_SEXUAL_TRAUMA',\n",
    "    'DIAGNOSIS_SUICIDALITY', 'age',\n",
    "    'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"Bipolar_and_Mood_disorder\", \"ethnicity_Dutch\", \"ethnicity_other\"\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "covariates_SubSubCat_Amitriptyline = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline',\n",
    "    'CAT_Antipsychotica',\n",
    "    'CAT_Benzodiazepine',\n",
    "    'DIAGNOSIS_CHILDHOOD_TRAUMA', 'DIAGNOSIS_CPTSD', 'DIAGNOSIS_SEXUAL_TRAUMA',\n",
    "    'DIAGNOSIS_SUICIDALITY', 'age',\n",
    "    'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"Bipolar_and_Mood_disorder\", \"ethnicity_Dutch\", \"ethnicity_other\"\n",
    "]\n",
    "\n",
    "\n",
    "covariates_SubSubCat_Venlafaxine = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline',\n",
    "    'CAT_Antipsychotica',\n",
    "    'CAT_Benzodiazepine',\n",
    "    'DIAGNOSIS_CHILDHOOD_TRAUMA', 'DIAGNOSIS_CPTSD', 'DIAGNOSIS_SEXUAL_TRAUMA',\n",
    "    'DIAGNOSIS_SUICIDALITY', 'age',\n",
    "    'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"Bipolar_and_Mood_disorder\", \"ethnicity_Dutch\", \"ethnicity_other\"\n",
    "]\n",
    "\n",
    "covariates_SubSubCat_Fluoxetine = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline',\n",
    "    'CAT_Antipsychotica',\n",
    "    'CAT_Benzodiazepine',\n",
    "    'DIAGNOSIS_CHILDHOOD_TRAUMA', 'DIAGNOSIS_CPTSD', 'DIAGNOSIS_SEXUAL_TRAUMA',\n",
    "    'DIAGNOSIS_SUICIDALITY', 'age',\n",
    "    'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"Bipolar_and_Mood_disorder\", \"ethnicity_Dutch\", \"ethnicity_other\"\n",
    "]\n",
    "\n",
    "covariates_SubSubCat_Topiramaat = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline',\n",
    "    'CAT_Antidepressiva',\n",
    "    'CAT_Antipsychotica',\n",
    "    'CAT_Benzodiazepine',\n",
    "    'DIAGNOSIS_CHILDHOOD_TRAUMA', 'DIAGNOSIS_CPTSD', 'DIAGNOSIS_SEXUAL_TRAUMA',\n",
    "    'DIAGNOSIS_SUICIDALITY', 'age',\n",
    "    'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"Bipolar_and_Mood_disorder\", \"ethnicity_Dutch\", \"ethnicity_other\"\n",
    "]\n",
    "\n",
    "covariates_SubSubCat_Tramadol = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline',\n",
    "    'CAT_Antidepressiva', 'CAT_Antipsychotica', 'CAT_Benzodiazepine',\n",
    "    'DIAGNOSIS_CHILDHOOD_TRAUMA', 'DIAGNOSIS_CPTSD', 'DIAGNOSIS_SEXUAL_TRAUMA',\n",
    "    'DIAGNOSIS_SUICIDALITY', 'age',\n",
    "    'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"Bipolar_and_Mood_disorder\", \"ethnicity_Dutch\", \"ethnicity_other\"\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "covariates_SubSubCat_Zopiclon = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline',\n",
    "    'CAT_Antidepressiva',\n",
    "    'CAT_Antipsychotica',\n",
    "    'CAT_Benzodiazepine',\n",
    "    'DIAGNOSIS_CHILDHOOD_TRAUMA', 'DIAGNOSIS_CPTSD', 'DIAGNOSIS_SEXUAL_TRAUMA',\n",
    "    'DIAGNOSIS_SUICIDALITY', 'age',\n",
    "    'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"Bipolar_and_Mood_disorder\", \"ethnicity_Dutch\", \"ethnicity_other\"\n",
    "]\n",
    "\n",
    "covariates_SubSubCat_Loprazolam = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline',\n",
    "    'CAT_Antidepressiva', 'CAT_Antipsychotica',\n",
    "    'DIAGNOSIS_CHILDHOOD_TRAUMA', 'DIAGNOSIS_CPTSD', 'DIAGNOSIS_SEXUAL_TRAUMA',\n",
    "    'DIAGNOSIS_SUICIDALITY', 'age',\n",
    "    'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"Bipolar_and_Mood_disorder\", \"ethnicity_Dutch\", \"ethnicity_other\"\n",
    "]\n",
    "\n",
    "covariates_SubSubCat_Alprazolam = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline',\n",
    "    'CAT_Antidepressiva', 'CAT_Antipsychotica',\n",
    "    'DIAGNOSIS_CHILDHOOD_TRAUMA', 'DIAGNOSIS_CPTSD', 'DIAGNOSIS_SEXUAL_TRAUMA',\n",
    "    'DIAGNOSIS_SUICIDALITY', 'age',\n",
    "    'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"Bipolar_and_Mood_disorder\", \"ethnicity_Dutch\", \"ethnicity_other\"\n",
    "]\n",
    "\n",
    "covariates_SubSubCat_promethazine = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline',\n",
    "    'CAT_Antidepressiva', 'CAT_Antipsychotica',\n",
    "    'CAT_Benzodiazepine',\n",
    "    'DIAGNOSIS_CHILDHOOD_TRAUMA', 'DIAGNOSIS_CPTSD', 'DIAGNOSIS_SEXUAL_TRAUMA',\n",
    "    'DIAGNOSIS_SUICIDALITY', 'age',\n",
    "    'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"Bipolar_and_Mood_disorder\", \"ethnicity_Dutch\", \"ethnicity_other\"\n",
    "]\n",
    "\n",
    "covariates_SubSubCat_Paroxetine = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline',\n",
    "    'CAT_Antipsychotica',\n",
    "    'CAT_Benzodiazepine',\n",
    "    'DIAGNOSIS_CHILDHOOD_TRAUMA', 'DIAGNOSIS_CPTSD', 'DIAGNOSIS_SEXUAL_TRAUMA',\n",
    "    'DIAGNOSIS_SUICIDALITY', 'age',\n",
    "    'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"Bipolar_and_Mood_disorder\", \"ethnicity_Dutch\", \"ethnicity_other\"\n",
    "]\n",
    "\n",
    "\n",
    "covariates_SubSubCat_Bupropion = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline',\n",
    "    'CAT_Antipsychotica',\n",
    "    'CAT_Benzodiazepine',\n",
    "    'DIAGNOSIS_CHILDHOOD_TRAUMA', 'DIAGNOSIS_CPTSD', 'DIAGNOSIS_SEXUAL_TRAUMA',\n",
    "    'DIAGNOSIS_SUICIDALITY', 'age',\n",
    "    'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"Bipolar_and_Mood_disorder\", \"ethnicity_Dutch\", \"ethnicity_other\"\n",
    "]\n",
    "\n",
    "covariates_SubSubCat_Methylfenidaat = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline',\n",
    "    'CAT_Antidepressiva',\n",
    "    'CAT_Antipsychotica',\n",
    "    'CAT_Benzodiazepine',\n",
    "    'DIAGNOSIS_CHILDHOOD_TRAUMA', 'DIAGNOSIS_CPTSD',\n",
    "    'DIAGNOSIS_SEXUAL_TRAUMA', 'DIAGNOSIS_SUICIDALITY',\n",
    "    'age',\n",
    "    'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"Bipolar_and_Mood_disorder\", \"ethnicity_Dutch\", \"ethnicity_other\"\n",
    "]\n",
    "\n",
    "covariates_SubSubCat_Olanzapine = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline',\n",
    "    'CAT_Antidepressiva',\n",
    "    'CAT_Benzodiazepine',\n",
    "    'DIAGNOSIS_CHILDHOOD_TRAUMA', 'DIAGNOSIS_CPTSD', 'DIAGNOSIS_SEXUAL_TRAUMA',\n",
    "    'DIAGNOSIS_SUICIDALITY', 'age',\n",
    "    'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"Bipolar_and_Mood_disorder\", \"ethnicity_Dutch\", \"ethnicity_other\"\n",
    "]\n",
    "\n",
    "\n",
    "covariates_SubSubCat_Zolpidem = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline',\n",
    "    'CAT_Antidepressiva', 'CAT_Antipsychotica',\n",
    "    'CAT_Benzodiazepine',\n",
    "    'DIAGNOSIS_CHILDHOOD_TRAUMA', 'DIAGNOSIS_CPTSD', 'DIAGNOSIS_SEXUAL_TRAUMA',\n",
    "    'DIAGNOSIS_SUICIDALITY', 'age',\n",
    "    'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"Bipolar_and_Mood_disorder\", \"ethnicity_Dutch\", \"ethnicity_other\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33230141-1b00-460e-b113-f27cd67a2e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# This finds all variables that start with covariates_SUbSubCAT_ or covariates_SubSubcat_\n",
    "final_covariates_map = defaultdict(list)\n",
    "final_covariates_map.update({\n",
    "    var.replace(\"covariates_\", \"\"): val\n",
    "    for var, val in globals().items()\n",
    "    if var.lower().startswith(\"covariates_subsubcat_\") and isinstance(val, list)\n",
    "})\n",
    "\n",
    "# Show detected group names\n",
    "print(\"Groups found:\", list(final_covariates_map.keys()))\n",
    "medication_groups = list(final_covariates_map.keys())\n",
    "print(medication_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ef235f-c23f-4f2c-a4ba-4be21721cb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "def run_all_SUBSUBCAT_group_models(imputed_dfs):\n",
    "    \"\"\"\n",
    "    Runs downstream analysis for each SUBSUBCAT medisubsubcation group using imputed datasets.\n",
    "\n",
    "    Parameters:\n",
    "    - imputed_dfs: list of 5 imputed DataFrames (from df_imputed_final_imp1.pkl ... imp5.pkl)\n",
    "    \n",
    "    Notes:\n",
    "    - Covariate lists must be defined as global variables: covariates_subsubcat_<group>\n",
    "    - Outputs are saved in: outputs/SUBSUBCAT_<GROUP>/\n",
    "    \"\"\"\n",
    "\n",
    "    print(\" Starting analysis for all SUBSUBCAT groups\")\n",
    "\n",
    "    for var_name in globals():\n",
    "        if var_name.lower().startswith(\"covariates_subsubcat_\") and isinstance(globals()[var_name], list):\n",
    "            group_name = var_name.replace(\"covariates_\", \"\")\n",
    "            group_name = group_name.replace(\"_\", \" \").title().replace(\" \", \"_\")  # e.g., subsubcat_z_drugs → Subsubcat_Z_Drugs\n",
    "            group_name = group_name.replace(\"Subsubcat_\", \"SUBSUBCAT_\")  # force prefix to uppercase\n",
    "\n",
    "            covariates = globals()[var_name]\n",
    "            output_dir = f\"outputs/{group_name}\"\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "            print(f\"\\n Processing {group_name}...\")\n",
    "\n",
    "            for k, df_imp in enumerate(imputed_dfs):\n",
    "                print(f\"  → Using imputation {k+1}\")\n",
    "\n",
    "                # Define X and Y\n",
    "                X = df_imp[covariates]\n",
    "                Y = df_imp[\"caps5_change_baseline\"]\n",
    "\n",
    "                # === Save X and Y as placeholder (replace with modeling later)\n",
    "                X.to_csv(f\"{output_dir}/X_imp{k+1}.csv\", index=False)\n",
    "                Y.to_frame(name=\"Y\").to_csv(f\"{output_dir}/Y_imp{k+1}.csv\", index=False)\n",
    "\n",
    "            print(f\" Done: {group_name}\")\n",
    "\n",
    "    print(\"\\n All SUBSUBCAT group analyses complete.\")\n",
    "\n",
    "# ========= STEP 4: Execute ========= #\n",
    "run_all_SUBSUBCAT_group_models(imputed_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e790f5-0421-408b-b159-ea2b11e06d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "for treatment_var in medication_groups:\n",
    "    print(f\"\\n {treatment_var}\")\n",
    "    \n",
    "    for i, df in enumerate(imputed_dfs):\n",
    "        if treatment_var not in df.columns:\n",
    "            print(f\"  Imp {i+1}:  Not found in columns.\")\n",
    "            continue\n",
    "\n",
    "        treated = (df[treatment_var] == 1).sum()\n",
    "        control = (df[treatment_var] == 0).sum()\n",
    "        missing = df[treatment_var].isna().sum()\n",
    "\n",
    "        print(f\"  Imp {i+1}: Treated = {treated}, Control = {control}, Missing = {missing}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07169c03-e41b-408d-b805-a8205c103695",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from collections import defaultdict\n",
    "\n",
    "# ✅ VIF computation function\n",
    "def compute_vif(X):\n",
    "    X = sm.add_constant(X, has_constant='add')\n",
    "    vif_df = pd.DataFrame()\n",
    "    vif_df[\"variable\"] = X.columns\n",
    "    vif_df[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "    return vif_df\n",
    "\n",
    "# ✅ Process each group\n",
    "for group in medication_groups:\n",
    "    print(f\"\\n🔍 Processing VIF for {group}\")\n",
    "\n",
    "    if group not in final_covariates_map:\n",
    "        print(f\" ⚠️ No covariates found for {group}. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    covariates = final_covariates_map[group]\n",
    "    vif_list = []\n",
    "\n",
    "    for i, df_imp in enumerate(imputed_dfs):\n",
    "        try:\n",
    "            X = df_imp[covariates].copy()\n",
    "            vif_df = compute_vif(X)\n",
    "            vif_df[\"imputation\"] = i + 1\n",
    "            vif_list.append(vif_df)\n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ Failed on imputation {i+1} for {group}: {e}\")\n",
    "\n",
    "    if vif_list:\n",
    "        all_vif = pd.concat(vif_list)\n",
    "        pooled_vif = all_vif.groupby(\"variable\")[\"VIF\"].mean().reset_index()\n",
    "        pooled_vif = pooled_vif.sort_values(by=\"VIF\", ascending=False)\n",
    "\n",
    "        output_folder = os.path.join(\"outputs\", group)\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "        pooled_vif.to_csv(os.path.join(output_folder, \"pooled_vif.csv\"), index=False)\n",
    "\n",
    "        print(f\" ✅ Saved: {output_folder}/pooled_vif.csv\")\n",
    "    else:\n",
    "        print(f\" ⚠️ Skipped {group}: No valid imputations.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926d616b-8015-4b96-92be-8ab6124676ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ---------- PS Estimation Function ----------\n",
    "def run_xgboost_ps_modeling(imputed_dfs, medication_groups, final_covariates_map):\n",
    "    for group in medication_groups:\n",
    "        print(f\" Running PS estimation for {group}\")\n",
    "\n",
    "        if group not in final_covariates_map:\n",
    "            print(f\" No covariates found for {group}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        output_folder = os.path.join(\"outputs\", group)\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "        covariates = final_covariates_map[group]\n",
    "        ps_matrix = pd.DataFrame()\n",
    "        auc_list = []\n",
    "\n",
    "        for i, df_imp in enumerate(imputed_dfs):\n",
    "            if group not in df_imp.columns:\n",
    "                print(f\" {group} not found in imputed dataset {i+1}. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            X = df_imp[covariates].copy()\n",
    "            T = df_imp[group]\n",
    "\n",
    "            # Drop missing treatment rows\n",
    "            valid_idx = T.dropna().index\n",
    "            X = X.loc[valid_idx]\n",
    "            T = T.loc[valid_idx]\n",
    "\n",
    "            # Train-test split for ROC\n",
    "            X_train, X_test, T_train, T_test = train_test_split(\n",
    "                X, T, stratify=T, test_size=0.3, random_state=42\n",
    "            )\n",
    "\n",
    "            try:\n",
    "                model = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "                model.fit(X_train, T_train)\n",
    "\n",
    "                ps_scores = model.predict_proba(X)[:, 1]\n",
    "                ps_matrix[f\"ps_imp{i+1}\"] = pd.Series(ps_scores, index=valid_idx)\n",
    "\n",
    "                # ROC & AUC\n",
    "                auc = roc_auc_score(T_test, model.predict_proba(X_test)[:, 1])\n",
    "                auc_list.append(auc)\n",
    "\n",
    "                fpr, tpr, _ = roc_curve(T_test, model.predict_proba(X_test)[:, 1])\n",
    "                plt.figure()\n",
    "                plt.plot(fpr, tpr, label=f\"AUC = {auc:.3f}\")\n",
    "                plt.plot([0, 1], [0, 1], 'k--')\n",
    "                plt.xlabel(\"False Positive Rate\")\n",
    "                plt.ylabel(\"True Positive Rate\")\n",
    "                plt.title(f\"ROC Curve - {group} (Imp {i+1})\")\n",
    "                plt.legend()\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(os.path.join(output_folder, f\"roc_curve_imp{i+1}.png\"))\n",
    "                plt.close()\n",
    "                print(f\"   Imp {i+1}: AUC = {auc:.3f}, ROC saved.\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"   Error in {group} (imp {i+1}): {e}\")\n",
    "\n",
    "        # Save AUCs and Composite PS\n",
    "        if not ps_matrix.empty:\n",
    "            # Fill NaN rows (from dropped subjects in some imputations) with mean\n",
    "            ps_matrix[\"composite_ps\"] = ps_matrix.mean(axis=1)\n",
    "            ps_matrix.to_excel(os.path.join(output_folder, \"propensity_scores.xlsx\"))\n",
    "\n",
    "            auc_df = pd.DataFrame({\n",
    "                \"imputation\": [f\"imp{i+1}\" for i in range(len(auc_list))],\n",
    "                \"AUC\": auc_list\n",
    "            })\n",
    "            auc_df.loc[len(auc_df.index)] = [\"mean\", np.mean(auc_list) if auc_list else np.nan]\n",
    "            auc_df.to_excel(os.path.join(output_folder, \"auc_scores.xlsx\"), index=False)\n",
    "\n",
    "            print(f\" Composite PS + AUC saved for {group}\")\n",
    "        else:\n",
    "            print(f\" No valid PS scores generated for {group}\")\n",
    "\n",
    "# ---------- Run ----------\n",
    "run_xgboost_ps_modeling(imputed_dfs, medication_groups, final_covariates_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e226945e-7d0a-45c7-b5bc-af3feda0e42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "def compute_feature_importance(imputed_dfs, medication_groups, final_covariates_map):\n",
    "    for group in medication_groups:\n",
    "        print(f\"\\n Computing feature importance for {group}\")\n",
    "\n",
    "        if group not in final_covariates_map:\n",
    "            print(f\" No covariates found for {group}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        covariates = final_covariates_map[group]\n",
    "        output_folder = os.path.join(\"outputs\", group)\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "        importance_df_list = []\n",
    "\n",
    "        for i, df_imp in enumerate(imputed_dfs):\n",
    "            if group not in df_imp.columns:\n",
    "                print(f\" {group} not in imputed dataset {i+1}. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            X = df_imp[covariates].copy()\n",
    "            T = df_imp[group]\n",
    "\n",
    "            # Drop NaNs in treatment\n",
    "            valid_idx = T.dropna().index\n",
    "            X = X.loc[valid_idx]\n",
    "            T = T.loc[valid_idx]\n",
    "\n",
    "            try:\n",
    "                model = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "                model.fit(X, T)\n",
    "\n",
    "                # Get feature importance\n",
    "                importances = model.get_booster().get_score(importance_type='gain')\n",
    "                df_feat = pd.DataFrame.from_dict(importances, orient='index', columns=[f\"imp{i+1}\"])\n",
    "                df_feat.index.name = 'feature'\n",
    "                importance_df_list.append(df_feat)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"   Error during modeling: {e}\")\n",
    "\n",
    "        if importance_df_list:\n",
    "            # Combine and average\n",
    "            all_feat = pd.concat(importance_df_list, axis=1).fillna(0)\n",
    "            all_feat[\"mean_importance\"] = all_feat.mean(axis=1)\n",
    "\n",
    "            # Filter top 30 non-zero\n",
    "            non_zero = all_feat[all_feat[\"mean_importance\"] > 0]\n",
    "            top30 = non_zero.sort_values(by=\"mean_importance\", ascending=False).head(30)\n",
    "\n",
    "            # Save to CSV\n",
    "            top30.to_csv(os.path.join(output_folder, \"feature_importance.csv\"))\n",
    "\n",
    "            # Plot\n",
    "            plt.figure(figsize=(10, 8))\n",
    "            plt.barh(top30.index[::-1], top30[\"mean_importance\"][::-1])  # plot top → bottom\n",
    "            plt.xlabel(\"Mean Gain Importance\")\n",
    "            plt.title(f\"Top 30 Feature Importance - {group}\")\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(output_folder, \"feature_importance_top30.png\"))\n",
    "            plt.close()\n",
    "\n",
    "            print(f\" Saved feature importance plot and CSV for {group}\")\n",
    "        else:\n",
    "            print(f\" No valid models for {group}\")\n",
    "\n",
    "#  Run\n",
    "compute_feature_importance(imputed_dfs, medication_groups, final_covariates_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccef9e49-f14c-4092-b010-cfcc18a50be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def compute_trimmed_clipped_iptw(ps_df, treatment, lower=0.05, upper=0.95, clip_max=10):\n",
    "    weights = []\n",
    "    keep_mask = (ps_df > lower) & (ps_df < upper)\n",
    "\n",
    "    for i in range(ps_df.shape[1]):\n",
    "        ps = ps_df.iloc[:, i].clip(lower=1e-6, upper=1 - 1e-6)  # avoid div by zero\n",
    "        mask = keep_mask.iloc[:, i]\n",
    "        w = pd.Series(np.nan, index=ps.index)\n",
    "\n",
    "        w[mask & (treatment == 1)] = 1 / ps[mask & (treatment == 1)]\n",
    "        w[mask & (treatment == 0)] = 1 / (1 - ps[mask & (treatment == 0)])\n",
    "        w = w.clip(upper=clip_max)\n",
    "        weights.append(w)\n",
    "\n",
    "    return pd.concat(weights, axis=1)\n",
    "\n",
    "\n",
    "def apply_rubins_rule_to_iptw(iptw_matrix):\n",
    "    \"\"\"\n",
    "    Given an IPTW matrix (n rows × M imputations), return Rubin’s rule pooled mean, SD, SE.\n",
    "    \"\"\"\n",
    "    M = iptw_matrix.shape[1]\n",
    "    q_bar = iptw_matrix.mean(axis=1)\n",
    "    u_bar = iptw_matrix.var(axis=1, ddof=1)\n",
    "    B = iptw_matrix.apply(lambda x: x.mean(), axis=1).var(ddof=1)\n",
    "    total_var = u_bar + (1 + 1/M) * B\n",
    "    total_se = np.sqrt(total_var)\n",
    "    return q_bar, u_bar.pow(0.5), total_se\n",
    "\n",
    "\n",
    "def run_trim_clip_save_all(imputed_dfs, medication_groups, final_covariates_map):\n",
    "    for group in medication_groups:\n",
    "        print(f\"\\n🔍 Processing IPTW + trimming + clipping for {group}\")\n",
    "        output_folder = os.path.join(\"outputs\", group)\n",
    "        ps_path = os.path.join(output_folder, \"propensity_scores.xlsx\")\n",
    "\n",
    "        if not os.path.exists(ps_path):\n",
    "            print(f\"⚠️ Missing PS file: {ps_path}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            ps_all = pd.read_excel(ps_path, index_col=0)\n",
    "            ps_cols = [col for col in ps_all.columns if col.startswith(\"ps_imp\")]\n",
    "            composite_index = ps_all.index\n",
    "\n",
    "            # Get treatment from one imputed dataset\n",
    "            T_full = None\n",
    "            for df in imputed_dfs:\n",
    "                if group in df.columns:\n",
    "                    T_full = df.loc[composite_index, group]\n",
    "                    break\n",
    "\n",
    "            if T_full is None:\n",
    "                print(f\"❌ Treatment column {group} not found in any imputed dataset.\")\n",
    "                continue\n",
    "\n",
    "            # Compute IPTW matrix (shape: n × M)\n",
    "            iptw_matrix = compute_trimmed_clipped_iptw(ps_all[ps_cols], T_full)\n",
    "            iptw_matrix.columns = [f\"iptw_imp{i+1}\" for i in range(iptw_matrix.shape[1])]\n",
    "\n",
    "            # Apply Rubin’s Rule for mean, SD, SE\n",
    "            iptw_matrix[\"iptw_mean\"], iptw_matrix[\"iptw_sd\"], iptw_matrix[\"iptw_se\"] = apply_rubins_rule_to_iptw(\n",
    "                iptw_matrix[[f\"iptw_imp{i+1}\" for i in range(iptw_matrix.shape[1])]]\n",
    "            )\n",
    "\n",
    "            # Save IPTW matrix separately\n",
    "            iptw_matrix.to_excel(os.path.join(output_folder, \"iptw_weights.xlsx\"))\n",
    "            print(f\"✅ Saved IPTW weights for {group}\")\n",
    "\n",
    "            # Save trimmed & clipped imputed datasets with IPTW\n",
    "            for i in range(5):\n",
    "                df = imputed_dfs[i].copy()\n",
    "                if group not in df.columns:\n",
    "                    continue\n",
    "\n",
    "                trimmed_idx = iptw_matrix.index.intersection(df.index)\n",
    "                needed_cols = final_covariates_map[group] + [group, \"caps5_change_baseline\"]\n",
    "\n",
    "                # Select only necessary columns\n",
    "                df_trimmed = df.loc[trimmed_idx, needed_cols].copy()\n",
    "                df_trimmed[\"iptw\"] = iptw_matrix[f\"iptw_imp{i+1}\"].loc[trimmed_idx]\n",
    "\n",
    "                # ✅ DROP rows with missing IPTW values\n",
    "                before = len(df_trimmed)\n",
    "                df_trimmed = df_trimmed.dropna(subset=[\"iptw\"])\n",
    "                after = len(df_trimmed)\n",
    "                print(f\"    ℹ️ Retained {after}/{before} rows after IPTW NaN drop.\")\n",
    "\n",
    "                # Save to .pkl\n",
    "                df_trimmed.to_pickle(os.path.join(output_folder, f\"trimmed_data_imp{i+1}.pkl\"))\n",
    "                print(f\"  💾 Saved trimmed dataset: {output_folder}/trimmed_data_imp{i+1}.*\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error in {group}: {e}\")\n",
    "\n",
    "\n",
    "run_trim_clip_save_all(imputed_dfs, medication_groups, final_covariates_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4ab4ce-3b0b-4fa9-bd99-204c2a8c8d47",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_ps_overlap_all_groups(medication_groups):\n",
    "    for group in medication_groups:\n",
    "        print(f\"\\n📊 Plotting PS overlap for {group}\")\n",
    "\n",
    "        folder = os.path.join(\"outputs\", group)\n",
    "        ps_file = os.path.join(folder, \"propensity_scores.xlsx\")\n",
    "        iptw_file = os.path.join(folder, \"iptw_weights.xlsx\")\n",
    "        trimmed_file = os.path.join(folder, \"trimmed_data_imp1.pkl\")\n",
    "\n",
    "        if not all(os.path.exists(f) for f in [ps_file, iptw_file, trimmed_file]):\n",
    "            print(f\"⚠️ Missing required files for {group}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            ps_df = pd.read_excel(ps_file, index_col=0)\n",
    "            iptw_df = pd.read_excel(iptw_file, index_col=0)\n",
    "            trimmed_df = pd.read_pickle(trimmed_file)\n",
    "\n",
    "            # Extract\n",
    "            ps = ps_df[\"composite_ps\"].reindex(trimmed_df.index)\n",
    "            w = iptw_df[\"iptw_mean\"].reindex(trimmed_df.index)\n",
    "            T = trimmed_df[group]\n",
    "\n",
    "            # Masks to remove NaNs\n",
    "            treated_mask = (T == 1) & ps.notna() & w.notna()\n",
    "            control_mask = (T == 0) & ps.notna() & w.notna()\n",
    "\n",
    "            treated = ps[treated_mask]\n",
    "            treated_w = w[treated_mask]\n",
    "\n",
    "            control = ps[control_mask]\n",
    "            control_w = w[control_mask]\n",
    "\n",
    "            # === Unweighted Plot ===\n",
    "            plt.figure(figsize=(8, 5))\n",
    "            plt.hist([treated, control], bins=25, label=[\"Treated\", \"Control\"], alpha=0.6)\n",
    "            plt.title(f\"Unweighted PS Overlap - {group}\")\n",
    "            plt.xlabel(\"Composite Propensity Score\")\n",
    "            plt.ylabel(\"Count\")\n",
    "            plt.legend()\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(folder, \"ps_overlap_unweighted.png\"))\n",
    "            plt.close()\n",
    "\n",
    "            # === Weighted Plot ===\n",
    "            plt.figure(figsize=(8, 5))\n",
    "            plt.hist([treated, control], bins=25, weights=[treated_w, control_w], label=[\"Treated\", \"Control\"], alpha=0.6)\n",
    "            plt.title(f\"Weighted PS Overlap - {group}\")\n",
    "            plt.xlabel(\"Composite Propensity Score\")\n",
    "            plt.ylabel(\"Weighted Count\")\n",
    "            plt.legend()\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(folder, \"ps_overlap_weighted.png\"))\n",
    "            plt.close()\n",
    "\n",
    "            print(f\"✅ Saved unweighted and weighted PS plots for {group}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error processing {group}: {e}\")\n",
    "\n",
    "# 🔁 Run\n",
    "plot_ps_overlap_all_groups(medication_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641bfc1a-fb30-4961-917d-ebe4ebcd2912",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set up base output folder\n",
    "output_base = \"outputs\"\n",
    "ps_file = \"propensity_scores.xlsx\"\n",
    "iptw_file = \"iptw_weights.xlsx\"\n",
    "trimmed_data_file = \"trimmed_data_imp1.pkl\"\n",
    "\n",
    "# Collect all treatment group folders\n",
    "groups = [g for g in medication_groups if os.path.isdir(os.path.join(output_base, g))]\n",
    "\n",
    "# Generate 4-panel overlap plots\n",
    "for group in groups:\n",
    "    group_path = os.path.join(output_base, group)\n",
    "    try:\n",
    "        # Load trimmed treatment info\n",
    "        trimmed_df = pd.read_pickle(os.path.join(group_path, trimmed_data_file))\n",
    "        index = trimmed_df.index\n",
    "\n",
    "        # Fix: case-insensitive match for treatment variable\n",
    "        possible_cols = [col for col in trimmed_df.columns if col.upper() == group.upper()]\n",
    "        if not possible_cols:\n",
    "            print(f\" Treatment variable {group} not found in {group}, skipping.\")\n",
    "            continue\n",
    "        treatment_var = possible_cols[0]\n",
    "        T = trimmed_df[treatment_var]\n",
    "\n",
    "        # Load composite PS (aligned to trimmed_df index)\n",
    "        ps_df = pd.read_excel(os.path.join(group_path, ps_file), index_col=0)\n",
    "        if 'composite_ps' not in ps_df.columns:\n",
    "            print(f\" Composite column missing in {ps_file}, skipping {group}.\")\n",
    "            continue\n",
    "        ps = ps_df.loc[index, 'composite_ps']\n",
    "\n",
    "        # Load IPTW weights (aligned to trimmed_df index)\n",
    "        weights_df = pd.read_excel(os.path.join(group_path, iptw_file), index_col=0)\n",
    "        if 'iptw_mean' not in weights_df.columns:\n",
    "            print(f\" IPTW weight column missing in {iptw_file}, skipping {group}.\")\n",
    "            continue\n",
    "        weights = weights_df.loc[index, 'iptw_mean']\n",
    "\n",
    "        # Prepare 4 datasets\n",
    "        raw_treated = ps[T == 1]\n",
    "        raw_control = ps[T == 0]\n",
    "        weighted_treated = (ps[T == 1], weights[T == 1])\n",
    "        weighted_control = (ps[T == 0], weights[T == 0])\n",
    "\n",
    "        # Create plot\n",
    "        fig, axs = plt.subplots(2, 2, figsize=(12, 8))\n",
    "        fig.suptitle(f\"Propensity Score Distribution - {group}\", fontsize=14)\n",
    "\n",
    "        axs[0, 0].hist(raw_treated, bins=20, alpha=0.7, color='blue')\n",
    "        axs[0, 0].set_title(\"Raw Treated\")\n",
    "\n",
    "        axs[0, 1].hist(raw_control, bins=20, alpha=0.7, color='green')\n",
    "        axs[0, 1].set_title(\"Raw Control\")\n",
    "\n",
    "        axs[1, 0].hist(weighted_treated[0], bins=20, weights=weighted_treated[1], alpha=0.7, color='blue')\n",
    "        axs[1, 0].set_title(\"Weighted Treated\")\n",
    "\n",
    "        axs[1, 1].hist(weighted_control[0], bins=20, weights=weighted_control[1], alpha=0.7, color='green')\n",
    "        axs[1, 1].set_title(\"Weighted Control\")\n",
    "\n",
    "        for ax in axs.flat:\n",
    "            ax.set_xlim(0, 1)\n",
    "            ax.set_xlabel(\"Propensity Score\")\n",
    "            ax.set_ylabel(\"Count\")\n",
    "\n",
    "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "\n",
    "        # Save figure\n",
    "        plot_path = os.path.join(group_path, f\"four_panel_overlap_{group}.png\")\n",
    "        plt.savefig(plot_path)\n",
    "        plt.close()\n",
    "        print(f\" ✅ Saved: {plot_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\" ❌ Error in {group}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09df311b-eec5-417c-971f-6469e9c3c56d",
   "metadata": {},
   "source": [
    "# ATT calculation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28309ec4-a8ac-4604-8db3-1d378cf68fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55852696-eebd-4277-a40c-626400ce90c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import KFold\n",
    "from scipy.stats import t, probplot\n",
    "import xgboost as xgb\n",
    "\n",
    "# -----------------------------\n",
    "# Configuration\n",
    "# -----------------------------\n",
    "n_repeats = 1\n",
    "seeds = list(range(1, 11))\n",
    "imputations = 5\n",
    "output_folder = \"outputs\"\n",
    "\n",
    "# -----------------------------\n",
    "# Diagnostic Plotting Function\n",
    "# -----------------------------\n",
    "def create_diagnostic_plots(residuals_data, fitted_data, group_name):\n",
    "    \"\"\"Create 4 diagnostic plots for model validation\"\"\"\n",
    "    plots_dir = os.path.join(output_folder, \"plots\")\n",
    "    os.makedirs(plots_dir, exist_ok=True)\n",
    "    \n",
    "    # Flatten the collected data\n",
    "    all_residuals = np.concatenate(residuals_data)\n",
    "    all_fitted = np.concatenate(fitted_data)\n",
    "    \n",
    "    # Create figure with 2x2 subplots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    fig.suptitle(f'Diagnostic Plots - {group_name}', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Residuals vs Fitted\n",
    "    axes[0,0].scatter(all_fitted, all_residuals, alpha=0.6, s=20)\n",
    "    axes[0,0].axhline(y=0, color='red', linestyle='--', alpha=0.8)\n",
    "    axes[0,0].set_xlabel('Fitted Values')\n",
    "    axes[0,0].set_ylabel('Residuals')\n",
    "    axes[0,0].set_title('Residuals vs Fitted')\n",
    "    axes[0,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. QQ Plot\n",
    "    probplot(all_residuals, dist=\"norm\", plot=axes[0,1])\n",
    "    axes[0,1].set_title('Q-Q Plot (Normal)')\n",
    "    axes[0,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Residual Histogram\n",
    "    axes[1,0].hist(all_residuals, bins=30, density=True, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    axes[1,0].set_xlabel('Residuals')\n",
    "    axes[1,0].set_ylabel('Density')\n",
    "    axes[1,0].set_title('Residual Distribution')\n",
    "    axes[1,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Scale-Location Plot\n",
    "    sqrt_abs_residuals = np.sqrt(np.abs(all_residuals))\n",
    "    axes[1,1].scatter(all_fitted, sqrt_abs_residuals, alpha=0.6, s=20)\n",
    "    axes[1,1].set_xlabel('Fitted Values')\n",
    "    axes[1,1].set_ylabel('√|Residuals|')\n",
    "    axes[1,1].set_title('Scale-Location Plot')\n",
    "    axes[1,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save plot\n",
    "    plot_filename = os.path.join(plots_dir, f'{group_name}.png')\n",
    "    plt.savefig(plot_filename, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"📊 Diagnostic plots saved for {group_name}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Rubin's Rule\n",
    "# -----------------------------\n",
    "def rubins_pool(estimates, ses):\n",
    "    m = len(estimates)\n",
    "    q_bar = np.mean(estimates)\n",
    "    u_bar = np.mean(np.square(ses))\n",
    "    b_m = np.var(estimates, ddof=1)\n",
    "    total_var = u_bar + ((1 + 1/m) * b_m)\n",
    "    total_se = np.sqrt(total_var)\n",
    "    ci_lower = q_bar - 1.96 * total_se\n",
    "    ci_upper = q_bar + 1.96 * total_se\n",
    "    p_value = 2 * (1 - t.cdf(np.abs(q_bar / total_se), df=m-1))\n",
    "    rounded_p = round(p_value, 5)\n",
    "    formatted_p = \"< 0.00001\" if rounded_p <= 0.00001 else f\"{rounded_p:.5f}\"\n",
    "    return q_bar, total_se, ci_lower, ci_upper, formatted_p\n",
    "\n",
    "# -----------------------------\n",
    "# SMD + Variance Ratio\n",
    "# -----------------------------\n",
    "def calculate_smd_vr(X, T, weights):\n",
    "    treated = X[T == 1]\n",
    "    control = X[T == 0]\n",
    "    w_treated = weights[T == 1]\n",
    "    w_control = weights[T == 0]\n",
    "    smd, vr = [], []\n",
    "    for col in X.columns:\n",
    "        try:\n",
    "            m1, m0 = np.average(treated[col], weights=w_treated), np.average(control[col], weights=w_control)\n",
    "            s1 = np.sqrt(np.average((treated[col] - m1) ** 2, weights=w_treated))\n",
    "            s0 = np.sqrt(np.average((control[col] - m0) ** 2, weights=w_control))\n",
    "            pooled_sd = np.sqrt((s1 ** 2 + s0 ** 2) / 2)\n",
    "            smd.append((m1 - m0) / pooled_sd if pooled_sd > 0 else 0)\n",
    "            vr.append(s1**2 / s0**2 if s0**2 > 0 else 0)\n",
    "        except Exception:\n",
    "            smd.append(np.nan)\n",
    "            vr.append(np.nan)\n",
    "    return np.nanmean(smd), np.nanmean(vr)\n",
    "\n",
    "# -----------------------------\n",
    "# XGBoost Main Loop (No DML)\n",
    "# -----------------------------\n",
    "def run_xgboost_with_trimmed_data(final_covariates_map):\n",
    "    att_results = []\n",
    "    balance_results = []\n",
    "\n",
    "    for group, covariates in final_covariates_map.items():\n",
    "        print(f\"\\n🚀 Running XGBoost for {group}\")\n",
    "        group_dir = os.path.join(output_folder, group)\n",
    "        os.makedirs(group_dir, exist_ok=True)\n",
    "\n",
    "        best_result = None\n",
    "        best_se = float(\"inf\")\n",
    "        \n",
    "        # Initialize lists to collect residuals and fitted values for diagnostic plots\n",
    "        group_residuals = []\n",
    "        group_fitted = []\n",
    "\n",
    "        for seed in seeds:\n",
    "            att_list, se_list, r2_list, rmse_list, smd_list, vr_list = [], [], [], [], [], []\n",
    "\n",
    "            for imp in range(1, imputations + 1):\n",
    "                file_path = os.path.join(group_dir, f\"trimmed_data_imp{imp}.pkl\")\n",
    "                if not os.path.exists(file_path):\n",
    "                    continue\n",
    "\n",
    "                df = pd.read_pickle(file_path)\n",
    "                if group not in df.columns or \"iptw\" not in df.columns:\n",
    "                    continue\n",
    "\n",
    "                X = df[covariates].copy()\n",
    "                T = df[group]\n",
    "                Y = df[\"caps5_change_baseline\"]\n",
    "                W = df[\"iptw\"]\n",
    "\n",
    "                for repeat in range(n_repeats):\n",
    "                    kf = KFold(n_splits=5, shuffle=True, random_state=seed + repeat)\n",
    "                    for train_idx, test_idx in kf.split(X):\n",
    "                        try:\n",
    "                            X_train, T_train, Y_train, W_train = (\n",
    "                                X.iloc[train_idx],\n",
    "                                T.iloc[train_idx],\n",
    "                                Y.iloc[train_idx],\n",
    "                                W.iloc[train_idx],\n",
    "                            )\n",
    "\n",
    "                            # XGBoost regression model\n",
    "                            model = xgb.XGBRegressor(n_estimators=100, max_depth=3, learning_rate=0.1, n_jobs=1, random_state=seed)\n",
    "                            \n",
    "                            # Add treatment variable to features\n",
    "                            X_train_with_T = X_train.copy()\n",
    "                            X_train_with_T[group] = T_train\n",
    "                            \n",
    "                            # Fit model\n",
    "                            model.fit(X_train_with_T, Y_train, sample_weight=W_train)\n",
    "                            \n",
    "                            # Predict outcomes for treated and control groups\n",
    "                            X_treated = X_train.copy()\n",
    "                            X_treated[group] = 1\n",
    "                            X_control = X_train.copy()\n",
    "                            X_control[group] = 0\n",
    "                            \n",
    "                            Y_pred_treated = model.predict(X_treated)\n",
    "                            Y_pred_control = model.predict(X_control)\n",
    "                            \n",
    "                            # Calculate ATT (Average Treatment Effect on Treated)\n",
    "                            treated_mask = T_train == 1\n",
    "                            if np.any(treated_mask):\n",
    "                                att = np.average(Y_pred_treated[treated_mask] - Y_pred_control[treated_mask], \n",
    "                                               weights=W_train[treated_mask])\n",
    "                                \n",
    "                                # Calculate standard error (approximate)\n",
    "                                treatment_effects = Y_pred_treated[treated_mask] - Y_pred_control[treated_mask]\n",
    "                                residual = treatment_effects - att\n",
    "                                se = np.sqrt(np.sum((W_train[treated_mask] * residual) ** 2)) / np.sum(W_train[treated_mask])\n",
    "\n",
    "                                \n",
    "                                att_list.append(att)\n",
    "                                se_list.append(se)\n",
    "\n",
    "                            # Model performance metrics\n",
    "                            Y_pred = model.predict(X_train_with_T)\n",
    "                            residuals = Y_train - Y_pred\n",
    "                            rmse = mean_squared_error(Y_train, Y_pred, squared=False)\n",
    "                            r2 = r2_score(Y_train, Y_pred)\n",
    "                            r2_list.append(r2)\n",
    "                            rmse_list.append(rmse)\n",
    "                            \n",
    "                            # Collect residuals and fitted values for diagnostic plots\n",
    "                            group_residuals.append(residuals.values)\n",
    "                            group_fitted.append(Y_pred)\n",
    "\n",
    "                            smd, vr = calculate_smd_vr(X_train, T_train, W_train)\n",
    "                            smd_list.append(smd)\n",
    "                            vr_list.append(vr)\n",
    "                        except Exception as e:\n",
    "                            print(f\"⚠️ Error in {group}, seed {seed}, imp {imp}, rep {repeat}: {e}\")\n",
    "\n",
    "            if att_list:\n",
    "                att, se, ci_l, ci_u, p_val = rubins_pool(att_list, se_list)\n",
    "                avg_r2 = np.mean(r2_list)\n",
    "                avg_rmse = np.mean(rmse_list)\n",
    "                avg_smd = np.mean(smd_list)\n",
    "                avg_vr = np.mean(vr_list)\n",
    "\n",
    "                balance_results.append({\n",
    "                    \"group\": group, \"seed\": seed, \"smd\": avg_smd, \"vr\": avg_vr\n",
    "                })\n",
    "\n",
    "                if se < best_se:\n",
    "                    best_se = se\n",
    "                    best_result = {\n",
    "                        \"group\": group, \"seed\": seed, \"att\": att, \"se\": se,\n",
    "                        \"ci_lower\": ci_l, \"ci_upper\": ci_u, \"p_value\": p_val,\n",
    "                        \"r2\": avg_r2, \"rmse\": avg_rmse\n",
    "                    }\n",
    "\n",
    "                print(f\"✅ {group} | Seed {seed}: ATT = {att:.4f}, SE = {se:.4f}, p = {p_val}\")\n",
    "            else:\n",
    "                print(f\"⚠️ No valid results for {group} | Seed {seed}\")\n",
    "\n",
    "        # Create diagnostic plots for this group\n",
    "        if group_residuals and group_fitted:\n",
    "            create_diagnostic_plots(group_residuals, group_fitted, group)\n",
    "\n",
    "        if best_result:\n",
    "            att_results.append(best_result)\n",
    "            print(f\"🏆 Best result for {group} → Seed {best_result['seed']} | SE = {best_result['se']:.4f}\")\n",
    "\n",
    "    # Save final output\n",
    "    pd.DataFrame(att_results).to_excel(\"xgb_rubin_summary_subsubcats.xlsx\", index=False)\n",
    "    pd.DataFrame(balance_results).to_excel(\"smd_vr_summary_subsubcats.xlsx\", index=False)\n",
    "    print(\"\\n🎯 All summary files saved.\")\n",
    "\n",
    "run_xgboost_with_trimmed_data(final_covariates_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc754ced-c08e-43a5-85c9-fed4eb54c55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unweighted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a560fef1-7417-4ef6-900a-9af1618ead6a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import KFold\n",
    "from scipy.stats import t, probplot\n",
    "import xgboost as xgb\n",
    "\n",
    "# -----------------------------\n",
    "# Configuration\n",
    "# -----------------------------\n",
    "n_repeats = 1\n",
    "seeds = list(range(1, 11))\n",
    "imputations = 5\n",
    "output_folder = \"outputs\"\n",
    "\n",
    "# -----------------------------\n",
    "# Diagnostic Plotting Function\n",
    "# -----------------------------\n",
    "def create_diagnostic_plots(residuals_data, fitted_data, group_name):\n",
    "    \"\"\"Create 4 diagnostic plots for model validation\"\"\"\n",
    "    plots_dir = os.path.join(output_folder, \"plots\")\n",
    "    os.makedirs(plots_dir, exist_ok=True)\n",
    "    \n",
    "    # Flatten the collected data\n",
    "    all_residuals = np.concatenate(residuals_data)\n",
    "    all_fitted = np.concatenate(fitted_data)\n",
    "    \n",
    "    # Create figure with 2x2 subplots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    fig.suptitle(f'Diagnostic Plots - {group_name}', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Residuals vs Fitted\n",
    "    axes[0,0].scatter(all_fitted, all_residuals, alpha=0.6, s=20)\n",
    "    axes[0,0].axhline(y=0, color='red', linestyle='--', alpha=0.8)\n",
    "    axes[0,0].set_xlabel('Fitted Values')\n",
    "    axes[0,0].set_ylabel('Residuals')\n",
    "    axes[0,0].set_title('Residuals vs Fitted')\n",
    "    axes[0,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. QQ Plot\n",
    "    probplot(all_residuals, dist=\"norm\", plot=axes[0,1])\n",
    "    axes[0,1].set_title('Q-Q Plot (Normal)')\n",
    "    axes[0,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Residual Histogram\n",
    "    axes[1,0].hist(all_residuals, bins=30, density=True, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    axes[1,0].set_xlabel('Residuals')\n",
    "    axes[1,0].set_ylabel('Density')\n",
    "    axes[1,0].set_title('Residual Distribution')\n",
    "    axes[1,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Scale-Location Plot\n",
    "    sqrt_abs_residuals = np.sqrt(np.abs(all_residuals))\n",
    "    axes[1,1].scatter(all_fitted, sqrt_abs_residuals, alpha=0.6, s=20)\n",
    "    axes[1,1].set_xlabel('Fitted Values')\n",
    "    axes[1,1].set_ylabel('√|Residuals|')\n",
    "    axes[1,1].set_title('Scale-Location Plot')\n",
    "    axes[1,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save plot\n",
    "    plot_filename = os.path.join(plots_dir, f'{group_name}_unweighted.png')\n",
    "    plt.savefig(plot_filename, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"📊 Diagnostic plots saved for {group_name}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Rubin's Rule\n",
    "# -----------------------------\n",
    "def rubins_pool(estimates, ses):\n",
    "    m = len(estimates)\n",
    "    q_bar = np.mean(estimates)\n",
    "    u_bar = np.mean(np.square(ses))\n",
    "    b_m = np.var(estimates, ddof=1)\n",
    "    total_var = u_bar + ((1 + 1/m) * b_m)\n",
    "    total_se = np.sqrt(total_var)\n",
    "    ci_lower = q_bar - 1.96 * total_se\n",
    "    ci_upper = q_bar + 1.96 * total_se\n",
    "    p_value = 2 * (1 - t.cdf(np.abs(q_bar / total_se), df=m-1))\n",
    "    rounded_p = round(p_value, 5)\n",
    "    formatted_p = \"< 0.00001\" if rounded_p <= 0.00001 else f\"{rounded_p:.5f}\"\n",
    "    return q_bar, total_se, ci_lower, ci_upper, formatted_p\n",
    "\n",
    "# -----------------------------\n",
    "# SMD + Variance Ratio\n",
    "# -----------------------------\n",
    "def calculate_smd_vr(X, T):\n",
    "    treated = X[T == 1]\n",
    "    control = X[T == 0]\n",
    "    smd, vr = [], []\n",
    "    for col in X.columns:\n",
    "        try:\n",
    "            m1, m0 = np.mean(treated[col]), np.mean(control[col])\n",
    "            s1 = np.std(treated[col])\n",
    "            s0 = np.std(control[col])\n",
    "            pooled_sd = np.sqrt((s1 ** 2 + s0 ** 2) / 2)\n",
    "            smd.append((m1 - m0) / pooled_sd if pooled_sd > 0 else 0)\n",
    "            vr.append(s1**2 / s0**2 if s0**2 > 0 else 0)\n",
    "        except Exception:\n",
    "            smd.append(np.nan)\n",
    "            vr.append(np.nan)\n",
    "    return np.nanmean(smd), np.nanmean(vr)\n",
    "\n",
    "# -----------------------------\n",
    "# XGBoost Main Loop (No DML)\n",
    "# -----------------------------\n",
    "def run_xgboost_with_trimmed_data(final_covariates_map):\n",
    "    att_results = []\n",
    "    balance_results = []\n",
    "\n",
    "    for group, covariates in final_covariates_map.items():\n",
    "        print(f\"\\n🚀 Running XGBoost for {group}\")\n",
    "        group_dir = os.path.join(output_folder, group)\n",
    "        os.makedirs(group_dir, exist_ok=True)\n",
    "\n",
    "        best_result = None\n",
    "        best_se = float(\"inf\")\n",
    "        \n",
    "        # Initialize lists to collect residuals and fitted values for diagnostic plots\n",
    "        group_residuals = []\n",
    "        group_fitted = []\n",
    "\n",
    "        for seed in seeds:\n",
    "            att_list, se_list, r2_list, rmse_list, smd_list, vr_list = [], [], [], [], [], []\n",
    "\n",
    "            for imp in range(1, imputations + 1):\n",
    "                file_path = os.path.join(group_dir, f\"trimmed_data_imp{imp}.pkl\")\n",
    "                if not os.path.exists(file_path):\n",
    "                    continue\n",
    "\n",
    "                df = pd.read_pickle(file_path)\n",
    "                if group not in df.columns or \"iptw\" not in df.columns:\n",
    "                    continue\n",
    "\n",
    "                X = df[covariates].copy()\n",
    "                T = df[group]\n",
    "                Y = df[\"caps5_change_baseline\"]\n",
    "                #W = df[\"iptw\"]\n",
    "\n",
    "                for repeat in range(n_repeats):\n",
    "                    kf = KFold(n_splits=5, shuffle=True, random_state=seed + repeat)\n",
    "                    for train_idx, test_idx in kf.split(X):\n",
    "                        try:\n",
    "                            X_train = X.iloc[train_idx]\n",
    "                            T_train = T.iloc[train_idx]\n",
    "                            Y_train = Y.iloc[train_idx]\n",
    "                            \n",
    "                            # XGBoost regression model\n",
    "                            model = xgb.XGBRegressor(n_estimators=100, max_depth=3, learning_rate=0.1, n_jobs=1, random_state=seed)\n",
    "                            \n",
    "                            # Add treatment variable to features\n",
    "                            X_train_with_T = X_train.copy()\n",
    "                            X_train_with_T[group] = T_train\n",
    "                            \n",
    "                            # Fit model\n",
    "                            model.fit(X_train_with_T, Y_train)\n",
    "                            \n",
    "                            # Predict outcomes for treated and control groups\n",
    "                            X_treated = X_train.copy()\n",
    "                            X_treated[group] = 1\n",
    "                            X_control = X_train.copy()\n",
    "                            X_control[group] = 0\n",
    "                            \n",
    "                            Y_pred_treated = model.predict(X_treated)\n",
    "                            Y_pred_control = model.predict(X_control)\n",
    "                            \n",
    "                            # Calculate ATT (Average Treatment Effect on Treated)\n",
    "                            treated_mask = T_train == 1\n",
    "                            if np.any(treated_mask):\n",
    "                                att = np.mean(Y_pred_treated[treated_mask] - Y_pred_control[treated_mask])\n",
    "                                \n",
    "                                # Calculate standard error (approximate)\n",
    "                                treatment_effects = Y_pred_treated[treated_mask] - Y_pred_control[treated_mask]\n",
    "                                residual = treatment_effects - att\n",
    "                                se = np.sqrt(np.sum((residual) ** 2)) / np.sum(treated_mask)\n",
    "                                att_list.append(att)\n",
    "                                se_list.append(se)\n",
    "\n",
    "                            # Model performance metrics\n",
    "                            Y_pred = model.predict(X_train_with_T)\n",
    "                            residuals = Y_train - Y_pred\n",
    "                            rmse = mean_squared_error(Y_train, Y_pred, squared=False)\n",
    "                            r2 = r2_score(Y_train, Y_pred)\n",
    "                            r2_list.append(r2)\n",
    "                            rmse_list.append(rmse)\n",
    "                            \n",
    "                            # Collect residuals and fitted values for diagnostic plots\n",
    "                            group_residuals.append(residuals.values)\n",
    "                            group_fitted.append(Y_pred)\n",
    "\n",
    "                            smd, vr = calculate_smd_vr(X_train, T_train)\n",
    "                            smd_list.append(smd)\n",
    "                            vr_list.append(vr)\n",
    "                        except Exception as e:\n",
    "                            print(f\"⚠️ Error in {group}, seed {seed}, imp {imp}, rep {repeat}: {e}\")\n",
    "\n",
    "            if att_list:\n",
    "                att, se, ci_l, ci_u, p_val = rubins_pool(att_list, se_list)\n",
    "                avg_r2 = np.mean(r2_list)\n",
    "                avg_rmse = np.mean(rmse_list)\n",
    "                avg_smd = np.mean(smd_list)\n",
    "                avg_vr = np.mean(vr_list)\n",
    "\n",
    "                balance_results.append({\n",
    "                    \"group\": group, \"seed\": seed, \"smd\": avg_smd, \"vr\": avg_vr\n",
    "                })\n",
    "\n",
    "                if se < best_se:\n",
    "                    best_se = se\n",
    "                    best_result = {\n",
    "                        \"group\": group, \"seed\": seed, \"att\": att, \"se\": se,\n",
    "                        \"ci_lower\": ci_l, \"ci_upper\": ci_u, \"p_value\": p_val,\n",
    "                        \"r2\": avg_r2, \"rmse\": avg_rmse\n",
    "                    }\n",
    "\n",
    "                print(f\"✅ {group} | Seed {seed}: ATT = {att:.4f}, SE = {se:.4f}, p = {p_val}\")\n",
    "            else:\n",
    "                print(f\"⚠️ No valid results for {group} | Seed {seed}\")\n",
    "\n",
    "        # Create diagnostic plots for this group\n",
    "        if group_residuals and group_fitted:\n",
    "            create_diagnostic_plots(group_residuals, group_fitted, group)\n",
    "\n",
    "        if best_result:\n",
    "            att_results.append(best_result)\n",
    "            print(f\"🏆 Best result for {group} → Seed {best_result['seed']} | SE = {best_result['se']:.4f}\")\n",
    "\n",
    "    # Save final output\n",
    "    pd.DataFrame(att_results).to_excel(\"xgb_rubin_summary_subsubcats_unweighted.xlsx\", index=False)\n",
    "    pd.DataFrame(balance_results).to_excel(\"smd_vr_summary_subsubcats_unweighted.xlsx\", index=False)\n",
    "    print(\"\\n🎯 All summary files saved.\")\n",
    "\n",
    "run_xgboost_with_trimmed_data(final_covariates_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4934729-7169-4ea1-8dd7-035896f38f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import sem, ttest_ind\n",
    "\n",
    "# ----------------------------------\n",
    "# File paths\n",
    "# ----------------------------------\n",
    "output_base = \"outputs\"\n",
    "att_file = \"xgb_rubin_summary_subsubcats.xlsx\"\n",
    "trimmed_file = \"trimmed_data_imp1.pkl\"\n",
    "auc_file = \"auc_scores.xlsx\"  # NEW\n",
    "\n",
    "# ----------------------------------\n",
    "# Load ATT Summary\n",
    "# ----------------------------------\n",
    "if os.path.exists(att_file):\n",
    "    att_df = pd.read_excel(att_file)\n",
    "else:\n",
    "    raise FileNotFoundError(\"❌ ATT summary file not found: xgb_rubin_summary_subsubcats.xlsx\")\n",
    "\n",
    "summary_rows = []\n",
    "\n",
    "# ----------------------------------\n",
    "# Loop over medication groups\n",
    "# ----------------------------------\n",
    "groups = [g for g in medication_groups if os.path.isdir(os.path.join(output_base, g))]\n",
    "\n",
    "for med in groups:\n",
    "    try:\n",
    "        group_path = os.path.join(output_base, med)\n",
    "\n",
    "        # Load trimmed data\n",
    "        df = pd.read_pickle(os.path.join(group_path, trimmed_file))\n",
    "\n",
    "        # Detect treatment column\n",
    "        treatment_cols = [col for col in df.columns if col.upper() == med.upper()]\n",
    "        if not treatment_cols:\n",
    "            print(f\"⚠️ Treatment column {med} not found in trimmed data. Skipping.\")\n",
    "            continue\n",
    "        treatment_var = treatment_cols[0]\n",
    "\n",
    "        # Extract treatment and outcome\n",
    "        T = df[treatment_var]\n",
    "        Y = df[\"caps5_change_baseline\"]\n",
    "\n",
    "        # Treated and control stats\n",
    "        treated = Y[T == 1]\n",
    "        control = Y[T == 0]\n",
    "\n",
    "        mean_treat = treated.mean()\n",
    "        se_treat = sem(treated) if len(treated) > 1 else np.nan\n",
    "\n",
    "        mean_ctrl = control.mean()\n",
    "        se_ctrl = sem(control) if len(control) > 1 else np.nan\n",
    "\n",
    "        # Cohen's d (unadjusted)\n",
    "        pooled_sd = np.sqrt(((treated.std() ** 2) + (control.std() ** 2)) / 2)\n",
    "        cohen_d = (mean_treat - mean_ctrl) / pooled_sd if pooled_sd > 0 else np.nan\n",
    "\n",
    "        # E-value (unadjusted)\n",
    "        delta = mean_treat - mean_ctrl\n",
    "        E = delta / abs(mean_ctrl) * 100 if mean_ctrl != 0 else np.nan\n",
    "\n",
    "        # Unadjusted p-value\n",
    "        try:\n",
    "            t_stat, p_val = ttest_ind(treated, control, equal_var=False, nan_policy=\"omit\")\n",
    "            rounded_p = round(p_val, 5)\n",
    "            formatted_p = \"< 0.00001\" if rounded_p < 0.00001 else rounded_p\n",
    "        except Exception:\n",
    "            formatted_p = np.nan\n",
    "\n",
    "        # AUC from new auc_scores.xlsx file\n",
    "        auc_val = np.nan\n",
    "        auc_path = os.path.join(group_path, auc_file)\n",
    "        if os.path.exists(auc_path):\n",
    "            auc_df = pd.read_excel(auc_path)\n",
    "            if \"AUC\" in auc_df.columns:\n",
    "                auc_val = auc_df[\"AUC\"].dropna().mean()\n",
    "\n",
    "        # Adjusted stats from Rubin summary\n",
    "        att_row = att_df[att_df[\"group\"].str.strip().str.upper() == med.strip().upper()]\n",
    "        if not att_row.empty:\n",
    "            att = att_row.iloc[0][\"att\"]\n",
    "            att_se = att_row.iloc[0][\"se\"]\n",
    "            att_p_val = att_row.iloc[0][\"p_value\"]\n",
    "            r2 = att_row.iloc[0][\"r2\"]\n",
    "            rmse = att_row.iloc[0][\"rmse\"]\n",
    "\n",
    "            try:\n",
    "                rounded_att_p = round(float(att_p_val), 5)\n",
    "                formatted_att_p = \"< 0.00001\" if rounded_att_p < 0.00001 else rounded_att_p\n",
    "            except:\n",
    "                formatted_att_p = att_p_val\n",
    "        else:\n",
    "            att, att_se, formatted_att_p, r2, rmse = np.nan, np.nan, np.nan, np.nan, np.nan\n",
    "\n",
    "        # Append full row\n",
    "        summary_rows.append({\n",
    "            'Medication Group': med,\n",
    "            'Mean Treated': mean_treat,\n",
    "            'SE Treated': se_treat,\n",
    "            'Mean Control': mean_ctrl,\n",
    "            'SE Control': se_ctrl,\n",
    "            'Cohen d': cohen_d,\n",
    "            'E (Unadjusted)': E,\n",
    "            'n Treated': len(treated),\n",
    "            'n Control': len(control),\n",
    "            #'Unadjusted p-value': formatted_p,\n",
    "            'ATT Estimate': att,\n",
    "            'ATT SE (Robust)': att_se,\n",
    "            'ATT p-value': formatted_att_p,\n",
    "            'R²': r2,\n",
    "            'RMSE': rmse,\n",
    "            'AUC': auc_val\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error in {med}: {e}\")\n",
    "\n",
    "# ----------------------------------\n",
    "# Save final summary\n",
    "# ----------------------------------\n",
    "summary_df = pd.DataFrame(summary_rows)\n",
    "summary_df = summary_df.sort_values(\"Medication Group\")\n",
    "summary_df.to_excel(\"Final_ATT_Summary_SubSubCat.xlsx\", index=False)\n",
    "print(\"✅ Final_ATT_Summary_SubSubCat saved.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9fc6c9a-6722-4950-bc5c-eae4aca6bc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "# ✅ Load the final summary table\n",
    "final_df = pd.read_excel(\"Final_ATT_Summary_SubSubCat.xlsx\")\n",
    "\n",
    "# ✅ Parse DML p-values (handle \"< 0.00001\")\n",
    "def parse_pval(p):\n",
    "    try:\n",
    "        if isinstance(p, str) and \"<\" in p:\n",
    "            return 0.000001\n",
    "        return float(p)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "final_df['ATT p-value'] = final_df['ATT p-value'].apply(parse_pval)\n",
    "\n",
    "# ✅ Plot settings\n",
    "width = 0.35\n",
    "\n",
    "# ✅ Plotting function for a single medication group\n",
    "def plot_single_group(row):\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    bars1 = ax.bar(-width/2, row['Mean Control'], width, \n",
    "                   yerr=row['SE Control'], label='Control', hatch='//', color='gray', capsize=5)\n",
    "    bars2 = ax.bar(+width/2, row['Mean Treated'], width, \n",
    "                   yerr=row['SE Treated'], label='Treated', color='steelblue', capsize=5)\n",
    "\n",
    "    label = (\n",
    "        f\"ATT = {row['ATT Estimate']:.2f}\\n\"\n",
    "        f\"d = {row['Cohen d']:.2f}, p = {row['ATT p-value']:.3f}\\n\"\n",
    "        f\"nT = {row['n Treated']}, nC = {row['n Control']}\\n\"\n",
    "        f\"E = {row['E (Unadjusted)']:.1f}%\"\n",
    "    )\n",
    "    max_y = max(row['Mean Control'], row['Mean Treated']) + 1.5\n",
    "    ax.text(0, max_y, label, ha='center', va='bottom', fontsize=9, color='#FFD700')\n",
    "\n",
    "    ax.axhline(0, color='black', linewidth=0.8)\n",
    "    ax.set_xticks([-width/2, +width/2])\n",
    "    ax.set_xticklabels(['Control', 'Treated'])\n",
    "    ax.set_title(f\"Group: {row['Medication Group']}\", fontsize=12, weight='bold')\n",
    "    ax.set_ylabel(\"CAPS5 Change Score\")\n",
    "    ax.set_ylim(bottom=0, top=max_y + 2)\n",
    "    ax.legend()\n",
    "    fig.tight_layout()\n",
    "    return fig\n",
    "\n",
    "# ✅ Generate and save all plots into a multi-page PDF\n",
    "with PdfPages(\"dml_att_barplot_subsubcat.pdf\") as pdf:\n",
    "    for idx, row in final_df.iterrows():\n",
    "        fig = plot_single_group(row)\n",
    "        pdf.savefig(fig, dpi=300, bbox_inches='tight')\n",
    "        plt.close(fig)\n",
    "\n",
    "print(\"✅ dml_att_barplot_subsubcat saved.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb20a16-342a-43ce-a51f-2a9d7e47c07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Love plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb55a5b1-1691-474a-a290-ba7b2e229dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "# ----------------------------------------\n",
    "# Functions to calculate balance\n",
    "# ----------------------------------------\n",
    "def calculate_smd(x1, x2, w1=None, w2=None):\n",
    "    def weighted_mean(x, w): return np.average(x, weights=w)\n",
    "    def weighted_var(x, w):\n",
    "        m = np.average(x, weights=w)\n",
    "        return np.average((x - m) ** 2, weights=w)\n",
    "    \n",
    "    m1 = weighted_mean(x1, w1) if w1 is not None else np.mean(x1)\n",
    "    m2 = weighted_mean(x2, w2) if w2 is not None else np.mean(x2)\n",
    "    v1 = weighted_var(x1, w1) if w1 is not None else np.var(x1, ddof=1)\n",
    "    v2 = weighted_var(x2, w2) if w2 is not None else np.var(x2, ddof=1)\n",
    "    \n",
    "    pooled_sd = np.sqrt((v1 + v2) / 2)\n",
    "    return np.abs(m1 - m2) / pooled_sd if pooled_sd > 0 else 0\n",
    "\n",
    "def variance_ratio(x1, x2, w1=None, w2=None):\n",
    "    def weighted_var(x, w):\n",
    "        m = np.average(x, weights=w)\n",
    "        return np.average((x - m) ** 2, weights=w)\n",
    "    \n",
    "    v1 = weighted_var(x1, w1) if w1 is not None else np.var(x1, ddof=1)\n",
    "    v2 = weighted_var(x2, w2) if w2 is not None else np.var(x2, ddof=1)\n",
    "    \n",
    "    return max(v1 / v2, v2 / v1) if v1 > 0 and v2 > 0 else 1\n",
    "\n",
    "# ----------------------------------------\n",
    "# Setup\n",
    "# ----------------------------------------\n",
    "output_base = \"outputs\"\n",
    "groups = [g for g in os.listdir(output_base) if os.path.isdir(os.path.join(output_base, g))]\n",
    "\n",
    "# Create a case-insensitive mapping\n",
    "final_covariates_map_lower = {k.lower(): v for k, v in final_covariates_map.items()}\n",
    "\n",
    "# ----------------------------------------\n",
    "# Main Loop\n",
    "# ----------------------------------------\n",
    "for group in groups:\n",
    "    if group.lower() not in final_covariates_map_lower:\n",
    "        continue\n",
    "\n",
    "    print(f\"\\n🔍 Processing {group}...\")\n",
    "\n",
    "    try:\n",
    "        group_path = os.path.join(output_base, group)\n",
    "        covariates = final_covariates_map_lower[group.lower()]\n",
    "        \n",
    "        column_name = None\n",
    "        for col in pd.read_pickle(os.path.join(group_path, \"trimmed_data_imp1.pkl\")).columns:\n",
    "            if col.lower() == group.lower():\n",
    "                column_name = col\n",
    "                break\n",
    "        if column_name is None:\n",
    "            print(f\"⚠️ Column not found for {group}, skipping.\")\n",
    "            continue\n",
    "\n",
    "        smd_unw_all, smd_w_all = [], []\n",
    "        vr_unw_all, vr_w_all = [], []\n",
    "\n",
    "        for i in range(1, 6):\n",
    "            df_path = os.path.join(group_path, f\"trimmed_data_imp{i}.pkl\")\n",
    "            iptw_path = os.path.join(group_path, \"iptw_weights.xlsx\")\n",
    "\n",
    "            if not os.path.exists(df_path) or not os.path.exists(iptw_path):\n",
    "                print(f\"⚠️ Missing data for {group} imp{i}, skipping.\")\n",
    "                continue\n",
    "\n",
    "            df = pd.read_pickle(df_path)\n",
    "            iptw_df = pd.read_excel(iptw_path, index_col=0)\n",
    "            T = df[column_name]\n",
    "            W = iptw_df.loc[df.index, \"iptw_mean\"]\n",
    "\n",
    "            smd_unw_i, smd_w_i, vr_unw_i, vr_w_i = [], [], [], []\n",
    "\n",
    "            for cov in covariates:\n",
    "                x1, x0 = df.loc[T == 1, cov], df.loc[T == 0, cov]\n",
    "                w1, w0 = W[T == 1], W[T == 0]\n",
    "\n",
    "                su = calculate_smd(x1, x0)\n",
    "                sw = calculate_smd(x1, x0, w1, w0)\n",
    "\n",
    "                vu = variance_ratio(x1, x0) if cov in ['treatmentdurationdays', 'CAPS5score_baseline', 'age'] else np.nan\n",
    "                vw = variance_ratio(x1, x0, w1, w0) if cov in ['treatmentdurationdays', 'CAPS5score_baseline', 'age'] else np.nan\n",
    "\n",
    "                smd_unw_i.append(su)\n",
    "                smd_w_i.append(sw)\n",
    "                vr_unw_i.append(vu)\n",
    "                vr_w_i.append(vw)\n",
    "\n",
    "            smd_unw_all.append(smd_unw_i)\n",
    "            smd_w_all.append(smd_w_i)\n",
    "            vr_unw_all.append(vr_unw_i)\n",
    "            vr_w_all.append(vr_w_i)\n",
    "\n",
    "        smd_unw = np.mean(smd_unw_all, axis=0)\n",
    "        smd_w = np.mean(smd_w_all, axis=0)\n",
    "        vr_unw = np.nanmean(vr_unw_all, axis=0)\n",
    "        vr_w = np.nanmean(vr_w_all, axis=0)\n",
    "\n",
    "        severity = []\n",
    "        for sw in smd_w:\n",
    "            if sw <= 0.1:\n",
    "                severity.append(\"Good\")\n",
    "            elif sw <= 0.2:\n",
    "                severity.append(\"Moderate\")\n",
    "            else:\n",
    "                severity.append(\"Poor\")\n",
    "\n",
    "        covariate_names = covariates\n",
    "        numeric_df = pd.DataFrame({\n",
    "            \"Covariate\": covariate_names,\n",
    "            \"SMD_Unweighted\": smd_unw,\n",
    "            \"SMD_Weighted\": smd_w,\n",
    "            \"Imbalance_Severity\": severity,\n",
    "            \"VR_Unweighted\": vr_unw,\n",
    "            \"VR_Weighted\": vr_w\n",
    "        })\n",
    "\n",
    "        numeric_path = os.path.join(group_path, f\"covariate_balance_table_{group}.xlsx\")\n",
    "        numeric_df.to_excel(numeric_path, index=False)\n",
    "        print(f\"📊 Exported numeric summary to: {numeric_path}\")\n",
    "\n",
    "        # -------------------------\n",
    "        # Plot\n",
    "        # -------------------------\n",
    "        labels = covariates\n",
    "        y_pos = np.arange(len(labels))\n",
    "\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(18, len(labels) * 0.45))\n",
    "\n",
    "        axes[0].scatter(smd_unw, y_pos, color='red', label=\"Unweighted\")\n",
    "        axes[0].scatter(smd_w, y_pos, color='blue', label=\"Weighted\")\n",
    "        axes[0].axvline(0.1, color='gray', linestyle='--', label=\"Threshold 0.1\")\n",
    "        axes[0].axvline(0.2, color='black', linestyle='--', label=\"Threshold 0.2\")\n",
    "        axes[0].set_xlim(0, max(max(smd_unw), max(smd_w), 0.25) + 0.05)\n",
    "        axes[0].set_yticks(y_pos)\n",
    "        axes[0].set_yticklabels(labels)\n",
    "        axes[0].invert_yaxis()\n",
    "        axes[0].set_title(\"Standardized Mean Differences (SMD)\")\n",
    "        axes[0].legend(loc=\"upper right\")\n",
    "        axes[0].grid(True)\n",
    "\n",
    "        vr_mask = [cov in ['treatmentdurationdays', 'CAPS5score_baseline', 'age'] for cov in covariates]\n",
    "        filtered_y = [i for i, b in enumerate(vr_mask) if b]\n",
    "        filtered_labels = [labels[i] for i in filtered_y]\n",
    "        filtered_vr_unw = [vr_unw[i] for i in filtered_y]\n",
    "        filtered_vr_w = [vr_w[i] for i in filtered_y]\n",
    "\n",
    "        axes[1].scatter(filtered_vr_unw, filtered_y, color='blue', marker='o', label=\"Unweighted\")\n",
    "        axes[1].scatter(filtered_vr_w, filtered_y, color='red', marker='x', label=\"Weighted\")\n",
    "        axes[1].axvline(2, color='gray', linestyle='--')\n",
    "        axes[1].axvline(0.5, color='gray', linestyle='--')\n",
    "        axes[1].set_xlim(0, max(filtered_vr_unw + filtered_vr_w + [2.5]) + 0.5)\n",
    "        axes[1].set_yticks(filtered_y)\n",
    "        axes[1].set_yticklabels(filtered_labels)\n",
    "        axes[1].invert_yaxis()\n",
    "        axes[1].set_title(\"Variance Ratio (VR)\")\n",
    "        axes[1].legend()\n",
    "        axes[1].grid(True)\n",
    "\n",
    "        fig.suptitle(f\"Covariate Balance for {group.replace('CAT_', '')}\", fontsize=14, weight='bold')\n",
    "        fig.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "        plot_path = os.path.join(group_path, f\"love_plot_{group}.pdf\")\n",
    "        fig.savefig(plot_path, dpi=300)\n",
    "        plt.close()\n",
    "        print(f\"✅ Saved love plot: {plot_path}\")\n",
    "        print(f\"📏 Max weighted SMD for {group}: {np.max(smd_w):.3f}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error in {group}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddcdd856-d7d1-4798-a62a-e15111f052d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5954afc7-c558-47c5-8220-2e870f68f38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "#-----------------------------\n",
    "# Generate heatmaps\n",
    "# -------------------------------\n",
    "for treatment_var in medication_groups:\n",
    "    print(f\"\\n========== Creating Heatmap for {treatment_var} ==========\")\n",
    "\n",
    "    try:\n",
    "        output_folder = os.path.join('outputs', treatment_var)\n",
    "        balance_path = os.path.join(output_folder, f'covariate_balance_table_{treatment_var}.xlsx')\n",
    "\n",
    "        if not os.path.exists(balance_path):\n",
    "            print(f\"❌ Balance file not found: {balance_path}\")\n",
    "            continue\n",
    "\n",
    "        balance_df = pd.read_excel(balance_path)\n",
    "\n",
    "        # ✅ Use finalized covariates + 'Propensity Score'\n",
    "        covariates = final_covariates_map[treatment_var] + ['Propensity Score']\n",
    "        balance_df = balance_df[balance_df['Covariate'].isin(covariates)]\n",
    "\n",
    "        # ✅ Check for CAPS5score_baseline\n",
    "        highlight_caps = 'CAPS5score_baseline' in balance_df['Covariate'].values\n",
    "\n",
    "        # ✅ Format for heatmap\n",
    "        heatmap_df = balance_df[['Covariate', 'SMD_Unweighted', 'SMD_Weighted']].copy()\n",
    "        heatmap_df.columns = ['Covariate', 'Unweighted', 'Weighted']\n",
    "        heatmap_df = heatmap_df.set_index('Covariate')\n",
    "        heatmap_df = heatmap_df.sort_values(by='Unweighted', ascending=False)\n",
    "\n",
    "        # ✅ Plot\n",
    "        plt.figure(figsize=(12, max(10, len(heatmap_df) * 0.35)))\n",
    "        ax = sns.heatmap(\n",
    "            heatmap_df,\n",
    "            cmap=\"coolwarm\",\n",
    "            annot=True,\n",
    "            fmt=\".2f\",\n",
    "            linewidths=0.6,\n",
    "            linecolor='gray',\n",
    "            cbar_kws={\"label\": \"Standardized Mean Difference\"}\n",
    "        )\n",
    "\n",
    "        plt.title(f\"Covariate Balance Heatmap (Rubin IPTW)\\n{treatment_var}\", fontsize=15, weight='bold')\n",
    "        plt.xlabel(\"Condition\")\n",
    "        plt.ylabel(\"Covariate\")\n",
    "\n",
    "        # ✅ Bold CAPS5score_baseline if present\n",
    "        if highlight_caps:\n",
    "            ylabels = [label.get_text() for label in ax.get_yticklabels()]\n",
    "            ax.set_yticklabels([\n",
    "                f\"{label} ←\" if label == 'CAPS5score_baseline' else label for label in ylabels\n",
    "            ])\n",
    "\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # ✅ Save image\n",
    "        save_path = os.path.join(output_folder, f'heatmap_smd_{treatment_var}.png')\n",
    "        plt.savefig(save_path, dpi=300)\n",
    "        plt.close()\n",
    "\n",
    "        print(f\"✅ Heatmap saved: {save_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error processing {treatment_var}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41205247-e0b5-4fbc-ba3e-6aef9f0db831",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c2fa40d9-c5d4-4d72-b2bb-c7fc640070bd",
   "metadata": {},
   "source": [
    "#### Linear:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "603b1f68-ffa1-4f34-af91-6e956a00dbea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Changed to: D:\\Work\\PTSD\\Linear\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")  # optional\n",
    "\n",
    "# Option 1 (recommended)\n",
    "new_path = r\"D:\\Work\\PTSD\\Linear\"\n",
    "\n",
    "os.chdir(new_path)  # Change working directory\n",
    "print(\"Changed to:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d71fe1e9-9097-47b9-a5ae-72f9408705de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(r\"data_baseline.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde047be-55f7-4cad-b980-cf121cf7ec5e",
   "metadata": {},
   "source": [
    "### CAT analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2be65d52-1c12-486c-9558-a1eecf3d9b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all necessary packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "# For visualization and future steps\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.experimental import enable_iterative_imputer  # Needed to enable the experimental feature\n",
    "from sklearn.impute import IterativeImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4e72b91-21f3-4b13-bdfc-d027fb163745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of dataset: (6125, 465)\n",
      "\n",
      "Sample columns: ['CIN5', 'StartDatum', 'BEH_MOD', 'BEHDAGEN_GEPLAND', 'AANTAL_PCL', 'TOESTWO', 'BEH_AFG', 'TK', 'MM_CAPS_IN', 'MM_CAPS_TK']\n",
      "\n",
      "Missing values:\n",
      " instrument_SDV_IN    6125\n",
      "Eaantal_TK           6125\n",
      "Dcriterium_FU        6125\n",
      "Cernst_FU            6125\n",
      "Caantal_FU           6125\n",
      "Ccriterium_FU        6125\n",
      "Bernst_FU            6125\n",
      "Baantal_FU           6125\n",
      "Bcriterium_FU        6125\n",
      "Eernst_TK            6125\n",
      "dtype: int64\n",
      "Shape after removing duplicates: (6125, 465)\n"
     ]
    }
   ],
   "source": [
    "# Check basic structure\n",
    "print(\"Shape of dataset:\", df.shape)\n",
    "print(\"\\nSample columns:\", df.columns.tolist()[:10])\n",
    "print(\"\\nMissing values:\\n\", df.isnull().sum().sort_values(ascending=False).head(10))\n",
    "\n",
    "# Remove duplicate rows\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "# Confirm shape after removing duplicates\n",
    "print(\"Shape after removing duplicates:\", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34494260-4db2-4e0e-9415-bd65ce18a0da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SDV_SEXE  gender_label\n",
      "2.0       Female          4602\n",
      "1.0       Male            1475\n",
      "3.0       Other             48\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pyreadstat\n",
    "\n",
    "# Load gender info\n",
    "gender_df, meta = pyreadstat.read_sav(\"SDV_IN_Gender_2019_2024.sav\")\n",
    "\n",
    "# Just extract SDV_SEXE column and append to df\n",
    "df[\"SDV_SEXE\"] = gender_df[\"SDV_SEXE\"].reset_index(drop=True)\n",
    "\n",
    "# Optional: map to labels\n",
    "gender_map = {1.0: \"Male\", 2.0: \"Female\", 3.0: \"Other\"}\n",
    "df[\"gender_label\"] = df[\"SDV_SEXE\"].map(gender_map)\n",
    "\n",
    "# Done! Check a sample\n",
    "print(df[[\"SDV_SEXE\", \"gender_label\"]].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8c8ee38-b998-4ab6-9123-8dbc00c7014b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gender dummy variables\n",
    "df['gender_1'] = (df['gender'] == 1).astype(int)\n",
    "df['gender_2'] = (df['gender'] == 2).astype(int)\n",
    "\n",
    "# SDV_SEXE dummy variables\n",
    "df['SDV_SEXE_1'] = (df['SDV_SEXE'] == 1).astype(int)\n",
    "df['SDV_SEXE_2'] = (df['SDV_SEXE'] == 2).astype(int)\n",
    "df['SDV_SEXE_3'] = (df['SDV_SEXE'] == 3).astype(int)\n",
    "\n",
    "# Create binary columns\n",
    "df['ethnicity_Dutch'] = np.where(df['ethnicity'] == 1, 1, 0)\n",
    "df['ethnicity_other'] = np.where(df['ethnicity'] != 1, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39916736-26ad-496b-aad5-ad8971ebd97e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining columns: 458\n"
     ]
    }
   ],
   "source": [
    "# Columns manually identified for removal (example set from the R script)\n",
    "cols_to_drop = [\n",
    "    'gender', 'ethnicity', 'CIN5', 'SDV_SEXE', 'StartDatum', 'STARTDATUM', 'DROPOUT_EARLYCOMPLETER', 'TOEST_WO',\n",
    "    'depressie_IN', 'TERUGKOMER', 'VROEGK_ST', 'gender_label',\n",
    "    'depr_m_psychose_huid', 'depr_z_psychose_huid', 'depr_z_psychose_verl',\n",
    "    'depr_m_psychose_verl', 'CAPS5score_followup', 'CAPS5_DAT_IN'\n",
    "]\n",
    "\n",
    "df = df.drop(columns=[col for col in cols_to_drop if col in df.columns])\n",
    "print(\"Remaining columns:\", df.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "64d8554a-5a61-4e93-aad3-cccbb21e91ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'BEH_DAGEN' in df.columns:\n",
    "    df.rename(columns={'BEH_DAGEN': 'treatmentdurationdays'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "13ecc924-a101-4aa2-8bda-6ccb35757f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean and standardize column names\n",
    "df.columns = (\n",
    "    df.columns\n",
    "    .str.replace(r\"\\.+\", \"_\", regex=True)\n",
    "    .str.replace(r\"[^a-zA-Z0-9_]\", \"\", regex=True)\n",
    "    .str.replace(\" \", \"_\")\n",
    "    .str.strip()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5cbcc1fe-29a8-487d-83e7-89ebc407d9df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CAPS5score_baseline: 0 missing\n",
      "CAPS5Score_TK: 0 missing\n"
     ]
    }
   ],
   "source": [
    "# Preview key outcome variables\n",
    "outcome_vars = ['CAPS5score_baseline', 'CAPS5Score_TK']\n",
    "for col in outcome_vars:\n",
    "    if col in df.columns:\n",
    "        print(f\"{col}: {df[col].isnull().sum()} missing\")\n",
    "\n",
    "# Calculate change score\n",
    "if 'CAPS5score_baseline' in df.columns and 'CAPS5Score_TK' in df.columns:\n",
    "    df['caps5_change_baseline'] = df['CAPS5Score_TK'] - df['CAPS5score_baseline']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b6b826fa-2bcc-47be-b765-b9e0ec65d2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define exceptions to keep\n",
    "protected_cols = [\n",
    "    \"DIAGNOSIS_ANXIETY_OCD\",\n",
    "    \"DIAGNOSIS_PSYCHOTIC\",\n",
    "    \"DIAGNOSIS_EATING_DISORDER\",\n",
    "    \"DIAGNOSIS_SUBSTANCE_DISORDER\", \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"EB_NON_TF_THERAPY\", \"OTHER_TREATM_APPROACH\", \"Bipolar_and_Mood_disorder\", 'SUBCAT_Selectieve_immunosuppresiva', 'treatmentdurationdays',\n",
    "'SUBCAT_Corticosteroiden',\n",
    "'SUBCAT_Immunomodulerend_Coxibs',\n",
    "'SUBCAT_Aminosalicylaten',\n",
    "'SUBCAT_calcineurineremmers',\n",
    "'SUBCAT_Anti_epileptica_Benzodiazepine',\n",
    "'SUBCAT_Paracetamol_overig_combinatie', 'SUBCAT_MAO_remmers', 'SUBCAT_psychostimulans_overige', 'SUBCAT_Interleukine_remmers'\n",
    "\n",
    "]\n",
    "\n",
    "\n",
    "# ----------------------------------------\n",
    "# 1. Drop columns with >95% missing values (except protected)\n",
    "thresh_missing = int(0.95 * len(df))\n",
    "missing_cols = [col for col in df.columns if df[col].isnull().sum() > (len(df) - thresh_missing)]\n",
    "missing_cols_to_drop = [col for col in missing_cols if col not in protected_cols]\n",
    "df = df.drop(columns=missing_cols_to_drop)\n",
    "\n",
    "# ----------------------------------------\n",
    "# 2. Drop near-zero variance columns (except protected)\n",
    "low_variance_cols = [col for col in df.columns if df[col].nunique(dropna=True) <= 1 and col not in protected_cols]\n",
    "df = df.drop(columns=low_variance_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ce9cb46-a001-4a67-a879-12609fe04975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Step 1 Complete: Cleaned dataset saved.\n"
     ]
    }
   ],
   "source": [
    "df.to_csv(\"cleaned_data_baseline.csv\", index=False)\n",
    "print(\" Step 1 Complete: Cleaned dataset saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "efec01fd-23a3-422f-a48f-2c743d1a95b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3d2cb15a-68ac-48dc-950d-468a617fa482",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from fancyimpute import IterativeImputer\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "961b11fc-d3ff-48fe-8f66-8f376dc1f989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6125, 204)\n",
      "DIAGNOSIS_ANXIETY_OCD           float64\n",
      "DIAGNOSIS_SMOKING               float64\n",
      "DIAGNOSIS_EATING_DISORDER       float64\n",
      "DIAGNOSIS_SUBSTANCE_DISORDER    float64\n",
      "DIAGNOSIS_PSYCHOTIC             float64\n",
      "DIAGNOSIS_SUICIDALITY           float64\n",
      "DIAGNOSIS_SEXUAL_TRAUMA         float64\n",
      "DIAGNOSIS_CHILDHOOD_TRAUMA        int64\n",
      "DIAGNOSIS_CPTSD                 float64\n",
      "treatmentdurationdays           float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Load the cleaned dataset from Step 1\n",
    "df = pd.read_csv(\"cleaned_data_baseline.csv\")\n",
    "\n",
    "# Quick check\n",
    "print(df.shape)\n",
    "print(df.dtypes.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4d323534-760c-4cbe-bd3d-b5f256772198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate Numerical and Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "06098718-4825-4c06-9b9c-31c5299ec0e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numerical Columns: 204\n",
      "Categorical Columns: 0\n"
     ]
    }
   ],
   "source": [
    "# Identify numerical and categorical columns\n",
    "numerical_cols = df.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "print(f\"Numerical Columns: {len(numerical_cols)}\")\n",
    "print(f\"Categorical Columns: {len(categorical_cols)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "975828d9-69eb-4e4f-9d85-274ae2afaadc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6125 entries, 0 to 6124\n",
      "Columns: 204 entries, DIAGNOSIS_ANXIETY_OCD to ethnicity_other\n",
      "dtypes: float64(10), int64(194)\n",
      "memory usage: 9.5 MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3dff8ed0-c7ad-4031-8c73-56704d132a8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Step 2 Complete: Final prepared dataset saved as 'final_prepared_data.csv'.\n"
     ]
    }
   ],
   "source": [
    "# Save the fully prepared data\n",
    "df.to_csv(\"final_prepared_data.csv\", index=False)\n",
    "print(\" Step 2 Complete: Final prepared dataset saved as 'final_prepared_data.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9213b96c-1f3e-4537-9881-1cf57f1cae8c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "STEP 1: MICE IMPUTATION\n",
      "==================================================\n",
      "\n",
      "=== Running MICE Imputation: Dataset 1 ===\n",
      " Completed imputation 1\n",
      "\n",
      "=== Running MICE Imputation: Dataset 2 ===\n",
      " Completed imputation 2\n",
      "\n",
      "=== Running MICE Imputation: Dataset 3 ===\n",
      " Completed imputation 3\n",
      "\n",
      "=== Running MICE Imputation: Dataset 4 ===\n",
      " Completed imputation 4\n",
      "\n",
      "=== Running MICE Imputation: Dataset 5 ===\n",
      " Completed imputation 5\n",
      "\n",
      "==================================================\n",
      "STEP 2: ROUNDING NUMERIC COLUMNS\n",
      "==================================================\n",
      " Imputation 1: Rounded 204 numeric columns to 0 decimal place(s).\n",
      " Imputation 2: Rounded 204 numeric columns to 0 decimal place(s).\n",
      " Imputation 3: Rounded 204 numeric columns to 0 decimal place(s).\n",
      " Imputation 4: Rounded 204 numeric columns to 0 decimal place(s).\n",
      " Imputation 5: Rounded 204 numeric columns to 0 decimal place(s).\n",
      "\n",
      "==================================================\n",
      "STEP 3: SAVING FINAL DATASETS\n",
      "==================================================\n",
      " Saved files for imputation 1:\n",
      "   → imputed_data/df_imputed_final_imp1.pkl\n",
      "   → imputed_data/df_imputed_final_imp1.csv\n",
      "   → imputed_data/df_imputed_final_imp1.xlsx\n",
      " Saved files for imputation 2:\n",
      "   → imputed_data/df_imputed_final_imp2.pkl\n",
      "   → imputed_data/df_imputed_final_imp2.csv\n",
      "   → imputed_data/df_imputed_final_imp2.xlsx\n",
      " Saved files for imputation 3:\n",
      "   → imputed_data/df_imputed_final_imp3.pkl\n",
      "   → imputed_data/df_imputed_final_imp3.csv\n",
      "   → imputed_data/df_imputed_final_imp3.xlsx\n",
      " Saved files for imputation 4:\n",
      "   → imputed_data/df_imputed_final_imp4.pkl\n",
      "   → imputed_data/df_imputed_final_imp4.csv\n",
      "   → imputed_data/df_imputed_final_imp4.xlsx\n",
      " Saved files for imputation 5:\n",
      "   → imputed_data/df_imputed_final_imp5.pkl\n",
      "   → imputed_data/df_imputed_final_imp5.csv\n",
      "   → imputed_data/df_imputed_final_imp5.xlsx\n",
      "\n",
      "==================================================\n",
      "STEP 4: VERIFYING DATASET DIFFERENCES\n",
      "==================================================\n",
      " Checking differences in 10 columns that had missing values...\n",
      " Column 'DIAGNOSIS_ANXIETY_OCD': 694 different values between datasets 1 & 2\n",
      " Column 'DIAGNOSIS_SMOKING': 50 different values between datasets 1 & 2\n",
      " Column 'DIAGNOSIS_EATING_DISORDER': IDENTICAL values between datasets 1 & 2\n",
      "\n",
      " SUCCESS: Datasets show proper variability!\n",
      "\n",
      "==================================================\n",
      " MICE IMPUTATION COMPLETE!\n",
      "==================================================\n",
      " Created 5 imputed datasets\n",
      " Applied rounding to all numeric columns\n",
      " Saved files in: imputed_data/\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.experimental import enable_iterative_imputer  # noqa\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "# ========== CONFIG ==========\n",
    "save_folder = \"imputed_data\"\n",
    "os.makedirs(save_folder, exist_ok=True)\n",
    "n_imputations = 5\n",
    "\n",
    "# ========== LOAD ==========\n",
    "# Ensure df is already defined\n",
    "assert 'df' in globals(), \"Please load the original DataFrame as `df` before running this script.\"\n",
    "\n",
    "# ========== IDENTIFY NUMERIC COLUMNS ==========\n",
    "numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "\n",
    "# ========== STEP 1: MICE IMPUTATION ==========\n",
    "print(\"=\" * 50)\n",
    "print(\"STEP 1: MICE IMPUTATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "imputed_dfs = []\n",
    "for i in range(1, n_imputations + 1):\n",
    "    print(f\"\\n=== Running MICE Imputation: Dataset {i} ===\")\n",
    "    #  NEW instance with different seed AND sample_posterior=True for randomness\n",
    "    mice_imputer = IterativeImputer(\n",
    "        max_iter=10, \n",
    "        random_state=42+i,  # Different base to avoid low numbers\n",
    "        sample_posterior=True,  #  KEY: This adds randomness!\n",
    "        n_nearest_features=None,\n",
    "        initial_strategy='mean'\n",
    "    )\n",
    "    # Fit-transform on numeric columns\n",
    "    imputed_array = mice_imputer.fit_transform(df[numeric_cols])\n",
    "    # Replace numeric columns in a copy of the original df\n",
    "    df_imputed = df.copy()\n",
    "    df_imputed[numeric_cols] = pd.DataFrame(imputed_array, columns=numeric_cols, index=df.index)\n",
    "    # Append to list\n",
    "    imputed_dfs.append(df_imputed)\n",
    "    print(f\" Completed imputation {i}\")\n",
    "\n",
    "# ========== STEP 2: ROUNDING ==========\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"STEP 2: ROUNDING NUMERIC COLUMNS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "def round_all_numeric_columns_all_imputations(imputed_dfs, decimals=0, verbose=True):\n",
    "    rounded_dfs = []\n",
    "    for i, df in enumerate(imputed_dfs):\n",
    "        df_copy = df.copy()\n",
    "        numeric_cols = df_copy.select_dtypes(include=[np.number]).columns\n",
    "        df_copy[numeric_cols] = df_copy[numeric_cols].round(decimals)\n",
    "        rounded_dfs.append(df_copy)\n",
    "        if verbose:\n",
    "            print(f\" Imputation {i+1}: Rounded {len(numeric_cols)} numeric columns to {decimals} decimal place(s).\")\n",
    "    return rounded_dfs\n",
    "\n",
    "# Apply rounding to all imputed datasets\n",
    "imputed_dfs = round_all_numeric_columns_all_imputations(imputed_dfs)\n",
    "\n",
    "# ========== STEP 3: SAVE FINAL DATASETS ==========\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"STEP 3: SAVING FINAL DATASETS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for i, df_imputed in enumerate(imputed_dfs, 1):\n",
    "    # Save outputs\n",
    "    pkl_path = f\"{save_folder}/df_imputed_final_imp{i}.pkl\"\n",
    "    csv_path = f\"{save_folder}/df_imputed_final_imp{i}.csv\"\n",
    "    excel_path = f\"{save_folder}/df_imputed_final_imp{i}.xlsx\"\n",
    "    \n",
    "    df_imputed.to_pickle(pkl_path)\n",
    "    df_imputed.to_csv(csv_path, index=False)\n",
    "    df_imputed.to_excel(excel_path, index=False)\n",
    "    \n",
    "    print(f\" Saved files for imputation {i}:\")\n",
    "    print(f\"   → {pkl_path}\")\n",
    "    print(f\"   → {csv_path}\")\n",
    "    print(f\"   → {excel_path}\")\n",
    "\n",
    "# ========== STEP 4: VERIFY DATASETS ARE DIFFERENT ==========\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"STEP 4: VERIFYING DATASET DIFFERENCES\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "def check_imputation_differences(imputed_dfs, verbose=True):\n",
    "    \"\"\"Check if imputed datasets are actually different from each other\"\"\"\n",
    "    if len(imputed_dfs) < 2:\n",
    "        print(\"  Only one dataset - cannot check differences\")\n",
    "        return\n",
    "    \n",
    "    # Get numeric columns that had missing values originally\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    missing_cols = [col for col in numeric_cols if df[col].isnull().any()]\n",
    "    \n",
    "    if not missing_cols:\n",
    "        print(\"  No missing values found in original data\")\n",
    "        return\n",
    "    \n",
    "    print(f\" Checking differences in {len(missing_cols)} columns that had missing values...\")\n",
    "    \n",
    "    differences_found = False\n",
    "    \n",
    "    for col in missing_cols[:3]:  # Check first 3 columns with missing values\n",
    "        # Compare first two datasets for this column\n",
    "        values_1 = imputed_dfs[0][col].values\n",
    "        values_2 = imputed_dfs[1][col].values\n",
    "        \n",
    "        if not np.array_equal(values_1, values_2):\n",
    "            differences_found = True\n",
    "            # Count how many values are different\n",
    "            diff_count = np.sum(values_1 != values_2)\n",
    "            print(f\" Column '{col}': {diff_count} different values between datasets 1 & 2\")\n",
    "        else:\n",
    "            print(f\" Column '{col}': IDENTICAL values between datasets 1 & 2\")\n",
    "    \n",
    "    if differences_found:\n",
    "        print(f\"\\n SUCCESS: Datasets show proper variability!\")\n",
    "    else:\n",
    "        print(f\"\\n  WARNING: Datasets appear identical - check random_state implementation\")\n",
    "    \n",
    "    return differences_found\n",
    "\n",
    "# Run the check\n",
    "check_imputation_differences(imputed_dfs)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\" MICE IMPUTATION COMPLETE!\")\n",
    "print(\"=\" * 50)\n",
    "print(f\" Created {n_imputations} imputed datasets\")\n",
    "print(f\" Applied rounding to all numeric columns\")\n",
    "print(f\" Saved files in: {save_folder}/\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c137e3fd-a734-4ec2-ba57-fb0c19225709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CHECKING IMPUTATION DIFFERENCES\n",
      "============================================================\n",
      "Dataset 1 vs Dataset 2: DIFFERENT ✅\n",
      "  'DIAGNOSIS_ANXIETY_OCD': 694 different values\n",
      "  'DIAGNOSIS_SMOKING': 50 different values\n",
      "  'DIAGNOSIS_SUBSTANCE_DISORDER': 240 different values\n",
      "  'DIAGNOSIS_PSYCHOTIC': 164 different values\n",
      "  'DIAGNOSIS_SUICIDALITY': 9 different values\n",
      "  'DIAGNOSIS_SEXUAL_TRAUMA': 15 different values\n",
      "  'DIAGNOSIS_CPTSD': 36 different values\n",
      "  'treatmentdurationdays': 1170 different values\n",
      "  'Bipolar_and_Mood_disorder': 928 different values\n",
      "  Total different values: 3306\n",
      "\n",
      "=== Checking all 5 datasets ===\n",
      "Dataset 1 vs Dataset 2: DIFFERENT ✅\n",
      "Dataset 1 vs Dataset 3: DIFFERENT ✅\n",
      "Dataset 1 vs Dataset 4: DIFFERENT ✅\n",
      "Dataset 1 vs Dataset 5: DIFFERENT ✅\n",
      "Dataset 2 vs Dataset 3: DIFFERENT ✅\n",
      "Dataset 2 vs Dataset 4: DIFFERENT ✅\n",
      "Dataset 2 vs Dataset 5: DIFFERENT ✅\n",
      "Dataset 3 vs Dataset 4: DIFFERENT ✅\n",
      "Dataset 3 vs Dataset 5: DIFFERENT ✅\n",
      "Dataset 4 vs Dataset 5: DIFFERENT ✅\n",
      "\n",
      "=== Checking differences in originally missing positions ===\n",
      "\n",
      "Column 'DIAGNOSIS_ANXIETY_OCD' (2484 missing values):\n",
      "  Dataset 1 vs 2: 694/2484 different imputed values ✅\n",
      "  Dataset 2 vs 3: 686/2484 different imputed values ✅\n",
      "  Dataset 3 vs 4: 726/2484 different imputed values ✅\n",
      "  Dataset 4 vs 5: 660/2484 different imputed values ✅\n",
      "\n",
      "Column 'DIAGNOSIS_SMOKING' (99 missing values):\n",
      "  Dataset 1 vs 2: 50/99 different imputed values ✅\n",
      "  Dataset 2 vs 3: 51/99 different imputed values ✅\n",
      "  Dataset 3 vs 4: 55/99 different imputed values ✅\n",
      "  Dataset 4 vs 5: 52/99 different imputed values ✅\n",
      "\n",
      "Column 'DIAGNOSIS_EATING_DISORDER' (2484 missing values):\n",
      "  Dataset 1 vs 2: IDENTICAL imputed values ❌\n",
      "  Dataset 2 vs 3: IDENTICAL imputed values ❌\n",
      "  Dataset 3 vs 4: IDENTICAL imputed values ❌\n",
      "  Dataset 4 vs 5: IDENTICAL imputed values ❌\n",
      "\n",
      "Column 'DIAGNOSIS_SUBSTANCE_DISORDER' (2484 missing values):\n",
      "  Dataset 1 vs 2: 240/2484 different imputed values ✅\n",
      "  Dataset 2 vs 3: 222/2484 different imputed values ✅\n",
      "  Dataset 3 vs 4: 255/2484 different imputed values ✅\n",
      "  Dataset 4 vs 5: 271/2484 different imputed values ✅\n",
      "\n",
      "Column 'DIAGNOSIS_PSYCHOTIC' (2484 missing values):\n",
      "  Dataset 1 vs 2: 164/2484 different imputed values ✅\n",
      "  Dataset 2 vs 3: 174/2484 different imputed values ✅\n",
      "  Dataset 3 vs 4: 183/2484 different imputed values ✅\n",
      "  Dataset 4 vs 5: 172/2484 different imputed values ✅\n",
      "\n",
      "Column 'DIAGNOSIS_SUICIDALITY' (19 missing values):\n",
      "  Dataset 1 vs 2: 9/19 different imputed values ✅\n",
      "  Dataset 2 vs 3: 6/19 different imputed values ✅\n",
      "  Dataset 3 vs 4: 4/19 different imputed values ✅\n",
      "  Dataset 4 vs 5: 12/19 different imputed values ✅\n",
      "\n",
      "Column 'DIAGNOSIS_SEXUAL_TRAUMA' (35 missing values):\n",
      "  Dataset 1 vs 2: 15/35 different imputed values ✅\n",
      "  Dataset 2 vs 3: 15/35 different imputed values ✅\n",
      "  Dataset 3 vs 4: 17/35 different imputed values ✅\n",
      "  Dataset 4 vs 5: 14/35 different imputed values ✅\n",
      "\n",
      "Column 'DIAGNOSIS_CPTSD' (90 missing values):\n",
      "  Dataset 1 vs 2: 36/90 different imputed values ✅\n",
      "  Dataset 2 vs 3: 32/90 different imputed values ✅\n",
      "  Dataset 3 vs 4: 40/90 different imputed values ✅\n",
      "  Dataset 4 vs 5: 33/90 different imputed values ✅\n",
      "\n",
      "Column 'treatmentdurationdays' (1431 missing values):\n",
      "  Dataset 1 vs 2: 1170/1431 different imputed values ✅\n",
      "  Dataset 2 vs 3: 1184/1431 different imputed values ✅\n",
      "  Dataset 3 vs 4: 1213/1431 different imputed values ✅\n",
      "  Dataset 4 vs 5: 1182/1431 different imputed values ✅\n",
      "\n",
      "Column 'Bipolar_and_Mood_disorder' (2484 missing values):\n",
      "  Dataset 1 vs 2: 928/2484 different imputed values ✅\n",
      "  Dataset 2 vs 3: 967/2484 different imputed values ✅\n",
      "  Dataset 3 vs 4: 972/2484 different imputed values ✅\n",
      "  Dataset 4 vs 5: 966/2484 different imputed values ✅\n",
      "\n",
      "🎉 SUCCESS: Found differences in imputed values!\n",
      "\n",
      "=== Sample imputed values (first 3 missing positions) ===\n",
      "\n",
      "Column 'DIAGNOSIS_ANXIETY_OCD' at positions [7, 9, 13]:\n",
      "  Dataset 1: [-0. -0. -0.]\n",
      "  Dataset 2: [ 0. -0.  0.]\n",
      "  Dataset 3: [-0. -1.  0.]\n",
      "  Dataset 4: [ 1. -1. -0.]\n",
      "  Dataset 5: [-0.  1. -0.]\n",
      "\n",
      "Column 'DIAGNOSIS_SMOKING' at positions [5, 8, 65]:\n",
      "  Dataset 1: [-0.  0.  1.]\n",
      "  Dataset 2: [0. 0. 0.]\n",
      "  Dataset 3: [2. 1. 0.]\n",
      "  Dataset 4: [-1.  0.  1.]\n",
      "  Dataset 5: [1. 1. 1.]\n",
      "\n",
      "Column 'DIAGNOSIS_EATING_DISORDER' at positions [7, 9, 13]:\n",
      "  Dataset 1: [-0. -0. -0.]\n",
      "  Dataset 2: [-0.  0.  0.]\n",
      "  Dataset 3: [-0. -0.  0.]\n",
      "  Dataset 4: [ 0.  0. -0.]\n",
      "  Dataset 5: [ 0.  0. -0.]\n",
      "\n",
      "Column 'DIAGNOSIS_SUBSTANCE_DISORDER' at positions [7, 9, 13]:\n",
      "  Dataset 1: [0. 0. 0.]\n",
      "  Dataset 2: [0. 0. 0.]\n",
      "  Dataset 3: [-0.  0.  0.]\n",
      "  Dataset 4: [-0. -0.  0.]\n",
      "  Dataset 5: [-0. -0.  0.]\n",
      "\n",
      "Column 'DIAGNOSIS_PSYCHOTIC' at positions [7, 9, 13]:\n",
      "  Dataset 1: [ 0. -0. -0.]\n",
      "  Dataset 2: [ 0. -0.  0.]\n",
      "  Dataset 3: [0. 0. 0.]\n",
      "  Dataset 4: [ 0. -0. -0.]\n",
      "  Dataset 5: [ 1. -1.  0.]\n",
      "\n",
      "Column 'DIAGNOSIS_SUICIDALITY' at positions [53, 211, 246]:\n",
      "  Dataset 1: [-0.  1.  0.]\n",
      "  Dataset 2: [0. 0. 0.]\n",
      "  Dataset 3: [1. 0. 0.]\n",
      "  Dataset 4: [ 1.  0. -0.]\n",
      "  Dataset 5: [1. 1. 0.]\n",
      "\n",
      "Column 'DIAGNOSIS_SEXUAL_TRAUMA' at positions [34, 35, 36]:\n",
      "  Dataset 1: [-0.  1.  0.]\n",
      "  Dataset 2: [1. 0. 0.]\n",
      "  Dataset 3: [1. 1. 1.]\n",
      "  Dataset 4: [0. 1. 1.]\n",
      "  Dataset 5: [0. 0. 1.]\n",
      "\n",
      "Column 'DIAGNOSIS_CPTSD' at positions [8, 65, 106]:\n",
      "  Dataset 1: [-0.  1. -1.]\n",
      "  Dataset 2: [ 1. -0.  0.]\n",
      "  Dataset 3: [1. 1. 0.]\n",
      "  Dataset 4: [ 1.  1. -0.]\n",
      "  Dataset 5: [0. 0. 1.]\n",
      "\n",
      "Column 'treatmentdurationdays' at positions [0, 1, 6]:\n",
      "  Dataset 1: [5. 3. 2.]\n",
      "  Dataset 2: [4. 3. 4.]\n",
      "  Dataset 3: [0. 3. 7.]\n",
      "  Dataset 4: [6. 4. 4.]\n",
      "  Dataset 5: [2. 5. 3.]\n",
      "\n",
      "Column 'Bipolar_and_Mood_disorder' at positions [7, 9, 13]:\n",
      "  Dataset 1: [0. 0. 1.]\n",
      "  Dataset 2: [0. 0. 1.]\n",
      "  Dataset 3: [ 0. -1. -0.]\n",
      "  Dataset 4: [1. 0. 0.]\n",
      "  Dataset 5: [0. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ========== METHOD 1: QUICK CHECK - Compare first 2 datasets ==========\n",
    "def quick_difference_check(imputed_dfs):\n",
    "    \"\"\"Quick check to see if first two datasets are different\"\"\"\n",
    "    if len(imputed_dfs) < 2:\n",
    "        print(\"Need at least 2 datasets to compare\")\n",
    "        return\n",
    "    \n",
    "    df1 = imputed_dfs[0]\n",
    "    df2 = imputed_dfs[1]\n",
    "    \n",
    "    # Check if dataframes are identical\n",
    "    are_identical = df1.equals(df2)\n",
    "    print(f\"Dataset 1 vs Dataset 2: {'IDENTICAL ❌' if are_identical else 'DIFFERENT ✅'}\")\n",
    "    \n",
    "    if not are_identical:\n",
    "        # Count different values\n",
    "        numeric_cols = df1.select_dtypes(include=[np.number]).columns\n",
    "        total_diff = 0\n",
    "        for col in numeric_cols:\n",
    "            diff_count = np.sum(df1[col] != df2[col])\n",
    "            if diff_count > 0:\n",
    "                total_diff += diff_count\n",
    "                print(f\"  '{col}': {diff_count} different values\")\n",
    "        print(f\"  Total different values: {total_diff}\")\n",
    "\n",
    "# ========== METHOD 2: DETAILED CHECK - All pairwise comparisons ==========\n",
    "def detailed_difference_check(imputed_dfs):\n",
    "    \"\"\"Check differences between all pairs of datasets\"\"\"\n",
    "    n_datasets = len(imputed_dfs)\n",
    "    print(f\"\\n=== Checking all {n_datasets} datasets ===\")\n",
    "    \n",
    "    numeric_cols = imputed_dfs[0].select_dtypes(include=[np.number]).columns\n",
    "    \n",
    "    for i in range(n_datasets):\n",
    "        for j in range(i+1, n_datasets):\n",
    "            are_identical = imputed_dfs[i].equals(imputed_dfs[j])\n",
    "            print(f\"Dataset {i+1} vs Dataset {j+1}: {'IDENTICAL ❌' if are_identical else 'DIFFERENT ✅'}\")\n",
    "\n",
    "# ========== METHOD 3: FOCUS ON ORIGINALLY MISSING VALUES ==========\n",
    "def check_missing_value_differences(original_df, imputed_dfs):\n",
    "    \"\"\"Check differences only in originally missing positions\"\"\"\n",
    "    print(f\"\\n=== Checking differences in originally missing positions ===\")\n",
    "    \n",
    "    numeric_cols = original_df.select_dtypes(include=[np.number]).columns\n",
    "    \n",
    "    differences_found = False\n",
    "    for col in numeric_cols:\n",
    "        if original_df[col].isnull().any():\n",
    "            missing_mask = original_df[col].isnull()\n",
    "            print(f\"\\nColumn '{col}' ({missing_mask.sum()} missing values):\")\n",
    "            \n",
    "            # Compare imputed values at missing positions\n",
    "            for i in range(len(imputed_dfs)-1):\n",
    "                imp1_values = imputed_dfs[i].loc[missing_mask, col]\n",
    "                imp2_values = imputed_dfs[i+1].loc[missing_mask, col]\n",
    "                \n",
    "                are_same = np.array_equal(imp1_values.values, imp2_values.values)\n",
    "                if not are_same:\n",
    "                    differences_found = True\n",
    "                    diff_count = np.sum(imp1_values.values != imp2_values.values)\n",
    "                    print(f\"  Dataset {i+1} vs {i+2}: {diff_count}/{len(imp1_values)} different imputed values ✅\")\n",
    "                else:\n",
    "                    print(f\"  Dataset {i+1} vs {i+2}: IDENTICAL imputed values ❌\")\n",
    "    \n",
    "    return differences_found\n",
    "\n",
    "# ========== METHOD 4: SAMPLE VALUES FROM EACH DATASET ==========\n",
    "def show_sample_imputed_values(original_df, imputed_dfs, n_samples=5):\n",
    "    \"\"\"Show sample imputed values from each dataset\"\"\"\n",
    "    print(f\"\\n=== Sample imputed values (first {n_samples} missing positions) ===\")\n",
    "    \n",
    "    numeric_cols = original_df.select_dtypes(include=[np.number]).columns\n",
    "    \n",
    "    for col in numeric_cols:\n",
    "        if original_df[col].isnull().any():\n",
    "            missing_positions = original_df[original_df[col].isnull()].index[:n_samples]\n",
    "            \n",
    "            print(f\"\\nColumn '{col}' at positions {list(missing_positions)}:\")\n",
    "            for i, df_imp in enumerate(imputed_dfs):\n",
    "                values = df_imp.loc[missing_positions, col].values\n",
    "                print(f\"  Dataset {i+1}: {values}\")\n",
    "\n",
    "# ========== RUN ALL CHECKS ==========\n",
    "print(\"=\" * 60)\n",
    "print(\"CHECKING IMPUTATION DIFFERENCES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Method 1: Quick check\n",
    "quick_difference_check(imputed_dfs)\n",
    "\n",
    "# Method 2: All pairwise comparisons  \n",
    "detailed_difference_check(imputed_dfs)\n",
    "\n",
    "# Method 3: Focus on originally missing values (assumes 'df' is your original dataframe)\n",
    "if 'df' in globals():\n",
    "    differences_found = check_missing_value_differences(df, imputed_dfs)\n",
    "    if differences_found:\n",
    "        print(f\"\\n🎉 SUCCESS: Found differences in imputed values!\")\n",
    "    else:\n",
    "        print(f\"\\n⚠️ WARNING: No differences found in imputed values!\")\n",
    "\n",
    "# Method 4: Show sample values\n",
    "if 'df' in globals():\n",
    "    show_sample_imputed_values(df, imputed_dfs, n_samples=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "24870e2f-65b4-4d77-b866-6c0ccdc2e409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y for imputation 1 defined. Sample values:\n",
      "0   -41.0\n",
      "1   -15.0\n",
      "2   -46.0\n",
      "3   -41.0\n",
      "4   -20.0\n",
      "Name: caps5_change_baseline, dtype: float64\n",
      "Y for imputation 2 defined. Sample values:\n",
      "0   -41.0\n",
      "1   -15.0\n",
      "2   -46.0\n",
      "3   -41.0\n",
      "4   -20.0\n",
      "Name: caps5_change_baseline, dtype: float64\n",
      "Y for imputation 3 defined. Sample values:\n",
      "0   -41.0\n",
      "1   -15.0\n",
      "2   -46.0\n",
      "3   -41.0\n",
      "4   -20.0\n",
      "Name: caps5_change_baseline, dtype: float64\n",
      "Y for imputation 4 defined. Sample values:\n",
      "0   -41.0\n",
      "1   -15.0\n",
      "2   -46.0\n",
      "3   -41.0\n",
      "4   -20.0\n",
      "Name: caps5_change_baseline, dtype: float64\n",
      "Y for imputation 5 defined. Sample values:\n",
      "0   -41.0\n",
      "1   -15.0\n",
      "2   -46.0\n",
      "3   -41.0\n",
      "4   -20.0\n",
      "Name: caps5_change_baseline, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "imputed_folder = \"imputed_data\"\n",
    "n_imputations = 5\n",
    "\n",
    "# Lists to hold DataFrames and Y vectors\n",
    "imputed_dfs = []\n",
    "Y_list = []\n",
    "\n",
    "for i in range(1, n_imputations + 1):\n",
    "    file_path = f\"{imputed_folder}/df_imputed_final_imp{i}.pkl\"\n",
    "    \n",
    "    # Load imputed DataFrame\n",
    "    df_imp = pd.read_pickle(file_path)\n",
    "    imputed_dfs.append(df_imp)\n",
    "\n",
    "    # Define Y for this imputation\n",
    "    Y = df_imp[\"caps5_change_baseline\"]\n",
    "    Y_list.append(Y)\n",
    "\n",
    "    print(f\"Y for imputation {i} defined. Sample values:\")\n",
    "    print(Y.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "34bb8f95-ea33-4ba5-89eb-708abf2aa238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Imputed Dataset 1 ===\n",
      "Medication Groups:\n",
      "['CAT_Antidepressiva', 'CAT_Benzodiazepine', 'CAT_Anti_epileptica', 'CAT_Antihistaminica', 'CAT_Opioden', 'CAT_Antipsychotica', 'CAT_Aceetanilidederivaten', 'CAT_Antihypertensiva', 'CAT_Salicylaat', 'CAT_NSAIDs', 'CAT_Migrainemiddelen', 'CAT_ADHD', 'CAT_Anticonceptiva', 'CAT_Z_drugs', 'CAT_Spierrelaxantia', 'CAT_Immunomodulerende_middelen', 'CAT_Alcoholverslaving', 'CAT_Stemmingsstabilisatoren', 'CAT_Parkinson', 'CAT_ALL', 'CAT_ALL_PSYCHOTROPICS', 'CAT_ALL_PSYCHOTROPICS_EXCL_BENZO', 'CAT_ALL_PSYCHOTROPICS_EXCL_SEDATIVES_HYPNOTICS']\n",
      "Total Medication Groups Found: 23\n",
      "\n",
      "=== Imputed Dataset 2 ===\n",
      "Medication Groups:\n",
      "['CAT_Antidepressiva', 'CAT_Benzodiazepine', 'CAT_Anti_epileptica', 'CAT_Antihistaminica', 'CAT_Opioden', 'CAT_Antipsychotica', 'CAT_Aceetanilidederivaten', 'CAT_Antihypertensiva', 'CAT_Salicylaat', 'CAT_NSAIDs', 'CAT_Migrainemiddelen', 'CAT_ADHD', 'CAT_Anticonceptiva', 'CAT_Z_drugs', 'CAT_Spierrelaxantia', 'CAT_Immunomodulerende_middelen', 'CAT_Alcoholverslaving', 'CAT_Stemmingsstabilisatoren', 'CAT_Parkinson', 'CAT_ALL', 'CAT_ALL_PSYCHOTROPICS', 'CAT_ALL_PSYCHOTROPICS_EXCL_BENZO', 'CAT_ALL_PSYCHOTROPICS_EXCL_SEDATIVES_HYPNOTICS']\n",
      "Total Medication Groups Found: 23\n",
      "\n",
      "=== Imputed Dataset 3 ===\n",
      "Medication Groups:\n",
      "['CAT_Antidepressiva', 'CAT_Benzodiazepine', 'CAT_Anti_epileptica', 'CAT_Antihistaminica', 'CAT_Opioden', 'CAT_Antipsychotica', 'CAT_Aceetanilidederivaten', 'CAT_Antihypertensiva', 'CAT_Salicylaat', 'CAT_NSAIDs', 'CAT_Migrainemiddelen', 'CAT_ADHD', 'CAT_Anticonceptiva', 'CAT_Z_drugs', 'CAT_Spierrelaxantia', 'CAT_Immunomodulerende_middelen', 'CAT_Alcoholverslaving', 'CAT_Stemmingsstabilisatoren', 'CAT_Parkinson', 'CAT_ALL', 'CAT_ALL_PSYCHOTROPICS', 'CAT_ALL_PSYCHOTROPICS_EXCL_BENZO', 'CAT_ALL_PSYCHOTROPICS_EXCL_SEDATIVES_HYPNOTICS']\n",
      "Total Medication Groups Found: 23\n",
      "\n",
      "=== Imputed Dataset 4 ===\n",
      "Medication Groups:\n",
      "['CAT_Antidepressiva', 'CAT_Benzodiazepine', 'CAT_Anti_epileptica', 'CAT_Antihistaminica', 'CAT_Opioden', 'CAT_Antipsychotica', 'CAT_Aceetanilidederivaten', 'CAT_Antihypertensiva', 'CAT_Salicylaat', 'CAT_NSAIDs', 'CAT_Migrainemiddelen', 'CAT_ADHD', 'CAT_Anticonceptiva', 'CAT_Z_drugs', 'CAT_Spierrelaxantia', 'CAT_Immunomodulerende_middelen', 'CAT_Alcoholverslaving', 'CAT_Stemmingsstabilisatoren', 'CAT_Parkinson', 'CAT_ALL', 'CAT_ALL_PSYCHOTROPICS', 'CAT_ALL_PSYCHOTROPICS_EXCL_BENZO', 'CAT_ALL_PSYCHOTROPICS_EXCL_SEDATIVES_HYPNOTICS']\n",
      "Total Medication Groups Found: 23\n",
      "\n",
      "=== Imputed Dataset 5 ===\n",
      "Medication Groups:\n",
      "['CAT_Antidepressiva', 'CAT_Benzodiazepine', 'CAT_Anti_epileptica', 'CAT_Antihistaminica', 'CAT_Opioden', 'CAT_Antipsychotica', 'CAT_Aceetanilidederivaten', 'CAT_Antihypertensiva', 'CAT_Salicylaat', 'CAT_NSAIDs', 'CAT_Migrainemiddelen', 'CAT_ADHD', 'CAT_Anticonceptiva', 'CAT_Z_drugs', 'CAT_Spierrelaxantia', 'CAT_Immunomodulerende_middelen', 'CAT_Alcoholverslaving', 'CAT_Stemmingsstabilisatoren', 'CAT_Parkinson', 'CAT_ALL', 'CAT_ALL_PSYCHOTROPICS', 'CAT_ALL_PSYCHOTROPICS_EXCL_BENZO', 'CAT_ALL_PSYCHOTROPICS_EXCL_SEDATIVES_HYPNOTICS']\n",
      "Total Medication Groups Found: 23\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load imputed DataFrames from saved files\n",
    "imputed_folder = \"imputed_data\"\n",
    "n_imputations = 5\n",
    "\n",
    "for i in range(1, n_imputations + 1):\n",
    "    print(f\"\\n=== Imputed Dataset {i} ===\")\n",
    "\n",
    "    # Load each imputed dataset\n",
    "    df_imp = pd.read_pickle(f\"{imputed_folder}/df_imputed_final_imp{i}.pkl\")\n",
    "\n",
    "    # Get all CAT_* columns\n",
    "    cat_columns = [col for col in df_imp.columns if col.startswith('CAT_')]\n",
    "\n",
    "    print(\"Medication Groups:\")\n",
    "    print(cat_columns)\n",
    "    print(\"Total Medication Groups Found:\", len(cat_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1dfbe506-f470-4787-9bda-417a090be3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "covariates_CAT_ADHD = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline', 'CAT_Anti_epileptica', 'CAT_Antidepressiva', 'CAT_Antipsychotica',\n",
    "    'CAT_Benzodiazepine', 'CAT_Opioden',\n",
    "    'CAT_Z_drugs', 'DIAGNOSIS_CHILDHOOD_TRAUMA', 'DIAGNOSIS_CPTSD',\n",
    "    'DIAGNOSIS_SEXUAL_TRAUMA', 'DIAGNOSIS_SMOKING', 'DIAGNOSIS_SUICIDALITY',\n",
    "    'age',\n",
    "    'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"EB_NON_TF_THERAPY\", \"OTHER_TREATM_APPROACH\", \"Bipolar_and_Mood_disorder\", \"ethnicity_Dutch\", \"ethnicity_other\"\n",
    "]\n",
    "\n",
    "\n",
    "covariates_CAT_Aceetanilidederivaten = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline', 'CAT_Anti_epileptica', 'CAT_Antidepressiva', 'CAT_Antipsychotica',\n",
    "    'CAT_Benzodiazepine', 'CAT_Opioden', 'CAT_Z_drugs',\n",
    "    'DIAGNOSIS_CHILDHOOD_TRAUMA', 'DIAGNOSIS_CPTSD', 'DIAGNOSIS_SEXUAL_TRAUMA',\n",
    "    'DIAGNOSIS_SMOKING', 'DIAGNOSIS_SUICIDALITY', 'age',\n",
    "    'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"EB_NON_TF_THERAPY\", \"OTHER_TREATM_APPROACH\", \"Bipolar_and_Mood_disorder\", \"ethnicity_Dutch\", \"ethnicity_other\"\n",
    "]\n",
    "\n",
    "\n",
    "covariates_CAT_Z_drugs = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline', 'CAT_Anti_epileptica', 'CAT_Antidepressiva', 'CAT_Antipsychotica',\n",
    "    'CAT_Benzodiazepine', 'CAT_Opioden',\n",
    "    'DIAGNOSIS_CHILDHOOD_TRAUMA', 'DIAGNOSIS_CPTSD', 'DIAGNOSIS_SEXUAL_TRAUMA',\n",
    "    'DIAGNOSIS_SMOKING', 'DIAGNOSIS_SUICIDALITY', 'age',\n",
    "    'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"EB_NON_TF_THERAPY\", \"OTHER_TREATM_APPROACH\", \"Bipolar_and_Mood_disorder\", \"ethnicity_Dutch\", \"ethnicity_other\"\n",
    "]\n",
    "\n",
    "\n",
    "covariates_CAT_Opioden = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline', 'CAT_Anti_epileptica',\n",
    "    'CAT_Antidepressiva',\n",
    "    'CAT_Antipsychotica', 'CAT_Benzodiazepine', 'CAT_Z_drugs',\n",
    "    'DIAGNOSIS_CHILDHOOD_TRAUMA', 'DIAGNOSIS_CPTSD', 'DIAGNOSIS_SEXUAL_TRAUMA',\n",
    "    'DIAGNOSIS_SMOKING', 'DIAGNOSIS_SUICIDALITY', 'age',\n",
    "    'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"EB_NON_TF_THERAPY\", \"OTHER_TREATM_APPROACH\", \"Bipolar_and_Mood_disorder\", \"ethnicity_Dutch\", \"ethnicity_other\"\n",
    "]\n",
    "\n",
    "\n",
    "covariates_CAT_NSAIDs = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline', 'CAT_Anti_epileptica',\n",
    "    'CAT_Antidepressiva',\n",
    "    'CAT_Antipsychotica', 'CAT_Benzodiazepine', 'CAT_Opioden', 'CAT_Z_drugs',\n",
    "    'DIAGNOSIS_CHILDHOOD_TRAUMA', 'DIAGNOSIS_CPTSD', 'DIAGNOSIS_SEXUAL_TRAUMA',\n",
    "    'DIAGNOSIS_SMOKING', 'DIAGNOSIS_SUICIDALITY', 'age',\n",
    "    'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"EB_NON_TF_THERAPY\", \"OTHER_TREATM_APPROACH\", \"Bipolar_and_Mood_disorder\", \"ethnicity_Dutch\", \"ethnicity_other\"\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "covariates_CAT_Benzodiazepine = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline', 'CAT_Anti_epileptica',\n",
    "    'CAT_Antidepressiva',\n",
    "    'CAT_Antipsychotica', 'CAT_Opioden', 'CAT_Z_drugs',\n",
    "    'DIAGNOSIS_CHILDHOOD_TRAUMA', 'DIAGNOSIS_CPTSD', 'DIAGNOSIS_SEXUAL_TRAUMA',\n",
    "    'DIAGNOSIS_SMOKING', 'DIAGNOSIS_SUICIDALITY', 'age',\n",
    "    'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"EB_NON_TF_THERAPY\", \"OTHER_TREATM_APPROACH\", \"Bipolar_and_Mood_disorder\", \"ethnicity_Dutch\", \"ethnicity_other\"\n",
    "]\n",
    "\n",
    "\n",
    "covariates_CAT_Antihypertensiva = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline', 'CAT_Anti_epileptica',\n",
    "    'CAT_Antidepressiva', 'CAT_Antipsychotica',\n",
    "    'CAT_Benzodiazepine', 'CAT_Opioden', 'CAT_Z_drugs',\n",
    "    'DIAGNOSIS_CHILDHOOD_TRAUMA', 'DIAGNOSIS_CPTSD', 'DIAGNOSIS_SEXUAL_TRAUMA',\n",
    "    'DIAGNOSIS_SMOKING', 'DIAGNOSIS_SUICIDALITY', 'age',\n",
    "    'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"EB_NON_TF_THERAPY\", \"OTHER_TREATM_APPROACH\", \"Bipolar_and_Mood_disorder\", \"ethnicity_Dutch\", \"ethnicity_other\"\n",
    "]\n",
    "\n",
    "\n",
    "covariates_CAT_Antihistaminica = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline', 'CAT_Anti_epileptica',\n",
    "    'CAT_Antidepressiva', 'CAT_Antipsychotica',\n",
    "    'CAT_Benzodiazepine', 'CAT_Opioden', 'CAT_Z_drugs',\n",
    "    'DIAGNOSIS_CHILDHOOD_TRAUMA', 'DIAGNOSIS_CPTSD', 'DIAGNOSIS_SEXUAL_TRAUMA',\n",
    "    'DIAGNOSIS_SMOKING', 'DIAGNOSIS_SUICIDALITY', 'age',\n",
    "    'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"EB_NON_TF_THERAPY\", \"OTHER_TREATM_APPROACH\", \"Bipolar_and_Mood_disorder\"\n",
    "]\n",
    "\n",
    "\n",
    "covariates_CAT_Anti_epileptica = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline', 'CAT_Antidepressiva', 'CAT_Antipsychotica',\n",
    "    'CAT_Benzodiazepine', 'CAT_Opioden', 'CAT_Z_drugs',\n",
    "    'DIAGNOSIS_CHILDHOOD_TRAUMA', 'DIAGNOSIS_CPTSD', 'DIAGNOSIS_SEXUAL_TRAUMA',\n",
    "    'DIAGNOSIS_SMOKING', 'DIAGNOSIS_SUICIDALITY', 'age',\n",
    "    'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"EB_NON_TF_THERAPY\", \"OTHER_TREATM_APPROACH\", \"Bipolar_and_Mood_disorder\", \"ethnicity_Dutch\", \"ethnicity_other\"\n",
    "]\n",
    "\n",
    "\n",
    "covariates_CAT_Antidepressiva = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline', 'CAT_Anti_epileptica', 'CAT_Antipsychotica',\n",
    "    'CAT_Benzodiazepine', 'CAT_Opioden', 'CAT_Z_drugs',\n",
    "    'DIAGNOSIS_CHILDHOOD_TRAUMA', 'DIAGNOSIS_CPTSD', 'DIAGNOSIS_SEXUAL_TRAUMA',\n",
    "    'DIAGNOSIS_SMOKING', 'DIAGNOSIS_SUICIDALITY', 'age',\n",
    "    'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"EB_NON_TF_THERAPY\", \"OTHER_TREATM_APPROACH\", \"Bipolar_and_Mood_disorder\", \"ethnicity_Dutch\", \"ethnicity_other\"\n",
    "]\n",
    "\n",
    "\n",
    "covariates_CAT_Antipsychotica = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline', 'CAT_Anti_epileptica',\n",
    "    'CAT_Antidepressiva',\n",
    "    'CAT_Benzodiazepine', 'CAT_Opioden', 'CAT_Z_drugs',\n",
    "    'DIAGNOSIS_CHILDHOOD_TRAUMA', 'DIAGNOSIS_CPTSD', 'DIAGNOSIS_SEXUAL_TRAUMA',\n",
    "    'DIAGNOSIS_SMOKING', 'DIAGNOSIS_SUICIDALITY', 'age',\n",
    "    'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"EB_NON_TF_THERAPY\", \"OTHER_TREATM_APPROACH\", \"Bipolar_and_Mood_disorder\", \"ethnicity_Dutch\", \"ethnicity_other\"\n",
    "]\n",
    "\n",
    "covariates_CAT_ALL_PSYCHOTROPICS_EXCL_BENZO = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline',\n",
    "    'CAT_Benzodiazepine', 'CAT_Anticonceptiva',\n",
    "    'DIAGNOSIS_CHILDHOOD_TRAUMA', 'DIAGNOSIS_CPTSD', 'DIAGNOSIS_SEXUAL_TRAUMA',\n",
    "    'DIAGNOSIS_SMOKING', 'DIAGNOSIS_SUICIDALITY', 'age',\n",
    "    'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"EB_NON_TF_THERAPY\", \"OTHER_TREATM_APPROACH\", \"Bipolar_and_Mood_disorder\", \"ethnicity_Dutch\", \"ethnicity_other\"\n",
    "]\n",
    "\n",
    "covariates_CAT_ALL_PSYCHOTROPICS_EXCL_SEDATIVES_HYPNOTICS = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline',\n",
    "    'CAT_Benzodiazepine', 'CAT_Z_drugs', 'CAT_Anticonceptiva',\n",
    "    'DIAGNOSIS_CHILDHOOD_TRAUMA', 'DIAGNOSIS_CPTSD', 'DIAGNOSIS_SEXUAL_TRAUMA',\n",
    "    'DIAGNOSIS_SMOKING', 'DIAGNOSIS_SUICIDALITY', 'age',\n",
    "    'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"EB_NON_TF_THERAPY\", \"OTHER_TREATM_APPROACH\", \"Bipolar_and_Mood_disorder\", \"ethnicity_Dutch\", \"ethnicity_other\"\n",
    "]\n",
    "\n",
    "\n",
    "covariates_CAT_ALL_PSYCHOTROPICS = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline',\n",
    "    'DIAGNOSIS_CHILDHOOD_TRAUMA', 'DIAGNOSIS_CPTSD', 'DIAGNOSIS_SEXUAL_TRAUMA',\n",
    "    'DIAGNOSIS_SMOKING', 'DIAGNOSIS_SUICIDALITY', 'age',\n",
    "    'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"EB_NON_TF_THERAPY\", \"OTHER_TREATM_APPROACH\", \"Bipolar_and_Mood_disorder\", \"ethnicity_Dutch\", \"ethnicity_other\"\n",
    "]\n",
    "\n",
    "\n",
    "covariates_CAT_ALL = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline',\n",
    "    'DIAGNOSIS_CHILDHOOD_TRAUMA',\n",
    "    'DIAGNOSIS_CPTSD', 'DIAGNOSIS_SEXUAL_TRAUMA',\n",
    "    'DIAGNOSIS_SMOKING', 'DIAGNOSIS_SUICIDALITY',\n",
    "    'age', 'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"EB_NON_TF_THERAPY\", \"OTHER_TREATM_APPROACH\", \"Bipolar_and_Mood_disorder\", \"ethnicity_Dutch\", \"ethnicity_other\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "85b6d585-f09c-4502-b467-7a8c20a8fe8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Groups found: ['CAT_ADHD', 'CAT_Aceetanilidederivaten', 'CAT_Z_drugs', 'CAT_Opioden', 'CAT_NSAIDs', 'CAT_Benzodiazepine', 'CAT_Antihypertensiva', 'CAT_Antihistaminica', 'CAT_Anti_epileptica', 'CAT_Antidepressiva', 'CAT_Antipsychotica', 'CAT_ALL_PSYCHOTROPICS_EXCL_BENZO', 'CAT_ALL_PSYCHOTROPICS_EXCL_SEDATIVES_HYPNOTICS', 'CAT_ALL_PSYCHOTROPICS', 'CAT_ALL']\n",
      "['CAT_ADHD', 'CAT_Aceetanilidederivaten', 'CAT_Z_drugs', 'CAT_Opioden', 'CAT_NSAIDs', 'CAT_Benzodiazepine', 'CAT_Antihypertensiva', 'CAT_Antihistaminica', 'CAT_Anti_epileptica', 'CAT_Antidepressiva', 'CAT_Antipsychotica', 'CAT_ALL_PSYCHOTROPICS_EXCL_BENZO', 'CAT_ALL_PSYCHOTROPICS_EXCL_SEDATIVES_HYPNOTICS', 'CAT_ALL_PSYCHOTROPICS', 'CAT_ALL']\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# This finds all variables that start with covariates_CAT_ or covariates_cat_\n",
    "final_covariates_map = defaultdict(list)\n",
    "final_covariates_map.update({\n",
    "    var.replace(\"covariates_\", \"\"): val\n",
    "    for var, val in globals().items()\n",
    "    if var.lower().startswith(\"covariates_cat_\") and isinstance(val, list)\n",
    "})\n",
    "\n",
    "# Show detected group names\n",
    "print(\"Groups found:\", list(final_covariates_map.keys()))\n",
    "medication_groups = list(final_covariates_map.keys())\n",
    "print(medication_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a5d938d8-9224-4db0-b753-03d15061db06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Starting analysis for all CAT groups\n",
      "\n",
      " Processing CAT_Adhd...\n",
      "  → Using imputation 1\n",
      "  → Using imputation 2\n",
      "  → Using imputation 3\n",
      "  → Using imputation 4\n",
      "  → Using imputation 5\n",
      " Done: CAT_Adhd\n",
      "\n",
      " Processing CAT_Aceetanilidederivaten...\n",
      "  → Using imputation 1\n",
      "  → Using imputation 2\n",
      "  → Using imputation 3\n",
      "  → Using imputation 4\n",
      "  → Using imputation 5\n",
      " Done: CAT_Aceetanilidederivaten\n",
      "\n",
      " Processing CAT_Z_Drugs...\n",
      "  → Using imputation 1\n",
      "  → Using imputation 2\n",
      "  → Using imputation 3\n",
      "  → Using imputation 4\n",
      "  → Using imputation 5\n",
      " Done: CAT_Z_Drugs\n",
      "\n",
      " Processing CAT_Opioden...\n",
      "  → Using imputation 1\n",
      "  → Using imputation 2\n",
      "  → Using imputation 3\n",
      "  → Using imputation 4\n",
      "  → Using imputation 5\n",
      " Done: CAT_Opioden\n",
      "\n",
      " Processing CAT_Nsaids...\n",
      "  → Using imputation 1\n",
      "  → Using imputation 2\n",
      "  → Using imputation 3\n",
      "  → Using imputation 4\n",
      "  → Using imputation 5\n",
      " Done: CAT_Nsaids\n",
      "\n",
      " Processing CAT_Benzodiazepine...\n",
      "  → Using imputation 1\n",
      "  → Using imputation 2\n",
      "  → Using imputation 3\n",
      "  → Using imputation 4\n",
      "  → Using imputation 5\n",
      " Done: CAT_Benzodiazepine\n",
      "\n",
      " Processing CAT_Antihypertensiva...\n",
      "  → Using imputation 1\n",
      "  → Using imputation 2\n",
      "  → Using imputation 3\n",
      "  → Using imputation 4\n",
      "  → Using imputation 5\n",
      " Done: CAT_Antihypertensiva\n",
      "\n",
      " Processing CAT_Antihistaminica...\n",
      "  → Using imputation 1\n",
      "  → Using imputation 2\n",
      "  → Using imputation 3\n",
      "  → Using imputation 4\n",
      "  → Using imputation 5\n",
      " Done: CAT_Antihistaminica\n",
      "\n",
      " Processing CAT_Anti_Epileptica...\n",
      "  → Using imputation 1\n",
      "  → Using imputation 2\n",
      "  → Using imputation 3\n",
      "  → Using imputation 4\n",
      "  → Using imputation 5\n",
      " Done: CAT_Anti_Epileptica\n",
      "\n",
      " Processing CAT_Antidepressiva...\n",
      "  → Using imputation 1\n",
      "  → Using imputation 2\n",
      "  → Using imputation 3\n",
      "  → Using imputation 4\n",
      "  → Using imputation 5\n",
      " Done: CAT_Antidepressiva\n",
      "\n",
      " Processing CAT_Antipsychotica...\n",
      "  → Using imputation 1\n",
      "  → Using imputation 2\n",
      "  → Using imputation 3\n",
      "  → Using imputation 4\n",
      "  → Using imputation 5\n",
      " Done: CAT_Antipsychotica\n",
      "\n",
      " Processing CAT_All_Psychotropics_Excl_Benzo...\n",
      "  → Using imputation 1\n",
      "  → Using imputation 2\n",
      "  → Using imputation 3\n",
      "  → Using imputation 4\n",
      "  → Using imputation 5\n",
      " Done: CAT_All_Psychotropics_Excl_Benzo\n",
      "\n",
      " Processing CAT_All_Psychotropics_Excl_Sedatives_Hypnotics...\n",
      "  → Using imputation 1\n",
      "  → Using imputation 2\n",
      "  → Using imputation 3\n",
      "  → Using imputation 4\n",
      "  → Using imputation 5\n",
      " Done: CAT_All_Psychotropics_Excl_Sedatives_Hypnotics\n",
      "\n",
      " Processing CAT_All_Psychotropics...\n",
      "  → Using imputation 1\n",
      "  → Using imputation 2\n",
      "  → Using imputation 3\n",
      "  → Using imputation 4\n",
      "  → Using imputation 5\n",
      " Done: CAT_All_Psychotropics\n",
      "\n",
      " Processing CAT_All...\n",
      "  → Using imputation 1\n",
      "  → Using imputation 2\n",
      "  → Using imputation 3\n",
      "  → Using imputation 4\n",
      "  → Using imputation 5\n",
      " Done: CAT_All\n",
      "\n",
      " All CAT group analyses complete.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "def run_all_CAT_group_models(imputed_dfs):\n",
    "    \"\"\"\n",
    "    Runs downstream analysis for each CAT medication group using imputed datasets.\n",
    "\n",
    "    Parameters:\n",
    "    - imputed_dfs: list of 5 imputed DataFrames (from df_imputed_final_imp1.pkl ... imp5.pkl)\n",
    "    \n",
    "    Notes:\n",
    "    - Covariate lists must be defined as global variables: covariates_cat_<group>\n",
    "    - Outputs are saved in: outputs/CAT_<GROUP>/\n",
    "    \"\"\"\n",
    "\n",
    "    print(\" Starting analysis for all CAT groups\")\n",
    "\n",
    "    for var_name in globals():\n",
    "        if var_name.lower().startswith(\"covariates_cat_\") and isinstance(globals()[var_name], list):\n",
    "            group_name = var_name.replace(\"covariates_\", \"\")\n",
    "            group_name = group_name.replace(\"_\", \" \").title().replace(\" \", \"_\")  # e.g., cat_z_drugs → Cat_Z_Drugs\n",
    "            group_name = group_name.replace(\"Cat_\", \"CAT_\")  # force prefix to uppercase\n",
    "\n",
    "            covariates = globals()[var_name]\n",
    "            output_dir = f\"outputs/{group_name}\"\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "            print(f\"\\n Processing {group_name}...\")\n",
    "\n",
    "            for k, df_imp in enumerate(imputed_dfs):\n",
    "                print(f\"  → Using imputation {k+1}\")\n",
    "\n",
    "                # Define X and Y\n",
    "                X = df_imp[covariates]\n",
    "                Y = df_imp[\"caps5_change_baseline\"]\n",
    "\n",
    "                # === Save X and Y as placeholder (replace with modeling later)\n",
    "                X.to_csv(f\"{output_dir}/X_imp{k+1}.csv\", index=False)\n",
    "                Y.to_frame(name=\"Y\").to_csv(f\"{output_dir}/Y_imp{k+1}.csv\", index=False)\n",
    "\n",
    "            print(f\" Done: {group_name}\")\n",
    "\n",
    "    print(\"\\n All CAT group analyses complete.\")\n",
    "\n",
    "# ========= STEP 4: Execute ========= #\n",
    "run_all_CAT_group_models(imputed_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6241fae9-50ab-4dfa-a004-474392a4f6ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " CAT_ADHD\n",
      "  Imp 1: Treated = 88, Control = 6037, Missing = 0\n",
      "  Imp 2: Treated = 88, Control = 6037, Missing = 0\n",
      "  Imp 3: Treated = 88, Control = 6037, Missing = 0\n",
      "  Imp 4: Treated = 88, Control = 6037, Missing = 0\n",
      "  Imp 5: Treated = 88, Control = 6037, Missing = 0\n",
      "\n",
      " CAT_Aceetanilidederivaten\n",
      "  Imp 1: Treated = 77, Control = 6048, Missing = 0\n",
      "  Imp 2: Treated = 77, Control = 6048, Missing = 0\n",
      "  Imp 3: Treated = 77, Control = 6048, Missing = 0\n",
      "  Imp 4: Treated = 77, Control = 6048, Missing = 0\n",
      "  Imp 5: Treated = 77, Control = 6048, Missing = 0\n",
      "\n",
      " CAT_Z_drugs\n",
      "  Imp 1: Treated = 89, Control = 6036, Missing = 0\n",
      "  Imp 2: Treated = 89, Control = 6036, Missing = 0\n",
      "  Imp 3: Treated = 89, Control = 6036, Missing = 0\n",
      "  Imp 4: Treated = 89, Control = 6036, Missing = 0\n",
      "  Imp 5: Treated = 89, Control = 6036, Missing = 0\n",
      "\n",
      " CAT_Opioden\n",
      "  Imp 1: Treated = 79, Control = 6046, Missing = 0\n",
      "  Imp 2: Treated = 79, Control = 6046, Missing = 0\n",
      "  Imp 3: Treated = 79, Control = 6046, Missing = 0\n",
      "  Imp 4: Treated = 79, Control = 6046, Missing = 0\n",
      "  Imp 5: Treated = 79, Control = 6046, Missing = 0\n",
      "\n",
      " CAT_NSAIDs\n",
      "  Imp 1: Treated = 52, Control = 6073, Missing = 0\n",
      "  Imp 2: Treated = 52, Control = 6073, Missing = 0\n",
      "  Imp 3: Treated = 52, Control = 6073, Missing = 0\n",
      "  Imp 4: Treated = 52, Control = 6073, Missing = 0\n",
      "  Imp 5: Treated = 52, Control = 6073, Missing = 0\n",
      "\n",
      " CAT_Benzodiazepine\n",
      "  Imp 1: Treated = 563, Control = 5562, Missing = 0\n",
      "  Imp 2: Treated = 563, Control = 5562, Missing = 0\n",
      "  Imp 3: Treated = 563, Control = 5562, Missing = 0\n",
      "  Imp 4: Treated = 563, Control = 5562, Missing = 0\n",
      "  Imp 5: Treated = 563, Control = 5562, Missing = 0\n",
      "\n",
      " CAT_Antihypertensiva\n",
      "  Imp 1: Treated = 74, Control = 6051, Missing = 0\n",
      "  Imp 2: Treated = 74, Control = 6051, Missing = 0\n",
      "  Imp 3: Treated = 74, Control = 6051, Missing = 0\n",
      "  Imp 4: Treated = 74, Control = 6051, Missing = 0\n",
      "  Imp 5: Treated = 74, Control = 6051, Missing = 0\n",
      "\n",
      " CAT_Antihistaminica\n",
      "  Imp 1: Treated = 86, Control = 6039, Missing = 0\n",
      "  Imp 2: Treated = 86, Control = 6039, Missing = 0\n",
      "  Imp 3: Treated = 86, Control = 6039, Missing = 0\n",
      "  Imp 4: Treated = 86, Control = 6039, Missing = 0\n",
      "  Imp 5: Treated = 86, Control = 6039, Missing = 0\n",
      "\n",
      " CAT_Anti_epileptica\n",
      "  Imp 1: Treated = 121, Control = 6004, Missing = 0\n",
      "  Imp 2: Treated = 121, Control = 6004, Missing = 0\n",
      "  Imp 3: Treated = 121, Control = 6004, Missing = 0\n",
      "  Imp 4: Treated = 121, Control = 6004, Missing = 0\n",
      "  Imp 5: Treated = 121, Control = 6004, Missing = 0\n",
      "\n",
      " CAT_Antidepressiva\n",
      "  Imp 1: Treated = 883, Control = 5242, Missing = 0\n",
      "  Imp 2: Treated = 883, Control = 5242, Missing = 0\n",
      "  Imp 3: Treated = 883, Control = 5242, Missing = 0\n",
      "  Imp 4: Treated = 883, Control = 5242, Missing = 0\n",
      "  Imp 5: Treated = 883, Control = 5242, Missing = 0\n",
      "\n",
      " CAT_Antipsychotica\n",
      "  Imp 1: Treated = 372, Control = 5753, Missing = 0\n",
      "  Imp 2: Treated = 372, Control = 5753, Missing = 0\n",
      "  Imp 3: Treated = 372, Control = 5753, Missing = 0\n",
      "  Imp 4: Treated = 372, Control = 5753, Missing = 0\n",
      "  Imp 5: Treated = 372, Control = 5753, Missing = 0\n",
      "\n",
      " CAT_ALL_PSYCHOTROPICS_EXCL_BENZO\n",
      "  Imp 1: Treated = 1213, Control = 4912, Missing = 0\n",
      "  Imp 2: Treated = 1213, Control = 4912, Missing = 0\n",
      "  Imp 3: Treated = 1213, Control = 4912, Missing = 0\n",
      "  Imp 4: Treated = 1213, Control = 4912, Missing = 0\n",
      "  Imp 5: Treated = 1213, Control = 4912, Missing = 0\n",
      "\n",
      " CAT_ALL_PSYCHOTROPICS_EXCL_SEDATIVES_HYPNOTICS\n",
      "  Imp 1: Treated = 1181, Control = 4944, Missing = 0\n",
      "  Imp 2: Treated = 1181, Control = 4944, Missing = 0\n",
      "  Imp 3: Treated = 1181, Control = 4944, Missing = 0\n",
      "  Imp 4: Treated = 1181, Control = 4944, Missing = 0\n",
      "  Imp 5: Treated = 1181, Control = 4944, Missing = 0\n",
      "\n",
      " CAT_ALL_PSYCHOTROPICS\n",
      "  Imp 1: Treated = 1444, Control = 4681, Missing = 0\n",
      "  Imp 2: Treated = 1444, Control = 4681, Missing = 0\n",
      "  Imp 3: Treated = 1444, Control = 4681, Missing = 0\n",
      "  Imp 4: Treated = 1444, Control = 4681, Missing = 0\n",
      "  Imp 5: Treated = 1444, Control = 4681, Missing = 0\n",
      "\n",
      " CAT_ALL\n",
      "  Imp 1: Treated = 1510, Control = 4615, Missing = 0\n",
      "  Imp 2: Treated = 1510, Control = 4615, Missing = 0\n",
      "  Imp 3: Treated = 1510, Control = 4615, Missing = 0\n",
      "  Imp 4: Treated = 1510, Control = 4615, Missing = 0\n",
      "  Imp 5: Treated = 1510, Control = 4615, Missing = 0\n"
     ]
    }
   ],
   "source": [
    "for treatment_var in medication_groups:\n",
    "    print(f\"\\n {treatment_var}\")\n",
    "    \n",
    "    for i, df in enumerate(imputed_dfs):\n",
    "        if treatment_var not in df.columns:\n",
    "            print(f\"  Imp {i+1}:  Not found in columns.\")\n",
    "            continue\n",
    "\n",
    "        treated = (df[treatment_var] == 1).sum()\n",
    "        control = (df[treatment_var] == 0).sum()\n",
    "        missing = df[treatment_var].isna().sum()\n",
    "\n",
    "        print(f\"  Imp {i+1}: Treated = {treated}, Control = {control}, Missing = {missing}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "42eeaea9-4e28-4afc-9c6d-201f5af8a3be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Processing VIF for CAT_ADHD\n",
      " ✅ Saved: outputs\\CAT_ADHD/pooled_vif.csv\n",
      "\n",
      "🔍 Processing VIF for CAT_Aceetanilidederivaten\n",
      " ✅ Saved: outputs\\CAT_Aceetanilidederivaten/pooled_vif.csv\n",
      "\n",
      "🔍 Processing VIF for CAT_Z_drugs\n",
      " ✅ Saved: outputs\\CAT_Z_drugs/pooled_vif.csv\n",
      "\n",
      "🔍 Processing VIF for CAT_Opioden\n",
      " ✅ Saved: outputs\\CAT_Opioden/pooled_vif.csv\n",
      "\n",
      "🔍 Processing VIF for CAT_NSAIDs\n",
      " ✅ Saved: outputs\\CAT_NSAIDs/pooled_vif.csv\n",
      "\n",
      "🔍 Processing VIF for CAT_Benzodiazepine\n",
      " ✅ Saved: outputs\\CAT_Benzodiazepine/pooled_vif.csv\n",
      "\n",
      "🔍 Processing VIF for CAT_Antihypertensiva\n",
      " ✅ Saved: outputs\\CAT_Antihypertensiva/pooled_vif.csv\n",
      "\n",
      "🔍 Processing VIF for CAT_Antihistaminica\n",
      " ✅ Saved: outputs\\CAT_Antihistaminica/pooled_vif.csv\n",
      "\n",
      "🔍 Processing VIF for CAT_Anti_epileptica\n",
      " ✅ Saved: outputs\\CAT_Anti_epileptica/pooled_vif.csv\n",
      "\n",
      "🔍 Processing VIF for CAT_Antidepressiva\n",
      " ✅ Saved: outputs\\CAT_Antidepressiva/pooled_vif.csv\n",
      "\n",
      "🔍 Processing VIF for CAT_Antipsychotica\n",
      " ✅ Saved: outputs\\CAT_Antipsychotica/pooled_vif.csv\n",
      "\n",
      "🔍 Processing VIF for CAT_ALL_PSYCHOTROPICS_EXCL_BENZO\n",
      " ✅ Saved: outputs\\CAT_ALL_PSYCHOTROPICS_EXCL_BENZO/pooled_vif.csv\n",
      "\n",
      "🔍 Processing VIF for CAT_ALL_PSYCHOTROPICS_EXCL_SEDATIVES_HYPNOTICS\n",
      " ✅ Saved: outputs\\CAT_ALL_PSYCHOTROPICS_EXCL_SEDATIVES_HYPNOTICS/pooled_vif.csv\n",
      "\n",
      "🔍 Processing VIF for CAT_ALL_PSYCHOTROPICS\n",
      " ✅ Saved: outputs\\CAT_ALL_PSYCHOTROPICS/pooled_vif.csv\n",
      "\n",
      "🔍 Processing VIF for CAT_ALL\n",
      " ✅ Saved: outputs\\CAT_ALL/pooled_vif.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from collections import defaultdict\n",
    "\n",
    "# ✅ VIF computation function\n",
    "def compute_vif(X):\n",
    "    X = sm.add_constant(X, has_constant='add')\n",
    "    vif_df = pd.DataFrame()\n",
    "    vif_df[\"variable\"] = X.columns\n",
    "    vif_df[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "    return vif_df\n",
    "\n",
    "# ✅ Process each group\n",
    "for group in medication_groups:\n",
    "    print(f\"\\n🔍 Processing VIF for {group}\")\n",
    "\n",
    "    if group not in final_covariates_map:\n",
    "        print(f\" ⚠️ No covariates found for {group}. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    covariates = final_covariates_map[group]\n",
    "    vif_list = []\n",
    "\n",
    "    for i, df_imp in enumerate(imputed_dfs):\n",
    "        try:\n",
    "            X = df_imp[covariates].copy()\n",
    "            vif_df = compute_vif(X)\n",
    "            vif_df[\"imputation\"] = i + 1\n",
    "            vif_list.append(vif_df)\n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ Failed on imputation {i+1} for {group}: {e}\")\n",
    "\n",
    "    if vif_list:\n",
    "        all_vif = pd.concat(vif_list)\n",
    "        pooled_vif = all_vif.groupby(\"variable\")[\"VIF\"].mean().reset_index()\n",
    "        pooled_vif = pooled_vif.sort_values(by=\"VIF\", ascending=False)\n",
    "\n",
    "        output_folder = os.path.join(\"outputs\", group)\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "        pooled_vif.to_csv(os.path.join(output_folder, \"pooled_vif.csv\"), index=False)\n",
    "\n",
    "        print(f\" ✅ Saved: {output_folder}/pooled_vif.csv\")\n",
    "    else:\n",
    "        print(f\" ⚠️ Skipped {group}: No valid imputations.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dc6c0f67-a42e-4156-8916-137f0284544a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Running PS estimation for CAT_ADHD\n",
      "   Imp 1: AUC = 0.722, ROC saved.\n",
      "   Imp 2: AUC = 0.717, ROC saved.\n",
      "   Imp 3: AUC = 0.710, ROC saved.\n",
      "   Imp 4: AUC = 0.730, ROC saved.\n",
      "   Imp 5: AUC = 0.732, ROC saved.\n",
      " Composite PS + AUC saved for CAT_ADHD\n",
      " Running PS estimation for CAT_Aceetanilidederivaten\n",
      "   Imp 1: AUC = 0.858, ROC saved.\n",
      "   Imp 2: AUC = 0.868, ROC saved.\n",
      "   Imp 3: AUC = 0.861, ROC saved.\n",
      "   Imp 4: AUC = 0.862, ROC saved.\n",
      "   Imp 5: AUC = 0.879, ROC saved.\n",
      " Composite PS + AUC saved for CAT_Aceetanilidederivaten\n",
      " Running PS estimation for CAT_Z_drugs\n",
      "   Imp 1: AUC = 0.893, ROC saved.\n",
      "   Imp 2: AUC = 0.886, ROC saved.\n",
      "   Imp 3: AUC = 0.890, ROC saved.\n",
      "   Imp 4: AUC = 0.885, ROC saved.\n",
      "   Imp 5: AUC = 0.883, ROC saved.\n",
      " Composite PS + AUC saved for CAT_Z_drugs\n",
      " Running PS estimation for CAT_Opioden\n",
      "   Imp 1: AUC = 0.832, ROC saved.\n",
      "   Imp 2: AUC = 0.842, ROC saved.\n",
      "   Imp 3: AUC = 0.842, ROC saved.\n",
      "   Imp 4: AUC = 0.857, ROC saved.\n",
      "   Imp 5: AUC = 0.839, ROC saved.\n",
      " Composite PS + AUC saved for CAT_Opioden\n",
      " Running PS estimation for CAT_NSAIDs\n",
      "   Imp 1: AUC = 0.877, ROC saved.\n",
      "   Imp 2: AUC = 0.862, ROC saved.\n",
      "   Imp 3: AUC = 0.862, ROC saved.\n",
      "   Imp 4: AUC = 0.852, ROC saved.\n",
      "   Imp 5: AUC = 0.833, ROC saved.\n",
      " Composite PS + AUC saved for CAT_NSAIDs\n",
      " Running PS estimation for CAT_Benzodiazepine\n",
      "   Imp 1: AUC = 0.851, ROC saved.\n",
      "   Imp 2: AUC = 0.850, ROC saved.\n",
      "   Imp 3: AUC = 0.852, ROC saved.\n",
      "   Imp 4: AUC = 0.853, ROC saved.\n",
      "   Imp 5: AUC = 0.851, ROC saved.\n",
      " Composite PS + AUC saved for CAT_Benzodiazepine\n",
      " Running PS estimation for CAT_Antihypertensiva\n",
      "   Imp 1: AUC = 0.904, ROC saved.\n",
      "   Imp 2: AUC = 0.893, ROC saved.\n",
      "   Imp 3: AUC = 0.901, ROC saved.\n",
      "   Imp 4: AUC = 0.899, ROC saved.\n",
      "   Imp 5: AUC = 0.902, ROC saved.\n",
      " Composite PS + AUC saved for CAT_Antihypertensiva\n",
      " Running PS estimation for CAT_Antihistaminica\n",
      "   Imp 1: AUC = 0.903, ROC saved.\n",
      "   Imp 2: AUC = 0.900, ROC saved.\n",
      "   Imp 3: AUC = 0.910, ROC saved.\n",
      "   Imp 4: AUC = 0.909, ROC saved.\n",
      "   Imp 5: AUC = 0.894, ROC saved.\n",
      " Composite PS + AUC saved for CAT_Antihistaminica\n",
      " Running PS estimation for CAT_Anti_epileptica\n",
      "   Imp 1: AUC = 0.856, ROC saved.\n",
      "   Imp 2: AUC = 0.844, ROC saved.\n",
      "   Imp 3: AUC = 0.859, ROC saved.\n",
      "   Imp 4: AUC = 0.842, ROC saved.\n",
      "   Imp 5: AUC = 0.845, ROC saved.\n",
      " Composite PS + AUC saved for CAT_Anti_epileptica\n",
      " Running PS estimation for CAT_Antidepressiva\n",
      "   Imp 1: AUC = 0.865, ROC saved.\n",
      "   Imp 2: AUC = 0.863, ROC saved.\n",
      "   Imp 3: AUC = 0.865, ROC saved.\n",
      "   Imp 4: AUC = 0.865, ROC saved.\n",
      "   Imp 5: AUC = 0.864, ROC saved.\n",
      " Composite PS + AUC saved for CAT_Antidepressiva\n",
      " Running PS estimation for CAT_Antipsychotica\n",
      "   Imp 1: AUC = 0.901, ROC saved.\n",
      "   Imp 2: AUC = 0.899, ROC saved.\n",
      "   Imp 3: AUC = 0.897, ROC saved.\n",
      "   Imp 4: AUC = 0.900, ROC saved.\n",
      "   Imp 5: AUC = 0.900, ROC saved.\n",
      " Composite PS + AUC saved for CAT_Antipsychotica\n",
      " Running PS estimation for CAT_ALL_PSYCHOTROPICS_EXCL_BENZO\n",
      "   Imp 1: AUC = 0.859, ROC saved.\n",
      "   Imp 2: AUC = 0.858, ROC saved.\n",
      "   Imp 3: AUC = 0.859, ROC saved.\n",
      "   Imp 4: AUC = 0.860, ROC saved.\n",
      "   Imp 5: AUC = 0.860, ROC saved.\n",
      " Composite PS + AUC saved for CAT_ALL_PSYCHOTROPICS_EXCL_BENZO\n",
      " Running PS estimation for CAT_ALL_PSYCHOTROPICS_EXCL_SEDATIVES_HYPNOTICS\n",
      "   Imp 1: AUC = 0.873, ROC saved.\n",
      "   Imp 2: AUC = 0.871, ROC saved.\n",
      "   Imp 3: AUC = 0.871, ROC saved.\n",
      "   Imp 4: AUC = 0.873, ROC saved.\n",
      "   Imp 5: AUC = 0.872, ROC saved.\n",
      " Composite PS + AUC saved for CAT_ALL_PSYCHOTROPICS_EXCL_SEDATIVES_HYPNOTICS\n",
      " Running PS estimation for CAT_ALL_PSYCHOTROPICS\n",
      "   Imp 1: AUC = 0.866, ROC saved.\n",
      "   Imp 2: AUC = 0.865, ROC saved.\n",
      "   Imp 3: AUC = 0.865, ROC saved.\n",
      "   Imp 4: AUC = 0.866, ROC saved.\n",
      "   Imp 5: AUC = 0.866, ROC saved.\n",
      " Composite PS + AUC saved for CAT_ALL_PSYCHOTROPICS\n",
      " Running PS estimation for CAT_ALL\n",
      "   Imp 1: AUC = 0.872, ROC saved.\n",
      "   Imp 2: AUC = 0.871, ROC saved.\n",
      "   Imp 3: AUC = 0.872, ROC saved.\n",
      "   Imp 4: AUC = 0.872, ROC saved.\n",
      "   Imp 5: AUC = 0.872, ROC saved.\n",
      " Composite PS + AUC saved for CAT_ALL\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ---------- PS Estimation Function ----------\n",
    "def run_logistic_ps_modeling(imputed_dfs, medication_groups, final_covariates_map):\n",
    "    for group in medication_groups:\n",
    "        print(f\" Running PS estimation for {group}\")\n",
    "\n",
    "        if group not in final_covariates_map:\n",
    "            print(f\" No covariates found for {group}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        output_folder = os.path.join(\"outputs\", group)\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "        covariates = final_covariates_map[group]\n",
    "        ps_matrix = pd.DataFrame()\n",
    "        auc_list = []\n",
    "\n",
    "        for i, df_imp in enumerate(imputed_dfs):\n",
    "            if group not in df_imp.columns:\n",
    "                print(f\" {group} not found in imputed dataset {i+1}. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            X = df_imp[covariates].copy()\n",
    "            T = df_imp[group]\n",
    "\n",
    "            # Drop missing treatment rows\n",
    "            valid_idx = T.dropna().index\n",
    "            X = X.loc[valid_idx]\n",
    "            T = T.loc[valid_idx]\n",
    "\n",
    "            # Train-test split for ROC\n",
    "            X_train, X_test, T_train, T_test = train_test_split(\n",
    "                X, T, stratify=T, test_size=0.3, random_state=42\n",
    "            )\n",
    "\n",
    "            try:\n",
    "                model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "                model.fit(X_train, T_train)\n",
    "\n",
    "                ps_scores = model.predict_proba(X)[:, 1]\n",
    "                ps_matrix[f\"ps_imp{i+1}\"] = pd.Series(ps_scores, index=valid_idx)\n",
    "\n",
    "                # ROC & AUC\n",
    "                auc = roc_auc_score(T_test, model.predict_proba(X_test)[:, 1])\n",
    "                auc_list.append(auc)\n",
    "\n",
    "                fpr, tpr, _ = roc_curve(T_test, model.predict_proba(X_test)[:, 1])\n",
    "                plt.figure()\n",
    "                plt.plot(fpr, tpr, label=f\"AUC = {auc:.3f}\")\n",
    "                plt.plot([0, 1], [0, 1], 'k--')\n",
    "                plt.xlabel(\"False Positive Rate\")\n",
    "                plt.ylabel(\"True Positive Rate\")\n",
    "                plt.title(f\"ROC Curve - {group} (Imp {i+1})\")\n",
    "                plt.legend()\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(os.path.join(output_folder, f\"roc_curve_imp{i+1}.png\"))\n",
    "                plt.close()\n",
    "                print(f\"   Imp {i+1}: AUC = {auc:.3f}, ROC saved.\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"   Error in {group} (imp {i+1}): {e}\")\n",
    "\n",
    "        # Save AUCs and Composite PS\n",
    "        if not ps_matrix.empty:\n",
    "            # Fill NaN rows (from dropped subjects in some imputations) with mean\n",
    "            ps_matrix[\"composite_ps\"] = ps_matrix.mean(axis=1)\n",
    "            ps_matrix.to_excel(os.path.join(output_folder, \"propensity_scores.xlsx\"))\n",
    "\n",
    "            auc_df = pd.DataFrame({\n",
    "                \"imputation\": [f\"imp{i+1}\" for i in range(len(auc_list))],\n",
    "                \"AUC\": auc_list\n",
    "            })\n",
    "            auc_df.loc[len(auc_df.index)] = [\"mean\", np.mean(auc_list) if auc_list else np.nan]\n",
    "            auc_df.to_excel(os.path.join(output_folder, \"auc_scores.xlsx\"), index=False)\n",
    "\n",
    "            print(f\" Composite PS + AUC saved for {group}\")\n",
    "        else:\n",
    "            print(f\" No valid PS scores generated for {group}\")\n",
    "\n",
    "# ---------- Run ----------\n",
    "run_logistic_ps_modeling(imputed_dfs, medication_groups, final_covariates_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "44cdd960-01e8-4562-ae13-a8e282fcd019",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Computing feature importance for CAT_ADHD\n",
      " Saved feature importance plot and CSV for CAT_ADHD\n",
      "\n",
      " Computing feature importance for CAT_Aceetanilidederivaten\n",
      " Saved feature importance plot and CSV for CAT_Aceetanilidederivaten\n",
      "\n",
      " Computing feature importance for CAT_Z_drugs\n",
      " Saved feature importance plot and CSV for CAT_Z_drugs\n",
      "\n",
      " Computing feature importance for CAT_Opioden\n",
      " Saved feature importance plot and CSV for CAT_Opioden\n",
      "\n",
      " Computing feature importance for CAT_NSAIDs\n",
      " Saved feature importance plot and CSV for CAT_NSAIDs\n",
      "\n",
      " Computing feature importance for CAT_Benzodiazepine\n",
      " Saved feature importance plot and CSV for CAT_Benzodiazepine\n",
      "\n",
      " Computing feature importance for CAT_Antihypertensiva\n",
      " Saved feature importance plot and CSV for CAT_Antihypertensiva\n",
      "\n",
      " Computing feature importance for CAT_Antihistaminica\n",
      " Saved feature importance plot and CSV for CAT_Antihistaminica\n",
      "\n",
      " Computing feature importance for CAT_Anti_epileptica\n",
      " Saved feature importance plot and CSV for CAT_Anti_epileptica\n",
      "\n",
      " Computing feature importance for CAT_Antidepressiva\n",
      " Saved feature importance plot and CSV for CAT_Antidepressiva\n",
      "\n",
      " Computing feature importance for CAT_Antipsychotica\n",
      " Saved feature importance plot and CSV for CAT_Antipsychotica\n",
      "\n",
      " Computing feature importance for CAT_ALL_PSYCHOTROPICS_EXCL_BENZO\n",
      " Saved feature importance plot and CSV for CAT_ALL_PSYCHOTROPICS_EXCL_BENZO\n",
      "\n",
      " Computing feature importance for CAT_ALL_PSYCHOTROPICS_EXCL_SEDATIVES_HYPNOTICS\n",
      " Saved feature importance plot and CSV for CAT_ALL_PSYCHOTROPICS_EXCL_SEDATIVES_HYPNOTICS\n",
      "\n",
      " Computing feature importance for CAT_ALL_PSYCHOTROPICS\n",
      " Saved feature importance plot and CSV for CAT_ALL_PSYCHOTROPICS\n",
      "\n",
      " Computing feature importance for CAT_ALL\n",
      " Saved feature importance plot and CSV for CAT_ALL\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def compute_feature_importance(imputed_dfs, medication_groups, final_covariates_map):\n",
    "    for group in medication_groups:\n",
    "        print(f\"\\n Computing feature importance for {group}\")\n",
    "\n",
    "        if group not in final_covariates_map:\n",
    "            print(f\" No covariates found for {group}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        covariates = final_covariates_map[group]\n",
    "        output_folder = os.path.join(\"outputs\", group)\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "        importance_df_list = []\n",
    "\n",
    "        for i, df_imp in enumerate(imputed_dfs):\n",
    "            if group not in df_imp.columns:\n",
    "                print(f\" {group} not in imputed dataset {i+1}. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            X = df_imp[covariates].copy()\n",
    "            T = df_imp[group]\n",
    "\n",
    "            # Drop NaNs in treatment\n",
    "            valid_idx = T.dropna().index\n",
    "            X = X.loc[valid_idx]\n",
    "            T = T.loc[valid_idx]\n",
    "\n",
    "            try:\n",
    "                model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "                model.fit(X, T)\n",
    "\n",
    "                # Get feature importance (absolute coefficients)\n",
    "                importances = np.abs(model.coef_[0])\n",
    "                importance_dict = dict(zip(X.columns, importances))\n",
    "                df_feat = pd.DataFrame.from_dict(importance_dict, orient='index', columns=[f\"imp{i+1}\"])\n",
    "                df_feat.index.name = 'feature'\n",
    "                importance_df_list.append(df_feat)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"   Error during modeling: {e}\")\n",
    "\n",
    "        if importance_df_list:\n",
    "            # Combine and average\n",
    "            all_feat = pd.concat(importance_df_list, axis=1).fillna(0)\n",
    "            all_feat[\"mean_importance\"] = all_feat.mean(axis=1)\n",
    "\n",
    "            # Filter top 30 non-zero\n",
    "            non_zero = all_feat[all_feat[\"mean_importance\"] > 0]\n",
    "            top30 = non_zero.sort_values(by=\"mean_importance\", ascending=False).head(30)\n",
    "\n",
    "            # Save to CSV\n",
    "            top30.to_csv(os.path.join(output_folder, \"feature_importance.csv\"))\n",
    "\n",
    "            # Plot\n",
    "            plt.figure(figsize=(10, 8))\n",
    "            plt.barh(top30.index[::-1], top30[\"mean_importance\"][::-1])  # plot top → bottom\n",
    "            plt.xlabel(\"Mean Gain Importance\")\n",
    "            plt.title(f\"Top 30 Feature Importance - {group}\")\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(output_folder, \"feature_importance_top30.png\"))\n",
    "            plt.close()\n",
    "\n",
    "            print(f\" Saved feature importance plot and CSV for {group}\")\n",
    "        else:\n",
    "            print(f\" No valid models for {group}\")\n",
    "\n",
    "#  Run\n",
    "compute_feature_importance(imputed_dfs, medication_groups, final_covariates_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "229b8d60-d190-472b-8594-d2b70c09a2a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Processing IPTW + trimming + clipping for CAT_ADHD\n",
      "✅ Saved IPTW weights for CAT_ADHD\n",
      "    ℹ️ Retained 392/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\CAT_ADHD/trimmed_data_imp1.*\n",
      "    ℹ️ Retained 383/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\CAT_ADHD/trimmed_data_imp2.*\n",
      "    ℹ️ Retained 395/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\CAT_ADHD/trimmed_data_imp3.*\n",
      "    ℹ️ Retained 388/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\CAT_ADHD/trimmed_data_imp4.*\n",
      "    ℹ️ Retained 404/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\CAT_ADHD/trimmed_data_imp5.*\n",
      "\n",
      "🔍 Processing IPTW + trimming + clipping for CAT_Aceetanilidederivaten\n",
      "✅ Saved IPTW weights for CAT_Aceetanilidederivaten\n",
      "    ℹ️ Retained 176/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\CAT_Aceetanilidederivaten/trimmed_data_imp1.*\n",
      "    ℹ️ Retained 181/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\CAT_Aceetanilidederivaten/trimmed_data_imp2.*\n",
      "    ℹ️ Retained 166/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\CAT_Aceetanilidederivaten/trimmed_data_imp3.*\n",
      "    ℹ️ Retained 178/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\CAT_Aceetanilidederivaten/trimmed_data_imp4.*\n",
      "    ℹ️ Retained 174/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\CAT_Aceetanilidederivaten/trimmed_data_imp5.*\n",
      "\n",
      "🔍 Processing IPTW + trimming + clipping for CAT_Z_drugs\n",
      "✅ Saved IPTW weights for CAT_Z_drugs\n",
      "    ℹ️ Retained 347/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\CAT_Z_drugs/trimmed_data_imp1.*\n",
      "    ℹ️ Retained 344/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\CAT_Z_drugs/trimmed_data_imp2.*\n",
      "    ℹ️ Retained 346/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\CAT_Z_drugs/trimmed_data_imp3.*\n",
      "    ℹ️ Retained 343/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\CAT_Z_drugs/trimmed_data_imp4.*\n",
      "    ℹ️ Retained 342/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\CAT_Z_drugs/trimmed_data_imp5.*\n",
      "\n",
      "🔍 Processing IPTW + trimming + clipping for CAT_Opioden\n",
      "✅ Saved IPTW weights for CAT_Opioden\n",
      "    ℹ️ Retained 310/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\CAT_Opioden/trimmed_data_imp1.*\n",
      "    ℹ️ Retained 312/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\CAT_Opioden/trimmed_data_imp2.*\n",
      "    ℹ️ Retained 310/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\CAT_Opioden/trimmed_data_imp3.*\n",
      "    ℹ️ Retained 307/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\CAT_Opioden/trimmed_data_imp4.*\n",
      "    ℹ️ Retained 316/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\CAT_Opioden/trimmed_data_imp5.*\n",
      "\n",
      "🔍 Processing IPTW + trimming + clipping for CAT_NSAIDs\n",
      "✅ Saved IPTW weights for CAT_NSAIDs\n",
      "    ℹ️ Retained 123/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\CAT_NSAIDs/trimmed_data_imp1.*\n",
      "    ℹ️ Retained 129/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\CAT_NSAIDs/trimmed_data_imp2.*\n",
      "    ℹ️ Retained 127/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\CAT_NSAIDs/trimmed_data_imp3.*\n",
      "    ℹ️ Retained 137/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\CAT_NSAIDs/trimmed_data_imp4.*\n",
      "    ℹ️ Retained 123/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\CAT_NSAIDs/trimmed_data_imp5.*\n",
      "\n",
      "🔍 Processing IPTW + trimming + clipping for CAT_Benzodiazepine\n",
      "✅ Saved IPTW weights for CAT_Benzodiazepine\n",
      "    ℹ️ Retained 2237/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\CAT_Benzodiazepine/trimmed_data_imp1.*\n",
      "    ℹ️ Retained 2231/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\CAT_Benzodiazepine/trimmed_data_imp2.*\n",
      "    ℹ️ Retained 2220/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\CAT_Benzodiazepine/trimmed_data_imp3.*\n",
      "    ℹ️ Retained 2235/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\CAT_Benzodiazepine/trimmed_data_imp4.*\n",
      "    ℹ️ Retained 2235/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\CAT_Benzodiazepine/trimmed_data_imp5.*\n",
      "\n",
      "🔍 Processing IPTW + trimming + clipping for CAT_Antihypertensiva\n",
      "✅ Saved IPTW weights for CAT_Antihypertensiva\n",
      "    ℹ️ Retained 295/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\CAT_Antihypertensiva/trimmed_data_imp1.*\n",
      "    ℹ️ Retained 302/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\CAT_Antihypertensiva/trimmed_data_imp2.*\n",
      "    ℹ️ Retained 296/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\CAT_Antihypertensiva/trimmed_data_imp3.*\n",
      "    ℹ️ Retained 294/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\CAT_Antihypertensiva/trimmed_data_imp4.*\n",
      "    ℹ️ Retained 293/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\CAT_Antihypertensiva/trimmed_data_imp5.*\n",
      "\n",
      "🔍 Processing IPTW + trimming + clipping for CAT_Antihistaminica\n",
      "✅ Saved IPTW weights for CAT_Antihistaminica\n",
      "    ℹ️ Retained 301/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\CAT_Antihistaminica/trimmed_data_imp1.*\n",
      "    ℹ️ Retained 303/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\CAT_Antihistaminica/trimmed_data_imp2.*\n",
      "    ℹ️ Retained 305/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\CAT_Antihistaminica/trimmed_data_imp3.*\n",
      "    ℹ️ Retained 302/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\CAT_Antihistaminica/trimmed_data_imp4.*\n",
      "    ℹ️ Retained 298/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\CAT_Antihistaminica/trimmed_data_imp5.*\n",
      "\n",
      "🔍 Processing IPTW + trimming + clipping for CAT_Anti_epileptica\n",
      "✅ Saved IPTW weights for CAT_Anti_epileptica\n",
      "    ℹ️ Retained 541/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\CAT_Anti_epileptica/trimmed_data_imp1.*\n",
      "    ℹ️ Retained 536/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\CAT_Anti_epileptica/trimmed_data_imp2.*\n",
      "    ℹ️ Retained 543/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\CAT_Anti_epileptica/trimmed_data_imp3.*\n",
      "    ℹ️ Retained 544/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\CAT_Anti_epileptica/trimmed_data_imp4.*\n",
      "    ℹ️ Retained 557/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\CAT_Anti_epileptica/trimmed_data_imp5.*\n",
      "\n",
      "🔍 Processing IPTW + trimming + clipping for CAT_Antidepressiva\n",
      "✅ Saved IPTW weights for CAT_Antidepressiva\n",
      "    ℹ️ Retained 3246/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\CAT_Antidepressiva/trimmed_data_imp1.*\n",
      "    ℹ️ Retained 3264/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\CAT_Antidepressiva/trimmed_data_imp2.*\n",
      "    ℹ️ Retained 3323/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\CAT_Antidepressiva/trimmed_data_imp3.*\n",
      "    ℹ️ Retained 3262/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\CAT_Antidepressiva/trimmed_data_imp4.*\n",
      "    ℹ️ Retained 3299/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\CAT_Antidepressiva/trimmed_data_imp5.*\n",
      "\n",
      "🔍 Processing IPTW + trimming + clipping for CAT_Antipsychotica\n",
      "✅ Saved IPTW weights for CAT_Antipsychotica\n",
      "    ℹ️ Retained 1675/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\CAT_Antipsychotica/trimmed_data_imp1.*\n",
      "    ℹ️ Retained 1672/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\CAT_Antipsychotica/trimmed_data_imp2.*\n",
      "    ℹ️ Retained 1680/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\CAT_Antipsychotica/trimmed_data_imp3.*\n",
      "    ℹ️ Retained 1675/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\CAT_Antipsychotica/trimmed_data_imp4.*\n",
      "    ℹ️ Retained 1673/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\CAT_Antipsychotica/trimmed_data_imp5.*\n",
      "\n",
      "🔍 Processing IPTW + trimming + clipping for CAT_ALL_PSYCHOTROPICS_EXCL_BENZO\n",
      "✅ Saved IPTW weights for CAT_ALL_PSYCHOTROPICS_EXCL_BENZO\n",
      "    ℹ️ Retained 4181/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\CAT_ALL_PSYCHOTROPICS_EXCL_BENZO/trimmed_data_imp1.*\n",
      "    ℹ️ Retained 4228/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\CAT_ALL_PSYCHOTROPICS_EXCL_BENZO/trimmed_data_imp2.*\n",
      "    ℹ️ Retained 4233/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\CAT_ALL_PSYCHOTROPICS_EXCL_BENZO/trimmed_data_imp3.*\n",
      "    ℹ️ Retained 4184/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\CAT_ALL_PSYCHOTROPICS_EXCL_BENZO/trimmed_data_imp4.*\n",
      "    ℹ️ Retained 4217/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\CAT_ALL_PSYCHOTROPICS_EXCL_BENZO/trimmed_data_imp5.*\n",
      "\n",
      "🔍 Processing IPTW + trimming + clipping for CAT_ALL_PSYCHOTROPICS_EXCL_SEDATIVES_HYPNOTICS\n",
      "✅ Saved IPTW weights for CAT_ALL_PSYCHOTROPICS_EXCL_SEDATIVES_HYPNOTICS\n",
      "    ℹ️ Retained 4015/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\CAT_ALL_PSYCHOTROPICS_EXCL_SEDATIVES_HYPNOTICS/trimmed_data_imp1.*\n",
      "    ℹ️ Retained 4036/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\CAT_ALL_PSYCHOTROPICS_EXCL_SEDATIVES_HYPNOTICS/trimmed_data_imp2.*\n",
      "    ℹ️ Retained 4061/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\CAT_ALL_PSYCHOTROPICS_EXCL_SEDATIVES_HYPNOTICS/trimmed_data_imp3.*\n",
      "    ℹ️ Retained 3978/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\CAT_ALL_PSYCHOTROPICS_EXCL_SEDATIVES_HYPNOTICS/trimmed_data_imp4.*\n",
      "    ℹ️ Retained 4017/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\CAT_ALL_PSYCHOTROPICS_EXCL_SEDATIVES_HYPNOTICS/trimmed_data_imp5.*\n",
      "\n",
      "🔍 Processing IPTW + trimming + clipping for CAT_ALL_PSYCHOTROPICS\n",
      "✅ Saved IPTW weights for CAT_ALL_PSYCHOTROPICS\n",
      "    ℹ️ Retained 4981/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\CAT_ALL_PSYCHOTROPICS/trimmed_data_imp1.*\n",
      "    ℹ️ Retained 5012/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\CAT_ALL_PSYCHOTROPICS/trimmed_data_imp2.*\n",
      "    ℹ️ Retained 5028/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\CAT_ALL_PSYCHOTROPICS/trimmed_data_imp3.*\n",
      "    ℹ️ Retained 5030/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\CAT_ALL_PSYCHOTROPICS/trimmed_data_imp4.*\n",
      "    ℹ️ Retained 5006/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\CAT_ALL_PSYCHOTROPICS/trimmed_data_imp5.*\n",
      "\n",
      "🔍 Processing IPTW + trimming + clipping for CAT_ALL\n",
      "✅ Saved IPTW weights for CAT_ALL\n",
      "    ℹ️ Retained 5036/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\CAT_ALL/trimmed_data_imp1.*\n",
      "    ℹ️ Retained 5072/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\CAT_ALL/trimmed_data_imp2.*\n",
      "    ℹ️ Retained 5074/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\CAT_ALL/trimmed_data_imp3.*\n",
      "    ℹ️ Retained 5049/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\CAT_ALL/trimmed_data_imp4.*\n",
      "    ℹ️ Retained 5080/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\CAT_ALL/trimmed_data_imp5.*\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def compute_trimmed_clipped_iptw(ps_df, treatment, lower=0.05, upper=0.95, clip_max=10):\n",
    "    weights = []\n",
    "    keep_mask = (ps_df > lower) & (ps_df < upper)\n",
    "\n",
    "    for i in range(ps_df.shape[1]):\n",
    "        ps = ps_df.iloc[:, i].clip(lower=1e-6, upper=1 - 1e-6)  # avoid div by zero\n",
    "        mask = keep_mask.iloc[:, i]\n",
    "        w = pd.Series(np.nan, index=ps.index)\n",
    "\n",
    "        w[mask & (treatment == 1)] = 1 / ps[mask & (treatment == 1)]\n",
    "        w[mask & (treatment == 0)] = 1 / (1 - ps[mask & (treatment == 0)])\n",
    "        w = w.clip(upper=clip_max)\n",
    "        weights.append(w)\n",
    "\n",
    "    return pd.concat(weights, axis=1)\n",
    "\n",
    "\n",
    "def apply_rubins_rule_to_iptw(iptw_matrix):\n",
    "    \"\"\"\n",
    "    Given an IPTW matrix (n rows × M imputations), return Rubin’s rule pooled mean, SD, SE.\n",
    "    \"\"\"\n",
    "    M = iptw_matrix.shape[1]\n",
    "    q_bar = iptw_matrix.mean(axis=1)\n",
    "    u_bar = iptw_matrix.var(axis=1, ddof=1)\n",
    "    B = iptw_matrix.apply(lambda x: x.mean(), axis=1).var(ddof=1)\n",
    "    total_var = u_bar + (1 + 1/M) * B\n",
    "    total_se = np.sqrt(total_var)\n",
    "    return q_bar, u_bar.pow(0.5), total_se\n",
    "\n",
    "\n",
    "def run_trim_clip_save_all(imputed_dfs, medication_groups, final_covariates_map):\n",
    "    for group in medication_groups:\n",
    "        print(f\"\\n🔍 Processing IPTW + trimming + clipping for {group}\")\n",
    "        output_folder = os.path.join(\"outputs\", group)\n",
    "        ps_path = os.path.join(output_folder, \"propensity_scores.xlsx\")\n",
    "\n",
    "        if not os.path.exists(ps_path):\n",
    "            print(f\"⚠️ Missing PS file: {ps_path}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            ps_all = pd.read_excel(ps_path, index_col=0)\n",
    "            ps_cols = [col for col in ps_all.columns if col.startswith(\"ps_imp\")]\n",
    "            composite_index = ps_all.index\n",
    "\n",
    "            # Get treatment from one imputed dataset\n",
    "            T_full = None\n",
    "            for df in imputed_dfs:\n",
    "                if group in df.columns:\n",
    "                    T_full = df.loc[composite_index, group]\n",
    "                    break\n",
    "\n",
    "            if T_full is None:\n",
    "                print(f\"❌ Treatment column {group} not found in any imputed dataset.\")\n",
    "                continue\n",
    "\n",
    "            # Compute IPTW matrix (shape: n × M)\n",
    "            iptw_matrix = compute_trimmed_clipped_iptw(ps_all[ps_cols], T_full)\n",
    "            iptw_matrix.columns = [f\"iptw_imp{i+1}\" for i in range(iptw_matrix.shape[1])]\n",
    "\n",
    "            # Apply Rubin’s Rule for mean, SD, SE\n",
    "            iptw_matrix[\"iptw_mean\"], iptw_matrix[\"iptw_sd\"], iptw_matrix[\"iptw_se\"] = apply_rubins_rule_to_iptw(\n",
    "                iptw_matrix[[f\"iptw_imp{i+1}\" for i in range(iptw_matrix.shape[1])]]\n",
    "            )\n",
    "\n",
    "            # Save IPTW matrix separately\n",
    "            iptw_matrix.to_excel(os.path.join(output_folder, \"iptw_weights.xlsx\"))\n",
    "            print(f\"✅ Saved IPTW weights for {group}\")\n",
    "\n",
    "            # Save trimmed & clipped imputed datasets with IPTW\n",
    "            for i in range(5):\n",
    "                df = imputed_dfs[i].copy()\n",
    "                if group not in df.columns:\n",
    "                    continue\n",
    "\n",
    "                trimmed_idx = iptw_matrix.index.intersection(df.index)\n",
    "                needed_cols = final_covariates_map[group] + [group, \"caps5_change_baseline\"]\n",
    "\n",
    "                # Select only necessary columns\n",
    "                df_trimmed = df.loc[trimmed_idx, needed_cols].copy()\n",
    "                df_trimmed[\"iptw\"] = iptw_matrix[f\"iptw_imp{i+1}\"].loc[trimmed_idx]\n",
    "\n",
    "                # ✅ DROP rows with missing IPTW values\n",
    "                before = len(df_trimmed)\n",
    "                df_trimmed = df_trimmed.dropna(subset=[\"iptw\"])\n",
    "                after = len(df_trimmed)\n",
    "                print(f\"    ℹ️ Retained {after}/{before} rows after IPTW NaN drop.\")\n",
    "\n",
    "                # Save to .pkl\n",
    "                df_trimmed.to_pickle(os.path.join(output_folder, f\"trimmed_data_imp{i+1}.pkl\"))\n",
    "                print(f\"  💾 Saved trimmed dataset: {output_folder}/trimmed_data_imp{i+1}.*\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error in {group}: {e}\")\n",
    "\n",
    "\n",
    "run_trim_clip_save_all(imputed_dfs, medication_groups, final_covariates_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "045e9079-9f4c-4aa3-aefe-5235be369d24",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Plotting PS overlap for CAT_ADHD\n",
      "✅ Saved unweighted and weighted PS plots for CAT_ADHD\n",
      "\n",
      "📊 Plotting PS overlap for CAT_Aceetanilidederivaten\n",
      "✅ Saved unweighted and weighted PS plots for CAT_Aceetanilidederivaten\n",
      "\n",
      "📊 Plotting PS overlap for CAT_Z_drugs\n",
      "✅ Saved unweighted and weighted PS plots for CAT_Z_drugs\n",
      "\n",
      "📊 Plotting PS overlap for CAT_Opioden\n",
      "✅ Saved unweighted and weighted PS plots for CAT_Opioden\n",
      "\n",
      "📊 Plotting PS overlap for CAT_NSAIDs\n",
      "✅ Saved unweighted and weighted PS plots for CAT_NSAIDs\n",
      "\n",
      "📊 Plotting PS overlap for CAT_Benzodiazepine\n",
      "✅ Saved unweighted and weighted PS plots for CAT_Benzodiazepine\n",
      "\n",
      "📊 Plotting PS overlap for CAT_Antihypertensiva\n",
      "✅ Saved unweighted and weighted PS plots for CAT_Antihypertensiva\n",
      "\n",
      "📊 Plotting PS overlap for CAT_Antihistaminica\n",
      "✅ Saved unweighted and weighted PS plots for CAT_Antihistaminica\n",
      "\n",
      "📊 Plotting PS overlap for CAT_Anti_epileptica\n",
      "✅ Saved unweighted and weighted PS plots for CAT_Anti_epileptica\n",
      "\n",
      "📊 Plotting PS overlap for CAT_Antidepressiva\n",
      "✅ Saved unweighted and weighted PS plots for CAT_Antidepressiva\n",
      "\n",
      "📊 Plotting PS overlap for CAT_Antipsychotica\n",
      "✅ Saved unweighted and weighted PS plots for CAT_Antipsychotica\n",
      "\n",
      "📊 Plotting PS overlap for CAT_ALL_PSYCHOTROPICS_EXCL_BENZO\n",
      "✅ Saved unweighted and weighted PS plots for CAT_ALL_PSYCHOTROPICS_EXCL_BENZO\n",
      "\n",
      "📊 Plotting PS overlap for CAT_ALL_PSYCHOTROPICS_EXCL_SEDATIVES_HYPNOTICS\n",
      "✅ Saved unweighted and weighted PS plots for CAT_ALL_PSYCHOTROPICS_EXCL_SEDATIVES_HYPNOTICS\n",
      "\n",
      "📊 Plotting PS overlap for CAT_ALL_PSYCHOTROPICS\n",
      "✅ Saved unweighted and weighted PS plots for CAT_ALL_PSYCHOTROPICS\n",
      "\n",
      "📊 Plotting PS overlap for CAT_ALL\n",
      "✅ Saved unweighted and weighted PS plots for CAT_ALL\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_ps_overlap_all_groups(medication_groups):\n",
    "    for group in medication_groups:\n",
    "        print(f\"\\n📊 Plotting PS overlap for {group}\")\n",
    "\n",
    "        folder = os.path.join(\"outputs\", group)\n",
    "        ps_file = os.path.join(folder, \"propensity_scores.xlsx\")\n",
    "        iptw_file = os.path.join(folder, \"iptw_weights.xlsx\")\n",
    "        trimmed_file = os.path.join(folder, \"trimmed_data_imp1.pkl\")\n",
    "\n",
    "        if not all(os.path.exists(f) for f in [ps_file, iptw_file, trimmed_file]):\n",
    "            print(f\"⚠️ Missing required files for {group}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            ps_df = pd.read_excel(ps_file, index_col=0)\n",
    "            iptw_df = pd.read_excel(iptw_file, index_col=0)\n",
    "            trimmed_df = pd.read_pickle(trimmed_file)\n",
    "\n",
    "            # Extract\n",
    "            ps = ps_df[\"composite_ps\"].reindex(trimmed_df.index)\n",
    "            w = iptw_df[\"iptw_mean\"].reindex(trimmed_df.index)\n",
    "            T = trimmed_df[group]\n",
    "\n",
    "            # Masks to remove NaNs\n",
    "            treated_mask = (T == 1) & ps.notna() & w.notna()\n",
    "            control_mask = (T == 0) & ps.notna() & w.notna()\n",
    "\n",
    "            treated = ps[treated_mask]\n",
    "            treated_w = w[treated_mask]\n",
    "\n",
    "            control = ps[control_mask]\n",
    "            control_w = w[control_mask]\n",
    "\n",
    "            # === Unweighted Plot ===\n",
    "            plt.figure(figsize=(8, 5))\n",
    "            plt.hist([treated, control], bins=25, label=[\"Treated\", \"Control\"], alpha=0.6)\n",
    "            plt.title(f\"Unweighted PS Overlap - {group}\")\n",
    "            plt.xlabel(\"Composite Propensity Score\")\n",
    "            plt.ylabel(\"Count\")\n",
    "            plt.legend()\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(folder, \"ps_overlap_unweighted.png\"))\n",
    "            plt.close()\n",
    "\n",
    "            # === Weighted Plot ===\n",
    "            plt.figure(figsize=(8, 5))\n",
    "            plt.hist([treated, control], bins=25, weights=[treated_w, control_w], label=[\"Treated\", \"Control\"], alpha=0.6)\n",
    "            plt.title(f\"Weighted PS Overlap - {group}\")\n",
    "            plt.xlabel(\"Composite Propensity Score\")\n",
    "            plt.ylabel(\"Weighted Count\")\n",
    "            plt.legend()\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(folder, \"ps_overlap_weighted.png\"))\n",
    "            plt.close()\n",
    "\n",
    "            print(f\"✅ Saved unweighted and weighted PS plots for {group}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error processing {group}: {e}\")\n",
    "\n",
    "# 🔁 Run\n",
    "plot_ps_overlap_all_groups(medication_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a63f995d-519c-4bdc-a3e2-c1abe92c4c2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ✅ Saved: outputs\\CAT_ADHD\\four_panel_overlap_CAT_ADHD.png\n",
      " ✅ Saved: outputs\\CAT_Aceetanilidederivaten\\four_panel_overlap_CAT_Aceetanilidederivaten.png\n",
      " ✅ Saved: outputs\\CAT_Z_drugs\\four_panel_overlap_CAT_Z_drugs.png\n",
      " ✅ Saved: outputs\\CAT_Opioden\\four_panel_overlap_CAT_Opioden.png\n",
      " ✅ Saved: outputs\\CAT_NSAIDs\\four_panel_overlap_CAT_NSAIDs.png\n",
      " ✅ Saved: outputs\\CAT_Benzodiazepine\\four_panel_overlap_CAT_Benzodiazepine.png\n",
      " ✅ Saved: outputs\\CAT_Antihypertensiva\\four_panel_overlap_CAT_Antihypertensiva.png\n",
      " ✅ Saved: outputs\\CAT_Antihistaminica\\four_panel_overlap_CAT_Antihistaminica.png\n",
      " ✅ Saved: outputs\\CAT_Anti_epileptica\\four_panel_overlap_CAT_Anti_epileptica.png\n",
      " ✅ Saved: outputs\\CAT_Antidepressiva\\four_panel_overlap_CAT_Antidepressiva.png\n",
      " ✅ Saved: outputs\\CAT_Antipsychotica\\four_panel_overlap_CAT_Antipsychotica.png\n",
      " ✅ Saved: outputs\\CAT_ALL_PSYCHOTROPICS_EXCL_BENZO\\four_panel_overlap_CAT_ALL_PSYCHOTROPICS_EXCL_BENZO.png\n",
      " ✅ Saved: outputs\\CAT_ALL_PSYCHOTROPICS_EXCL_SEDATIVES_HYPNOTICS\\four_panel_overlap_CAT_ALL_PSYCHOTROPICS_EXCL_SEDATIVES_HYPNOTICS.png\n",
      " ✅ Saved: outputs\\CAT_ALL_PSYCHOTROPICS\\four_panel_overlap_CAT_ALL_PSYCHOTROPICS.png\n",
      " ✅ Saved: outputs\\CAT_ALL\\four_panel_overlap_CAT_ALL.png\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set up base output folder\n",
    "output_base = \"outputs\"\n",
    "ps_file = \"propensity_scores.xlsx\"\n",
    "iptw_file = \"iptw_weights.xlsx\"\n",
    "trimmed_data_file = \"trimmed_data_imp1.pkl\"\n",
    "\n",
    "# Collect all treatment group folders\n",
    "groups = [g for g in medication_groups if os.path.isdir(os.path.join(output_base, g))]\n",
    "\n",
    "# Generate 4-panel overlap plots\n",
    "for group in groups:\n",
    "    group_path = os.path.join(output_base, group)\n",
    "    try:\n",
    "        # Load trimmed treatment info\n",
    "        trimmed_df = pd.read_pickle(os.path.join(group_path, trimmed_data_file))\n",
    "        index = trimmed_df.index\n",
    "\n",
    "        # Fix: case-insensitive match for treatment variable\n",
    "        possible_cols = [col for col in trimmed_df.columns if col.upper() == group.upper()]\n",
    "        if not possible_cols:\n",
    "            print(f\" Treatment variable {group} not found in {group}, skipping.\")\n",
    "            continue\n",
    "        treatment_var = possible_cols[0]\n",
    "        T = trimmed_df[treatment_var]\n",
    "\n",
    "        # Load composite PS (aligned to trimmed_df index)\n",
    "        ps_df = pd.read_excel(os.path.join(group_path, ps_file), index_col=0)\n",
    "        if 'composite_ps' not in ps_df.columns:\n",
    "            print(f\" Composite column missing in {ps_file}, skipping {group}.\")\n",
    "            continue\n",
    "        ps = ps_df.loc[index, 'composite_ps']\n",
    "\n",
    "        # Load IPTW weights (aligned to trimmed_df index)\n",
    "        weights_df = pd.read_excel(os.path.join(group_path, iptw_file), index_col=0)\n",
    "        if 'iptw_mean' not in weights_df.columns:\n",
    "            print(f\" IPTW weight column missing in {iptw_file}, skipping {group}.\")\n",
    "            continue\n",
    "        weights = weights_df.loc[index, 'iptw_mean']\n",
    "\n",
    "        # Prepare 4 datasets\n",
    "        raw_treated = ps[T == 1]\n",
    "        raw_control = ps[T == 0]\n",
    "        weighted_treated = (ps[T == 1], weights[T == 1])\n",
    "        weighted_control = (ps[T == 0], weights[T == 0])\n",
    "\n",
    "        # Create plot\n",
    "        fig, axs = plt.subplots(2, 2, figsize=(12, 8))\n",
    "        fig.suptitle(f\"Propensity Score Distribution - {group}\", fontsize=14)\n",
    "\n",
    "        axs[0, 0].hist(raw_treated, bins=20, alpha=0.7, color='blue')\n",
    "        axs[0, 0].set_title(\"Raw Treated\")\n",
    "\n",
    "        axs[0, 1].hist(raw_control, bins=20, alpha=0.7, color='green')\n",
    "        axs[0, 1].set_title(\"Raw Control\")\n",
    "\n",
    "        axs[1, 0].hist(weighted_treated[0], bins=20, weights=weighted_treated[1], alpha=0.7, color='blue')\n",
    "        axs[1, 0].set_title(\"Weighted Treated\")\n",
    "\n",
    "        axs[1, 1].hist(weighted_control[0], bins=20, weights=weighted_control[1], alpha=0.7, color='green')\n",
    "        axs[1, 1].set_title(\"Weighted Control\")\n",
    "\n",
    "        for ax in axs.flat:\n",
    "            ax.set_xlim(0, 1)\n",
    "            ax.set_xlabel(\"Propensity Score\")\n",
    "            ax.set_ylabel(\"Count\")\n",
    "\n",
    "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "\n",
    "        # Save figure\n",
    "        plot_path = os.path.join(group_path, f\"four_panel_overlap_{group}.png\")\n",
    "        plt.savefig(plot_path)\n",
    "        plt.close()\n",
    "        print(f\" ✅ Saved: {plot_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\" ❌ Error in {group}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9ae9b6b5-f873-4db1-a724-174b17b565c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ATT calculation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0d8ec178-4c5a-4efb-8a0b-7af1743d12fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weighted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "385e2c8c-d84f-413c-97ac-97aaacfd3190",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🚀 Running OLS for CAT_ADHD\n",
      "✅ CAT_ADHD | Seed 1: ATT = 0.8360, SE = 2.4919, p = 0.75412\n",
      "✅ CAT_ADHD | Seed 2: ATT = 1.2408, SE = 2.4817, p = 0.64335\n",
      "✅ CAT_ADHD | Seed 3: ATT = -0.4415, SE = 3.3983, p = 0.90290\n",
      "✅ CAT_ADHD | Seed 4: ATT = 1.2011, SE = 3.9538, p = 0.77644\n",
      "✅ CAT_ADHD | Seed 5: ATT = 1.3198, SE = 2.6504, p = 0.64464\n",
      "✅ CAT_ADHD | Seed 6: ATT = -1.1362, SE = 1.7887, p = 0.55982\n",
      "✅ CAT_ADHD | Seed 7: ATT = 0.8207, SE = 2.8346, p = 0.78656\n",
      "✅ CAT_ADHD | Seed 8: ATT = 0.1788, SE = 2.4396, p = 0.94509\n",
      "✅ CAT_ADHD | Seed 9: ATT = -0.2881, SE = 3.3107, p = 0.93483\n",
      "✅ CAT_ADHD | Seed 10: ATT = 0.2123, SE = 3.1963, p = 0.95023\n",
      "📊 Diagnostic plots saved for CAT_ADHD\n",
      "🏆 Best result for CAT_ADHD → Seed 6 | SE = 1.7887\n",
      "\n",
      "🚀 Running OLS for CAT_Aceetanilidederivaten\n",
      "✅ CAT_Aceetanilidederivaten | Seed 1: ATT = 1.3972, SE = 6.8213, p = 0.84771\n",
      "✅ CAT_Aceetanilidederivaten | Seed 2: ATT = 2.2173, SE = 5.4355, p = 0.70422\n",
      "✅ CAT_Aceetanilidederivaten | Seed 3: ATT = 2.8694, SE = 6.5133, p = 0.68230\n",
      "✅ CAT_Aceetanilidederivaten | Seed 4: ATT = 1.3809, SE = 3.8770, p = 0.73969\n",
      "✅ CAT_Aceetanilidederivaten | Seed 5: ATT = 4.2621, SE = 4.4297, p = 0.39045\n",
      "✅ CAT_Aceetanilidederivaten | Seed 6: ATT = 2.1034, SE = 3.8083, p = 0.61013\n",
      "✅ CAT_Aceetanilidederivaten | Seed 7: ATT = 3.0044, SE = 5.3062, p = 0.60151\n",
      "✅ CAT_Aceetanilidederivaten | Seed 8: ATT = 1.0168, SE = 5.5364, p = 0.86322\n",
      "✅ CAT_Aceetanilidederivaten | Seed 9: ATT = 1.1174, SE = 2.6613, p = 0.69616\n",
      "✅ CAT_Aceetanilidederivaten | Seed 10: ATT = 2.2943, SE = 3.2609, p = 0.52050\n",
      "📊 Diagnostic plots saved for CAT_Aceetanilidederivaten\n",
      "🏆 Best result for CAT_Aceetanilidederivaten → Seed 9 | SE = 2.6613\n",
      "\n",
      "🚀 Running OLS for CAT_Z_drugs\n",
      "✅ CAT_Z_drugs | Seed 1: ATT = 6.6272, SE = 3.8989, p = 0.16440\n",
      "✅ CAT_Z_drugs | Seed 2: ATT = 7.1941, SE = 3.7329, p = 0.12622\n",
      "✅ CAT_Z_drugs | Seed 3: ATT = 7.2892, SE = 3.7791, p = 0.12598\n",
      "✅ CAT_Z_drugs | Seed 4: ATT = 5.7418, SE = 4.0088, p = 0.22533\n",
      "✅ CAT_Z_drugs | Seed 5: ATT = 7.6235, SE = 3.8533, p = 0.11902\n",
      "✅ CAT_Z_drugs | Seed 6: ATT = 6.5147, SE = 3.4642, p = 0.13320\n",
      "✅ CAT_Z_drugs | Seed 7: ATT = 5.8339, SE = 3.6898, p = 0.18902\n",
      "✅ CAT_Z_drugs | Seed 8: ATT = 6.0400, SE = 3.2528, p = 0.13689\n",
      "✅ CAT_Z_drugs | Seed 9: ATT = 7.6143, SE = 3.8033, p = 0.11585\n",
      "✅ CAT_Z_drugs | Seed 10: ATT = 7.4565, SE = 2.5949, p = 0.04531\n",
      "📊 Diagnostic plots saved for CAT_Z_drugs\n",
      "🏆 Best result for CAT_Z_drugs → Seed 10 | SE = 2.5949\n",
      "\n",
      "🚀 Running OLS for CAT_Opioden\n",
      "✅ CAT_Opioden | Seed 1: ATT = 3.2538, SE = 3.5494, p = 0.41117\n",
      "✅ CAT_Opioden | Seed 2: ATT = 4.9045, SE = 3.3319, p = 0.21500\n",
      "✅ CAT_Opioden | Seed 3: ATT = 4.0682, SE = 4.6543, p = 0.43142\n",
      "✅ CAT_Opioden | Seed 4: ATT = 2.2977, SE = 4.1221, p = 0.60697\n",
      "✅ CAT_Opioden | Seed 5: ATT = 3.2682, SE = 3.6315, p = 0.41903\n",
      "✅ CAT_Opioden | Seed 6: ATT = 3.8148, SE = 3.8666, p = 0.37969\n",
      "✅ CAT_Opioden | Seed 7: ATT = 4.3448, SE = 3.4106, p = 0.27169\n",
      "✅ CAT_Opioden | Seed 8: ATT = 3.2793, SE = 3.8863, p = 0.44628\n",
      "✅ CAT_Opioden | Seed 9: ATT = 4.9620, SE = 3.7481, p = 0.25614\n",
      "✅ CAT_Opioden | Seed 10: ATT = 4.7030, SE = 4.6364, p = 0.36778\n",
      "📊 Diagnostic plots saved for CAT_Opioden\n",
      "🏆 Best result for CAT_Opioden → Seed 2 | SE = 3.3319\n",
      "\n",
      "🚀 Running OLS for CAT_NSAIDs\n",
      "✅ CAT_NSAIDs | Seed 1: ATT = -1.5274, SE = 5.8002, p = 0.80530\n",
      "✅ CAT_NSAIDs | Seed 2: ATT = -5.1366, SE = 4.3766, p = 0.30565\n",
      "✅ CAT_NSAIDs | Seed 3: ATT = -0.0769, SE = 6.6856, p = 0.99138\n",
      "✅ CAT_NSAIDs | Seed 4: ATT = -0.6484, SE = 6.2515, p = 0.92239\n",
      "✅ CAT_NSAIDs | Seed 5: ATT = -4.4876, SE = 4.8361, p = 0.40596\n",
      "✅ CAT_NSAIDs | Seed 6: ATT = -6.0150, SE = 5.5551, p = 0.33981\n",
      "✅ CAT_NSAIDs | Seed 7: ATT = -4.9287, SE = 5.6550, p = 0.43263\n",
      "✅ CAT_NSAIDs | Seed 8: ATT = -4.0342, SE = 4.8023, p = 0.44816\n",
      "✅ CAT_NSAIDs | Seed 9: ATT = -0.8664, SE = 7.3033, p = 0.91129\n",
      "✅ CAT_NSAIDs | Seed 10: ATT = -0.5836, SE = 6.4744, p = 0.93251\n",
      "📊 Diagnostic plots saved for CAT_NSAIDs\n",
      "🏆 Best result for CAT_NSAIDs → Seed 2 | SE = 4.3766\n",
      "\n",
      "🚀 Running OLS for CAT_Benzodiazepine\n",
      "✅ CAT_Benzodiazepine | Seed 1: ATT = 0.8707, SE = 0.8696, p = 0.37333\n",
      "✅ CAT_Benzodiazepine | Seed 2: ATT = 0.2530, SE = 0.8896, p = 0.79022\n",
      "✅ CAT_Benzodiazepine | Seed 3: ATT = 0.9269, SE = 0.8929, p = 0.35785\n",
      "✅ CAT_Benzodiazepine | Seed 4: ATT = 0.8713, SE = 0.8518, p = 0.36417\n",
      "✅ CAT_Benzodiazepine | Seed 5: ATT = 0.6968, SE = 1.1406, p = 0.57426\n",
      "✅ CAT_Benzodiazepine | Seed 6: ATT = 0.9500, SE = 1.4607, p = 0.55094\n",
      "✅ CAT_Benzodiazepine | Seed 7: ATT = 0.1716, SE = 0.9393, p = 0.86395\n",
      "✅ CAT_Benzodiazepine | Seed 8: ATT = 0.7797, SE = 0.8405, p = 0.40613\n",
      "✅ CAT_Benzodiazepine | Seed 9: ATT = 1.0775, SE = 1.1360, p = 0.39658\n",
      "✅ CAT_Benzodiazepine | Seed 10: ATT = 0.3354, SE = 1.6701, p = 0.85063\n",
      "📊 Diagnostic plots saved for CAT_Benzodiazepine\n",
      "🏆 Best result for CAT_Benzodiazepine → Seed 8 | SE = 0.8405\n",
      "\n",
      "🚀 Running OLS for CAT_Antihypertensiva\n",
      "✅ CAT_Antihypertensiva | Seed 1: ATT = -1.7594, SE = 4.8669, p = 0.73600\n",
      "✅ CAT_Antihypertensiva | Seed 2: ATT = 1.4646, SE = 4.0174, p = 0.73389\n",
      "✅ CAT_Antihypertensiva | Seed 3: ATT = 0.4399, SE = 4.3540, p = 0.92438\n",
      "✅ CAT_Antihypertensiva | Seed 4: ATT = -0.4532, SE = 3.8805, p = 0.91266\n",
      "✅ CAT_Antihypertensiva | Seed 5: ATT = -2.8848, SE = 2.6995, p = 0.34543\n",
      "✅ CAT_Antihypertensiva | Seed 6: ATT = -1.5731, SE = 4.0628, p = 0.71833\n",
      "✅ CAT_Antihypertensiva | Seed 7: ATT = 0.8112, SE = 5.7347, p = 0.89435\n",
      "✅ CAT_Antihypertensiva | Seed 8: ATT = -0.8842, SE = 4.0260, p = 0.83691\n",
      "✅ CAT_Antihypertensiva | Seed 9: ATT = -0.9094, SE = 3.4295, p = 0.80399\n",
      "✅ CAT_Antihypertensiva | Seed 10: ATT = 0.6367, SE = 5.1653, p = 0.90784\n",
      "📊 Diagnostic plots saved for CAT_Antihypertensiva\n",
      "🏆 Best result for CAT_Antihypertensiva → Seed 5 | SE = 2.6995\n",
      "\n",
      "🚀 Running OLS for CAT_Antihistaminica\n",
      "✅ CAT_Antihistaminica | Seed 1: ATT = -3.7728, SE = 3.9158, p = 0.38987\n",
      "✅ CAT_Antihistaminica | Seed 2: ATT = -2.6982, SE = 3.1789, p = 0.44380\n",
      "✅ CAT_Antihistaminica | Seed 3: ATT = -2.4567, SE = 3.7438, p = 0.54752\n",
      "✅ CAT_Antihistaminica | Seed 4: ATT = -6.0852, SE = 4.0132, p = 0.20403\n",
      "✅ CAT_Antihistaminica | Seed 5: ATT = -5.2607, SE = 2.6898, p = 0.12214\n",
      "✅ CAT_Antihistaminica | Seed 6: ATT = -2.6159, SE = 3.1784, p = 0.45672\n",
      "✅ CAT_Antihistaminica | Seed 7: ATT = -2.6929, SE = 3.0469, p = 0.42671\n",
      "✅ CAT_Antihistaminica | Seed 8: ATT = -2.7133, SE = 4.0457, p = 0.53917\n",
      "✅ CAT_Antihistaminica | Seed 9: ATT = -5.2357, SE = 2.6506, p = 0.11945\n",
      "✅ CAT_Antihistaminica | Seed 10: ATT = -1.7271, SE = 2.9076, p = 0.58447\n",
      "📊 Diagnostic plots saved for CAT_Antihistaminica\n",
      "🏆 Best result for CAT_Antihistaminica → Seed 9 | SE = 2.6506\n",
      "\n",
      "🚀 Running OLS for CAT_Anti_epileptica\n",
      "✅ CAT_Anti_epileptica | Seed 1: ATT = 3.1128, SE = 2.5232, p = 0.28486\n",
      "✅ CAT_Anti_epileptica | Seed 2: ATT = 2.4012, SE = 3.2708, p = 0.50358\n",
      "✅ CAT_Anti_epileptica | Seed 3: ATT = 2.8659, SE = 2.1685, p = 0.25683\n",
      "✅ CAT_Anti_epileptica | Seed 4: ATT = 3.6521, SE = 3.1506, p = 0.31087\n",
      "✅ CAT_Anti_epileptica | Seed 5: ATT = 2.8479, SE = 3.3680, p = 0.44541\n",
      "✅ CAT_Anti_epileptica | Seed 6: ATT = 1.5340, SE = 2.7720, p = 0.60948\n",
      "✅ CAT_Anti_epileptica | Seed 7: ATT = 3.0366, SE = 2.6266, p = 0.31200\n",
      "✅ CAT_Anti_epileptica | Seed 8: ATT = 2.1917, SE = 2.4654, p = 0.42424\n",
      "✅ CAT_Anti_epileptica | Seed 9: ATT = 1.9949, SE = 2.4755, p = 0.46549\n",
      "✅ CAT_Anti_epileptica | Seed 10: ATT = 3.5697, SE = 2.8969, p = 0.28533\n",
      "📊 Diagnostic plots saved for CAT_Anti_epileptica\n",
      "🏆 Best result for CAT_Anti_epileptica → Seed 3 | SE = 2.1685\n",
      "\n",
      "🚀 Running OLS for CAT_Antidepressiva\n",
      "✅ CAT_Antidepressiva | Seed 1: ATT = 1.9887, SE = 1.2898, p = 0.19796\n",
      "✅ CAT_Antidepressiva | Seed 2: ATT = 1.6086, SE = 0.7266, p = 0.09123\n",
      "✅ CAT_Antidepressiva | Seed 3: ATT = 2.0278, SE = 0.7553, p = 0.05495\n",
      "✅ CAT_Antidepressiva | Seed 4: ATT = 1.9525, SE = 0.7110, p = 0.05158\n",
      "✅ CAT_Antidepressiva | Seed 5: ATT = 2.1264, SE = 1.2402, p = 0.16156\n",
      "✅ CAT_Antidepressiva | Seed 6: ATT = 1.7532, SE = 0.8256, p = 0.10095\n",
      "✅ CAT_Antidepressiva | Seed 7: ATT = 1.6148, SE = 1.3041, p = 0.28332\n",
      "✅ CAT_Antidepressiva | Seed 8: ATT = 1.5767, SE = 1.0398, p = 0.20404\n",
      "✅ CAT_Antidepressiva | Seed 9: ATT = 1.6411, SE = 0.7856, p = 0.10496\n",
      "✅ CAT_Antidepressiva | Seed 10: ATT = 1.6566, SE = 0.9033, p = 0.14060\n",
      "📊 Diagnostic plots saved for CAT_Antidepressiva\n",
      "🏆 Best result for CAT_Antidepressiva → Seed 4 | SE = 0.7110\n",
      "\n",
      "🚀 Running OLS for CAT_Antipsychotica\n",
      "✅ CAT_Antipsychotica | Seed 1: ATT = 1.0746, SE = 2.0678, p = 0.63071\n",
      "✅ CAT_Antipsychotica | Seed 2: ATT = 0.4104, SE = 2.1928, p = 0.86065\n",
      "✅ CAT_Antipsychotica | Seed 3: ATT = 0.4890, SE = 1.2720, p = 0.72020\n",
      "✅ CAT_Antipsychotica | Seed 4: ATT = 0.8743, SE = 0.9887, p = 0.42651\n",
      "✅ CAT_Antipsychotica | Seed 5: ATT = 0.9548, SE = 1.2644, p = 0.49219\n",
      "✅ CAT_Antipsychotica | Seed 6: ATT = 0.8630, SE = 1.0965, p = 0.47529\n",
      "✅ CAT_Antipsychotica | Seed 7: ATT = 0.8736, SE = 0.9567, p = 0.41281\n",
      "✅ CAT_Antipsychotica | Seed 8: ATT = 0.9238, SE = 1.5586, p = 0.58525\n",
      "✅ CAT_Antipsychotica | Seed 9: ATT = -0.3686, SE = 1.5726, p = 0.82618\n",
      "✅ CAT_Antipsychotica | Seed 10: ATT = 1.1478, SE = 1.4138, p = 0.46242\n",
      "📊 Diagnostic plots saved for CAT_Antipsychotica\n",
      "🏆 Best result for CAT_Antipsychotica → Seed 7 | SE = 0.9567\n",
      "\n",
      "🚀 Running OLS for CAT_ALL_PSYCHOTROPICS_EXCL_BENZO\n",
      "✅ CAT_ALL_PSYCHOTROPICS_EXCL_BENZO | Seed 1: ATT = 2.2140, SE = 1.1494, p = 0.12635\n",
      "✅ CAT_ALL_PSYCHOTROPICS_EXCL_BENZO | Seed 2: ATT = 1.9736, SE = 0.7587, p = 0.05998\n",
      "✅ CAT_ALL_PSYCHOTROPICS_EXCL_BENZO | Seed 3: ATT = 2.0412, SE = 0.9412, p = 0.09595\n",
      "✅ CAT_ALL_PSYCHOTROPICS_EXCL_BENZO | Seed 4: ATT = 2.1270, SE = 0.7024, p = 0.03884\n",
      "✅ CAT_ALL_PSYCHOTROPICS_EXCL_BENZO | Seed 5: ATT = 1.8959, SE = 0.7201, p = 0.05802\n",
      "✅ CAT_ALL_PSYCHOTROPICS_EXCL_BENZO | Seed 6: ATT = 1.8219, SE = 1.1018, p = 0.17357\n",
      "✅ CAT_ALL_PSYCHOTROPICS_EXCL_BENZO | Seed 7: ATT = 2.0548, SE = 0.9091, p = 0.08665\n",
      "✅ CAT_ALL_PSYCHOTROPICS_EXCL_BENZO | Seed 8: ATT = 1.6503, SE = 0.7095, p = 0.08059\n",
      "✅ CAT_ALL_PSYCHOTROPICS_EXCL_BENZO | Seed 9: ATT = 2.5338, SE = 0.7549, p = 0.02840\n",
      "✅ CAT_ALL_PSYCHOTROPICS_EXCL_BENZO | Seed 10: ATT = 2.1473, SE = 1.0229, p = 0.10374\n",
      "📊 Diagnostic plots saved for CAT_ALL_PSYCHOTROPICS_EXCL_BENZO\n",
      "🏆 Best result for CAT_ALL_PSYCHOTROPICS_EXCL_BENZO → Seed 4 | SE = 0.7024\n",
      "\n",
      "🚀 Running OLS for CAT_ALL_PSYCHOTROPICS_EXCL_SEDATIVES_HYPNOTICS\n",
      "✅ CAT_ALL_PSYCHOTROPICS_EXCL_SEDATIVES_HYPNOTICS | Seed 1: ATT = 1.6222, SE = 0.9984, p = 0.17954\n",
      "✅ CAT_ALL_PSYCHOTROPICS_EXCL_SEDATIVES_HYPNOTICS | Seed 2: ATT = 1.9102, SE = 0.9629, p = 0.11829\n",
      "✅ CAT_ALL_PSYCHOTROPICS_EXCL_SEDATIVES_HYPNOTICS | Seed 3: ATT = 1.5592, SE = 0.7435, p = 0.10400\n",
      "✅ CAT_ALL_PSYCHOTROPICS_EXCL_SEDATIVES_HYPNOTICS | Seed 4: ATT = 2.0426, SE = 0.9042, p = 0.08677\n",
      "✅ CAT_ALL_PSYCHOTROPICS_EXCL_SEDATIVES_HYPNOTICS | Seed 5: ATT = 1.3685, SE = 0.7921, p = 0.15910\n",
      "✅ CAT_ALL_PSYCHOTROPICS_EXCL_SEDATIVES_HYPNOTICS | Seed 6: ATT = 1.5706, SE = 0.7212, p = 0.09497\n",
      "✅ CAT_ALL_PSYCHOTROPICS_EXCL_SEDATIVES_HYPNOTICS | Seed 7: ATT = 1.9649, SE = 0.9735, p = 0.11372\n",
      "✅ CAT_ALL_PSYCHOTROPICS_EXCL_SEDATIVES_HYPNOTICS | Seed 8: ATT = 1.9737, SE = 0.8509, p = 0.08118\n",
      "✅ CAT_ALL_PSYCHOTROPICS_EXCL_SEDATIVES_HYPNOTICS | Seed 9: ATT = 1.8011, SE = 0.9944, p = 0.14435\n",
      "✅ CAT_ALL_PSYCHOTROPICS_EXCL_SEDATIVES_HYPNOTICS | Seed 10: ATT = 1.5455, SE = 1.1259, p = 0.24178\n",
      "📊 Diagnostic plots saved for CAT_ALL_PSYCHOTROPICS_EXCL_SEDATIVES_HYPNOTICS\n",
      "🏆 Best result for CAT_ALL_PSYCHOTROPICS_EXCL_SEDATIVES_HYPNOTICS → Seed 6 | SE = 0.7212\n",
      "\n",
      "🚀 Running OLS for CAT_ALL_PSYCHOTROPICS\n",
      "✅ CAT_ALL_PSYCHOTROPICS | Seed 1: ATT = 1.6385, SE = 0.7309, p = 0.08845\n",
      "✅ CAT_ALL_PSYCHOTROPICS | Seed 2: ATT = 1.6962, SE = 0.8305, p = 0.11066\n",
      "✅ CAT_ALL_PSYCHOTROPICS | Seed 3: ATT = 2.0547, SE = 0.6360, p = 0.03196\n",
      "✅ CAT_ALL_PSYCHOTROPICS | Seed 4: ATT = 1.9882, SE = 0.6137, p = 0.03168\n",
      "✅ CAT_ALL_PSYCHOTROPICS | Seed 5: ATT = 1.9715, SE = 0.8186, p = 0.07369\n",
      "✅ CAT_ALL_PSYCHOTROPICS | Seed 6: ATT = 1.8154, SE = 0.7647, p = 0.07648\n",
      "✅ CAT_ALL_PSYCHOTROPICS | Seed 7: ATT = 1.7481, SE = 0.6683, p = 0.05908\n",
      "✅ CAT_ALL_PSYCHOTROPICS | Seed 8: ATT = 1.7594, SE = 0.9298, p = 0.13140\n",
      "✅ CAT_ALL_PSYCHOTROPICS | Seed 9: ATT = 1.8717, SE = 0.8833, p = 0.10148\n",
      "✅ CAT_ALL_PSYCHOTROPICS | Seed 10: ATT = 1.8228, SE = 0.6300, p = 0.04441\n",
      "📊 Diagnostic plots saved for CAT_ALL_PSYCHOTROPICS\n",
      "🏆 Best result for CAT_ALL_PSYCHOTROPICS → Seed 4 | SE = 0.6137\n",
      "\n",
      "🚀 Running OLS for CAT_ALL\n",
      "✅ CAT_ALL | Seed 1: ATT = 2.1709, SE = 1.2447, p = 0.15608\n",
      "✅ CAT_ALL | Seed 2: ATT = 1.8525, SE = 0.6436, p = 0.04509\n",
      "✅ CAT_ALL | Seed 3: ATT = 1.9769, SE = 0.6953, p = 0.04671\n",
      "✅ CAT_ALL | Seed 4: ATT = 1.8024, SE = 0.8610, p = 0.10444\n",
      "✅ CAT_ALL | Seed 5: ATT = 2.1548, SE = 1.0414, p = 0.10735\n",
      "✅ CAT_ALL | Seed 6: ATT = 1.8669, SE = 0.7623, p = 0.07051\n",
      "✅ CAT_ALL | Seed 7: ATT = 1.9866, SE = 1.0131, p = 0.12144\n",
      "✅ CAT_ALL | Seed 8: ATT = 2.5515, SE = 0.8262, p = 0.03664\n",
      "✅ CAT_ALL | Seed 9: ATT = 2.3417, SE = 0.7450, p = 0.03474\n",
      "✅ CAT_ALL | Seed 10: ATT = 2.2052, SE = 0.7628, p = 0.04453\n",
      "📊 Diagnostic plots saved for CAT_ALL\n",
      "🏆 Best result for CAT_ALL → Seed 2 | SE = 0.6436\n",
      "\n",
      "🎯 All summary files saved.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import KFold\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import t, probplot\n",
    "import xgboost as xgb\n",
    "\n",
    "# -----------------------------\n",
    "# Configuration\n",
    "# -----------------------------\n",
    "seeds = list(range(1, 11))\n",
    "imputations = 5\n",
    "output_folder = \"outputs\"\n",
    "\n",
    "# -----------------------------\n",
    "# Diagnostic Plotting Function\n",
    "# -----------------------------\n",
    "def create_diagnostic_plots(residuals_data, fitted_data, group_name):\n",
    "    \"\"\"Create 4 diagnostic plots for model validation\"\"\"\n",
    "    plots_dir = os.path.join(output_folder, \"plots\")\n",
    "    os.makedirs(plots_dir, exist_ok=True)\n",
    "    \n",
    "    # Flatten the collected data\n",
    "    all_residuals = np.concatenate(residuals_data)\n",
    "    all_fitted = np.concatenate(fitted_data)\n",
    "    \n",
    "    # Create figure with 2x2 subplots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    fig.suptitle(f'Diagnostic Plots - {group_name}', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Residuals vs Fitted\n",
    "    axes[0,0].scatter(all_fitted, all_residuals, alpha=0.6, s=20)\n",
    "    axes[0,0].axhline(y=0, color='red', linestyle='--', alpha=0.8)\n",
    "    axes[0,0].set_xlabel('Fitted Values')\n",
    "    axes[0,0].set_ylabel('Residuals')\n",
    "    axes[0,0].set_title('Residuals vs Fitted')\n",
    "    axes[0,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. QQ Plot\n",
    "    probplot(all_residuals, dist=\"norm\", plot=axes[0,1])\n",
    "    axes[0,1].set_title('Q-Q Plot (Normal)')\n",
    "    axes[0,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Residual Histogram\n",
    "    axes[1,0].hist(all_residuals, bins=30, density=True, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    axes[1,0].set_xlabel('Residuals')\n",
    "    axes[1,0].set_ylabel('Density')\n",
    "    axes[1,0].set_title('Residual Distribution')\n",
    "    axes[1,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Scale-Location Plot\n",
    "    sqrt_abs_residuals = np.sqrt(np.abs(all_residuals))\n",
    "    axes[1,1].scatter(all_fitted, sqrt_abs_residuals, alpha=0.6, s=20)\n",
    "    axes[1,1].set_xlabel('Fitted Values')\n",
    "    axes[1,1].set_ylabel('√|Residuals|')\n",
    "    axes[1,1].set_title('Scale-Location Plot')\n",
    "    axes[1,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save plot\n",
    "    plot_filename = os.path.join(plots_dir, f'{group_name}.png')\n",
    "    plt.savefig(plot_filename, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"📊 Diagnostic plots saved for {group_name}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Rubin's Rule\n",
    "# -----------------------------\n",
    "def rubins_pool(estimates, ses):\n",
    "    m = len(estimates)\n",
    "    q_bar = np.mean(estimates)\n",
    "    u_bar = np.mean(np.square(ses))\n",
    "    b_m = np.var(estimates, ddof=1)\n",
    "    total_var = u_bar + ((1 + 1/m) * b_m)\n",
    "    total_se = np.sqrt(total_var)\n",
    "    ci_lower = q_bar - 1.96 * total_se\n",
    "    ci_upper = q_bar + 1.96 * total_se\n",
    "    p_value = 2 * (1 - t.cdf(np.abs(q_bar / total_se), df=m-1))\n",
    "    rounded_p = round(p_value, 5)\n",
    "    formatted_p = \"< 0.00001\" if rounded_p <= 0.00001 else f\"{rounded_p:.5f}\"\n",
    "    return q_bar, total_se, ci_lower, ci_upper, formatted_p\n",
    "\n",
    "# -----------------------------\n",
    "# SMD + Variance Ratio\n",
    "# -----------------------------\n",
    "def calculate_smd_vr(X, T, weights):\n",
    "    treated = X[T == 1]\n",
    "    control = X[T == 0]\n",
    "    w_treated = weights[T == 1]\n",
    "    w_control = weights[T == 0]\n",
    "    smd, vr = [], []\n",
    "    for col in X.columns:\n",
    "        try:\n",
    "            m1, m0 = np.average(treated[col], weights=w_treated), np.average(control[col], weights=w_control)\n",
    "            s1 = np.sqrt(np.average((treated[col] - m1) ** 2, weights=w_treated))\n",
    "            s0 = np.sqrt(np.average((control[col] - m0) ** 2, weights=w_control))\n",
    "            pooled_sd = np.sqrt((s1 ** 2 + s0 ** 2) / 2)\n",
    "            smd.append((m1 - m0) / pooled_sd if pooled_sd > 0 else 0)\n",
    "            vr.append(s1**2 / s0**2 if s0**2 > 0 else 0)\n",
    "        except Exception:\n",
    "            smd.append(np.nan)\n",
    "            vr.append(np.nan)\n",
    "    return np.nanmean(smd), np.nanmean(vr)\n",
    "\n",
    "# -----------------------------\n",
    "# OLS Main Loop\n",
    "# -----------------------------\n",
    "def run_dml_with_trimmed_data(final_covariates_map):\n",
    "    att_results = []\n",
    "    balance_results = []\n",
    "\n",
    "    for group, covariates in final_covariates_map.items():\n",
    "        print(f\"\\n🚀 Running OLS for {group}\")\n",
    "        group_dir = os.path.join(output_folder, group)\n",
    "        os.makedirs(group_dir, exist_ok=True)\n",
    "\n",
    "        best_result = None\n",
    "        best_se = float(\"inf\")\n",
    "        \n",
    "        # Initialize lists to collect residuals and fitted values for diagnostic plots\n",
    "        group_residuals = []\n",
    "        group_fitted = []\n",
    "\n",
    "        for seed in seeds:\n",
    "            # Set random seed for this iteration\n",
    "            np.random.seed(seed)\n",
    "            \n",
    "            att_list, se_list, r2_list, rmse_list, smd_list, vr_list = [], [], [], [], [], []\n",
    "\n",
    "            for imp in range(1, imputations + 1):\n",
    "                file_path = os.path.join(group_dir, f\"trimmed_data_imp{imp}.pkl\")\n",
    "                if not os.path.exists(file_path):\n",
    "                    continue\n",
    "\n",
    "                df = pd.read_pickle(file_path)\n",
    "                if group not in df.columns or \"iptw\" not in df.columns:\n",
    "                    continue\n",
    "\n",
    "                # Add bootstrap sampling with seed-based randomization\n",
    "                n_samples = len(df)\n",
    "                bootstrap_idx = np.random.choice(n_samples, size=n_samples, replace=True)\n",
    "                df_bootstrap = df.iloc[bootstrap_idx].reset_index(drop=True)\n",
    "\n",
    "                X = df_bootstrap[covariates].copy()\n",
    "                T = df_bootstrap[group]\n",
    "                Y = df_bootstrap[\"caps5_change_baseline\"]\n",
    "                W = df_bootstrap[\"iptw\"]\n",
    "\n",
    "                try:\n",
    "                    # Create design matrix with treatment variable and covariates\n",
    "                    X_ols = pd.concat([T, X], axis=1)\n",
    "                    X_ols = sm.add_constant(X_ols)\n",
    "                    \n",
    "                    # Fit weighted OLS with robust standard errors\n",
    "                    ols_model = sm.WLS(Y, X_ols, weights=W).fit(cov_type='HC1')\n",
    "                    \n",
    "                    # Extract treatment effect (coefficient of treatment variable)\n",
    "                    att = ols_model.params[group]  # Treatment coefficient\n",
    "                    se = ols_model.bse[group]  # Robust standard error for treatment\n",
    "                    \n",
    "                    att_list.append(att)\n",
    "                    se_list.append(se)\n",
    "\n",
    "                    # Calculate model fit statistics\n",
    "                    Y_pred = ols_model.fittedvalues\n",
    "                    residuals = ols_model.resid\n",
    "                    rmse = mean_squared_error(Y, Y_pred, squared=False)\n",
    "                    r2 = ols_model.rsquared\n",
    "                    r2_list.append(r2)\n",
    "                    rmse_list.append(rmse)\n",
    "                    \n",
    "                    # Collect residuals and fitted values for diagnostic plots\n",
    "                    group_residuals.append(residuals.values)\n",
    "                    group_fitted.append(Y_pred.values)\n",
    "\n",
    "                    smd, vr = calculate_smd_vr(X, T, W)\n",
    "                    smd_list.append(smd)\n",
    "                    vr_list.append(vr)\n",
    "                except Exception as e:\n",
    "                    print(f\"⚠️ Error in {group}, seed {seed}, imp {imp}: {e}\")\n",
    "\n",
    "            if att_list:\n",
    "                att, se, ci_l, ci_u, p_val = rubins_pool(att_list, se_list)\n",
    "                avg_r2 = np.mean(r2_list)\n",
    "                avg_rmse = np.mean(rmse_list)\n",
    "                avg_smd = np.mean(smd_list)\n",
    "                avg_vr = np.mean(vr_list)\n",
    "\n",
    "                balance_results.append({\n",
    "                    \"group\": group, \"seed\": seed, \"smd\": avg_smd, \"vr\": avg_vr\n",
    "                })\n",
    "\n",
    "                if se < best_se:\n",
    "                    best_se = se\n",
    "                    best_result = {\n",
    "                        \"group\": group, \"seed\": seed, \"att\": att, \"se\": se,\n",
    "                        \"ci_lower\": ci_l, \"ci_upper\": ci_u, \"p_value\": p_val,\n",
    "                        \"r2\": avg_r2, \"rmse\": avg_rmse\n",
    "                    }\n",
    "\n",
    "                print(f\"✅ {group} | Seed {seed}: ATT = {att:.4f}, SE = {se:.4f}, p = {p_val}\")\n",
    "            else:\n",
    "                print(f\"⚠️ No valid results for {group} | Seed {seed}\")\n",
    "\n",
    "        # Create diagnostic plots for this group\n",
    "        if group_residuals and group_fitted:\n",
    "            create_diagnostic_plots(group_residuals, group_fitted, group)\n",
    "\n",
    "        if best_result:\n",
    "            att_results.append(best_result)\n",
    "            print(f\"🏆 Best result for {group} → Seed {best_result['seed']} | SE = {best_result['se']:.4f}\")\n",
    "\n",
    "    # Save final output\n",
    "    pd.DataFrame(att_results).to_excel(\"ols_rubin_summary_cats.xlsx\", index=False)\n",
    "    pd.DataFrame(balance_results).to_excel(\"smd_vr_summary_cats.xlsx\", index=False)\n",
    "    print(\"\\n🎯 All summary files saved.\")\n",
    "\n",
    "run_dml_with_trimmed_data(final_covariates_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "711869a8-2bfe-485e-aff1-ce2d761df39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unweighted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "354cb95e-628a-4234-b37a-085d0fd153ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🚀 Running OLS for CAT_ADHD\n",
      "✅ CAT_ADHD | Seed 1: ATT = 0.8977, SE = 3.1989, p = 0.79292\n",
      "✅ CAT_ADHD | Seed 2: ATT = 0.6329, SE = 2.8531, p = 0.83530\n",
      "✅ CAT_ADHD | Seed 3: ATT = -0.1555, SE = 3.5564, p = 0.96721\n",
      "✅ CAT_ADHD | Seed 4: ATT = 0.8576, SE = 4.7931, p = 0.86670\n",
      "✅ CAT_ADHD | Seed 5: ATT = 0.7333, SE = 2.6519, p = 0.79584\n",
      "✅ CAT_ADHD | Seed 6: ATT = -1.4120, SE = 2.4237, p = 0.59144\n",
      "✅ CAT_ADHD | Seed 7: ATT = 1.2545, SE = 2.9350, p = 0.69107\n",
      "✅ CAT_ADHD | Seed 8: ATT = 0.0107, SE = 2.8703, p = 0.99721\n",
      "✅ CAT_ADHD | Seed 9: ATT = -0.1739, SE = 3.5807, p = 0.96359\n",
      "✅ CAT_ADHD | Seed 10: ATT = -0.0825, SE = 3.9422, p = 0.98431\n",
      "📊 Diagnostic plots saved for CAT_ADHD\n",
      "🏆 Best result for CAT_ADHD → Seed 6 | SE = 2.4237\n",
      "\n",
      "🚀 Running OLS for CAT_Aceetanilidederivaten\n",
      "✅ CAT_Aceetanilidederivaten | Seed 1: ATT = 1.7266, SE = 6.9697, p = 0.81654\n",
      "✅ CAT_Aceetanilidederivaten | Seed 2: ATT = 2.7754, SE = 4.4979, p = 0.57060\n",
      "✅ CAT_Aceetanilidederivaten | Seed 3: ATT = 3.0011, SE = 6.7271, p = 0.67860\n",
      "✅ CAT_Aceetanilidederivaten | Seed 4: ATT = 1.8441, SE = 4.3621, p = 0.69422\n",
      "✅ CAT_Aceetanilidederivaten | Seed 5: ATT = 5.0418, SE = 4.4609, p = 0.32156\n",
      "✅ CAT_Aceetanilidederivaten | Seed 6: ATT = 1.4382, SE = 4.2729, p = 0.75335\n",
      "✅ CAT_Aceetanilidederivaten | Seed 7: ATT = 4.0841, SE = 5.7828, p = 0.51900\n",
      "✅ CAT_Aceetanilidederivaten | Seed 8: ATT = 1.5617, SE = 4.9378, p = 0.76762\n",
      "✅ CAT_Aceetanilidederivaten | Seed 9: ATT = 1.8939, SE = 2.6305, p = 0.51138\n",
      "✅ CAT_Aceetanilidederivaten | Seed 10: ATT = 2.3961, SE = 4.3453, p = 0.61070\n",
      "📊 Diagnostic plots saved for CAT_Aceetanilidederivaten\n",
      "🏆 Best result for CAT_Aceetanilidederivaten → Seed 9 | SE = 2.6305\n",
      "\n",
      "🚀 Running OLS for CAT_Z_drugs\n",
      "✅ CAT_Z_drugs | Seed 1: ATT = 6.7230, SE = 4.4233, p = 0.20317\n",
      "✅ CAT_Z_drugs | Seed 2: ATT = 7.2035, SE = 4.2473, p = 0.16512\n",
      "✅ CAT_Z_drugs | Seed 3: ATT = 7.0783, SE = 4.7152, p = 0.20771\n",
      "✅ CAT_Z_drugs | Seed 4: ATT = 4.6837, SE = 4.6370, p = 0.36961\n",
      "✅ CAT_Z_drugs | Seed 5: ATT = 7.0411, SE = 4.2757, p = 0.17495\n",
      "✅ CAT_Z_drugs | Seed 6: ATT = 6.4190, SE = 3.7897, p = 0.16555\n",
      "✅ CAT_Z_drugs | Seed 7: ATT = 5.2282, SE = 4.2383, p = 0.28490\n",
      "✅ CAT_Z_drugs | Seed 8: ATT = 5.5876, SE = 3.5740, p = 0.19300\n",
      "✅ CAT_Z_drugs | Seed 9: ATT = 7.0802, SE = 4.1275, p = 0.16143\n",
      "✅ CAT_Z_drugs | Seed 10: ATT = 6.8351, SE = 3.8780, p = 0.15277\n",
      "📊 Diagnostic plots saved for CAT_Z_drugs\n",
      "🏆 Best result for CAT_Z_drugs → Seed 8 | SE = 3.5740\n",
      "\n",
      "🚀 Running OLS for CAT_Opioden\n",
      "✅ CAT_Opioden | Seed 1: ATT = 2.5307, SE = 4.0299, p = 0.56408\n",
      "✅ CAT_Opioden | Seed 2: ATT = 4.7808, SE = 3.7780, p = 0.27441\n",
      "✅ CAT_Opioden | Seed 3: ATT = 3.7648, SE = 4.2010, p = 0.42081\n",
      "✅ CAT_Opioden | Seed 4: ATT = 3.1691, SE = 4.3704, p = 0.50852\n",
      "✅ CAT_Opioden | Seed 5: ATT = 3.2665, SE = 3.6299, p = 0.41905\n",
      "✅ CAT_Opioden | Seed 6: ATT = 4.2291, SE = 5.5722, p = 0.49013\n",
      "✅ CAT_Opioden | Seed 7: ATT = 5.3511, SE = 3.6927, p = 0.22090\n",
      "✅ CAT_Opioden | Seed 8: ATT = 3.9954, SE = 4.1563, p = 0.39084\n",
      "✅ CAT_Opioden | Seed 9: ATT = 3.0610, SE = 4.0716, p = 0.49399\n",
      "✅ CAT_Opioden | Seed 10: ATT = 5.2982, SE = 5.1543, p = 0.36209\n",
      "📊 Diagnostic plots saved for CAT_Opioden\n",
      "🏆 Best result for CAT_Opioden → Seed 5 | SE = 3.6299\n",
      "\n",
      "🚀 Running OLS for CAT_NSAIDs\n",
      "✅ CAT_NSAIDs | Seed 1: ATT = -1.9830, SE = 5.4084, p = 0.73245\n",
      "✅ CAT_NSAIDs | Seed 2: ATT = -5.0071, SE = 5.7611, p = 0.43381\n",
      "✅ CAT_NSAIDs | Seed 3: ATT = 1.0185, SE = 7.0426, p = 0.89200\n",
      "✅ CAT_NSAIDs | Seed 4: ATT = -0.0792, SE = 7.2036, p = 0.99176\n",
      "✅ CAT_NSAIDs | Seed 5: ATT = -3.5837, SE = 5.0718, p = 0.51880\n",
      "✅ CAT_NSAIDs | Seed 6: ATT = -4.8065, SE = 5.3959, p = 0.42339\n",
      "✅ CAT_NSAIDs | Seed 7: ATT = -2.7663, SE = 5.3973, p = 0.63528\n",
      "✅ CAT_NSAIDs | Seed 8: ATT = -3.1554, SE = 6.0005, p = 0.62679\n",
      "✅ CAT_NSAIDs | Seed 9: ATT = -2.2439, SE = 6.0821, p = 0.73088\n",
      "✅ CAT_NSAIDs | Seed 10: ATT = -1.3461, SE = 6.1168, p = 0.83660\n",
      "📊 Diagnostic plots saved for CAT_NSAIDs\n",
      "🏆 Best result for CAT_NSAIDs → Seed 5 | SE = 5.0718\n",
      "\n",
      "🚀 Running OLS for CAT_Benzodiazepine\n",
      "✅ CAT_Benzodiazepine | Seed 1: ATT = 0.5735, SE = 0.8565, p = 0.53984\n",
      "✅ CAT_Benzodiazepine | Seed 2: ATT = -0.0804, SE = 0.9844, p = 0.93881\n",
      "✅ CAT_Benzodiazepine | Seed 3: ATT = 0.4601, SE = 0.8670, p = 0.62370\n",
      "✅ CAT_Benzodiazepine | Seed 4: ATT = 0.3546, SE = 0.9409, p = 0.72543\n",
      "✅ CAT_Benzodiazepine | Seed 5: ATT = 0.1768, SE = 1.0514, p = 0.87465\n",
      "✅ CAT_Benzodiazepine | Seed 6: ATT = 0.4073, SE = 1.5533, p = 0.80611\n",
      "✅ CAT_Benzodiazepine | Seed 7: ATT = -0.3402, SE = 1.0713, p = 0.76674\n",
      "✅ CAT_Benzodiazepine | Seed 8: ATT = 0.3176, SE = 0.8087, p = 0.71456\n",
      "✅ CAT_Benzodiazepine | Seed 9: ATT = 0.6894, SE = 1.1042, p = 0.56625\n",
      "✅ CAT_Benzodiazepine | Seed 10: ATT = -0.1308, SE = 1.5582, p = 0.93716\n",
      "📊 Diagnostic plots saved for CAT_Benzodiazepine\n",
      "🏆 Best result for CAT_Benzodiazepine → Seed 8 | SE = 0.8087\n",
      "\n",
      "🚀 Running OLS for CAT_Antihypertensiva\n",
      "✅ CAT_Antihypertensiva | Seed 1: ATT = -0.7702, SE = 5.7546, p = 0.89999\n",
      "✅ CAT_Antihypertensiva | Seed 2: ATT = 1.3941, SE = 4.4797, p = 0.77119\n",
      "✅ CAT_Antihypertensiva | Seed 3: ATT = 0.8459, SE = 5.5235, p = 0.88570\n",
      "✅ CAT_Antihypertensiva | Seed 4: ATT = 0.2492, SE = 3.9282, p = 0.95246\n",
      "✅ CAT_Antihypertensiva | Seed 5: ATT = -2.3603, SE = 3.4353, p = 0.52979\n",
      "✅ CAT_Antihypertensiva | Seed 6: ATT = -0.6732, SE = 5.1371, p = 0.90207\n",
      "✅ CAT_Antihypertensiva | Seed 7: ATT = 2.0527, SE = 6.4407, p = 0.76589\n",
      "✅ CAT_Antihypertensiva | Seed 8: ATT = 0.9165, SE = 3.5170, p = 0.80728\n",
      "✅ CAT_Antihypertensiva | Seed 9: ATT = -1.5651, SE = 3.9846, p = 0.71450\n",
      "✅ CAT_Antihypertensiva | Seed 10: ATT = 1.5857, SE = 6.0259, p = 0.80544\n",
      "📊 Diagnostic plots saved for CAT_Antihypertensiva\n",
      "🏆 Best result for CAT_Antihypertensiva → Seed 5 | SE = 3.4353\n",
      "\n",
      "🚀 Running OLS for CAT_Antihistaminica\n",
      "✅ CAT_Antihistaminica | Seed 1: ATT = -4.0196, SE = 4.0617, p = 0.37838\n",
      "✅ CAT_Antihistaminica | Seed 2: ATT = -2.8399, SE = 3.4389, p = 0.45531\n",
      "✅ CAT_Antihistaminica | Seed 3: ATT = -2.4102, SE = 3.8317, p = 0.56348\n",
      "✅ CAT_Antihistaminica | Seed 4: ATT = -6.3451, SE = 4.7155, p = 0.24965\n",
      "✅ CAT_Antihistaminica | Seed 5: ATT = -4.4268, SE = 3.0007, p = 0.21418\n",
      "✅ CAT_Antihistaminica | Seed 6: ATT = -2.6115, SE = 3.2361, p = 0.46493\n",
      "✅ CAT_Antihistaminica | Seed 7: ATT = -2.1966, SE = 3.8689, p = 0.60054\n",
      "✅ CAT_Antihistaminica | Seed 8: ATT = -2.5331, SE = 4.2655, p = 0.58456\n",
      "✅ CAT_Antihistaminica | Seed 9: ATT = -4.3615, SE = 3.5599, p = 0.28772\n",
      "✅ CAT_Antihistaminica | Seed 10: ATT = -1.6494, SE = 3.7683, p = 0.68420\n",
      "📊 Diagnostic plots saved for CAT_Antihistaminica\n",
      "🏆 Best result for CAT_Antihistaminica → Seed 5 | SE = 3.0007\n",
      "\n",
      "🚀 Running OLS for CAT_Anti_epileptica\n",
      "✅ CAT_Anti_epileptica | Seed 1: ATT = 2.9164, SE = 2.9580, p = 0.37999\n",
      "✅ CAT_Anti_epileptica | Seed 2: ATT = 2.2092, SE = 3.3013, p = 0.54003\n",
      "✅ CAT_Anti_epileptica | Seed 3: ATT = 2.7496, SE = 2.4762, p = 0.32907\n",
      "✅ CAT_Anti_epileptica | Seed 4: ATT = 3.6304, SE = 2.7186, p = 0.25267\n",
      "✅ CAT_Anti_epileptica | Seed 5: ATT = 3.2517, SE = 3.0688, p = 0.34907\n",
      "✅ CAT_Anti_epileptica | Seed 6: ATT = 1.7588, SE = 3.4295, p = 0.63509\n",
      "✅ CAT_Anti_epileptica | Seed 7: ATT = 2.7071, SE = 2.7200, p = 0.37595\n",
      "✅ CAT_Anti_epileptica | Seed 8: ATT = 2.1661, SE = 3.1024, p = 0.52351\n",
      "✅ CAT_Anti_epileptica | Seed 9: ATT = 1.9924, SE = 2.5840, p = 0.48369\n",
      "✅ CAT_Anti_epileptica | Seed 10: ATT = 3.3061, SE = 2.7542, p = 0.29621\n",
      "📊 Diagnostic plots saved for CAT_Anti_epileptica\n",
      "🏆 Best result for CAT_Anti_epileptica → Seed 3 | SE = 2.4762\n",
      "\n",
      "🚀 Running OLS for CAT_Antidepressiva\n",
      "✅ CAT_Antidepressiva | Seed 1: ATT = 1.3585, SE = 1.1921, p = 0.31808\n",
      "✅ CAT_Antidepressiva | Seed 2: ATT = 1.0647, SE = 0.5917, p = 0.14635\n",
      "✅ CAT_Antidepressiva | Seed 3: ATT = 1.4549, SE = 0.7045, p = 0.10781\n",
      "✅ CAT_Antidepressiva | Seed 4: ATT = 1.5396, SE = 0.6884, p = 0.08897\n",
      "✅ CAT_Antidepressiva | Seed 5: ATT = 1.5732, SE = 1.2139, p = 0.26471\n",
      "✅ CAT_Antidepressiva | Seed 6: ATT = 1.2006, SE = 0.7865, p = 0.20159\n",
      "✅ CAT_Antidepressiva | Seed 7: ATT = 1.0723, SE = 1.0865, p = 0.37956\n",
      "✅ CAT_Antidepressiva | Seed 8: ATT = 0.8356, SE = 0.9288, p = 0.41919\n",
      "✅ CAT_Antidepressiva | Seed 9: ATT = 1.1278, SE = 0.7495, p = 0.20680\n",
      "✅ CAT_Antidepressiva | Seed 10: ATT = 1.1435, SE = 0.8954, p = 0.27069\n",
      "📊 Diagnostic plots saved for CAT_Antidepressiva\n",
      "🏆 Best result for CAT_Antidepressiva → Seed 2 | SE = 0.5917\n",
      "\n",
      "🚀 Running OLS for CAT_Antipsychotica\n",
      "✅ CAT_Antipsychotica | Seed 1: ATT = 1.3286, SE = 2.0344, p = 0.54935\n",
      "✅ CAT_Antipsychotica | Seed 2: ATT = 1.0812, SE = 2.1179, p = 0.63658\n",
      "✅ CAT_Antipsychotica | Seed 3: ATT = 1.0297, SE = 1.4775, p = 0.52423\n",
      "✅ CAT_Antipsychotica | Seed 4: ATT = 1.3871, SE = 1.0985, p = 0.27528\n",
      "✅ CAT_Antipsychotica | Seed 5: ATT = 1.6142, SE = 1.1999, p = 0.24974\n",
      "✅ CAT_Antipsychotica | Seed 6: ATT = 1.5272, SE = 1.1169, p = 0.24331\n",
      "✅ CAT_Antipsychotica | Seed 7: ATT = 1.4345, SE = 1.0390, p = 0.23955\n",
      "✅ CAT_Antipsychotica | Seed 8: ATT = 1.3284, SE = 1.6980, p = 0.47774\n",
      "✅ CAT_Antipsychotica | Seed 9: ATT = 0.3101, SE = 1.4435, p = 0.84041\n",
      "✅ CAT_Antipsychotica | Seed 10: ATT = 1.8826, SE = 1.4130, p = 0.25360\n",
      "📊 Diagnostic plots saved for CAT_Antipsychotica\n",
      "🏆 Best result for CAT_Antipsychotica → Seed 7 | SE = 1.0390\n",
      "\n",
      "🚀 Running OLS for CAT_ALL_PSYCHOTROPICS_EXCL_BENZO\n",
      "✅ CAT_ALL_PSYCHOTROPICS_EXCL_BENZO | Seed 1: ATT = 1.7256, SE = 1.1210, p = 0.19855\n",
      "✅ CAT_ALL_PSYCHOTROPICS_EXCL_BENZO | Seed 2: ATT = 1.6250, SE = 0.6777, p = 0.07455\n",
      "✅ CAT_ALL_PSYCHOTROPICS_EXCL_BENZO | Seed 3: ATT = 1.4986, SE = 0.7436, p = 0.11411\n",
      "✅ CAT_ALL_PSYCHOTROPICS_EXCL_BENZO | Seed 4: ATT = 1.8556, SE = 0.6224, p = 0.04069\n",
      "✅ CAT_ALL_PSYCHOTROPICS_EXCL_BENZO | Seed 5: ATT = 1.4394, SE = 0.6416, p = 0.08828\n",
      "✅ CAT_ALL_PSYCHOTROPICS_EXCL_BENZO | Seed 6: ATT = 1.3431, SE = 1.0047, p = 0.25224\n",
      "✅ CAT_ALL_PSYCHOTROPICS_EXCL_BENZO | Seed 7: ATT = 1.7349, SE = 0.7237, p = 0.07457\n",
      "✅ CAT_ALL_PSYCHOTROPICS_EXCL_BENZO | Seed 8: ATT = 1.2941, SE = 0.6209, p = 0.10554\n",
      "✅ CAT_ALL_PSYCHOTROPICS_EXCL_BENZO | Seed 9: ATT = 2.0343, SE = 0.6626, p = 0.03729\n",
      "✅ CAT_ALL_PSYCHOTROPICS_EXCL_BENZO | Seed 10: ATT = 1.9791, SE = 0.7716, p = 0.06232\n",
      "📊 Diagnostic plots saved for CAT_ALL_PSYCHOTROPICS_EXCL_BENZO\n",
      "🏆 Best result for CAT_ALL_PSYCHOTROPICS_EXCL_BENZO → Seed 8 | SE = 0.6209\n",
      "\n",
      "🚀 Running OLS for CAT_ALL_PSYCHOTROPICS_EXCL_SEDATIVES_HYPNOTICS\n",
      "✅ CAT_ALL_PSYCHOTROPICS_EXCL_SEDATIVES_HYPNOTICS | Seed 1: ATT = 1.2724, SE = 1.0627, p = 0.29728\n",
      "✅ CAT_ALL_PSYCHOTROPICS_EXCL_SEDATIVES_HYPNOTICS | Seed 2: ATT = 1.6061, SE = 0.7710, p = 0.10565\n",
      "✅ CAT_ALL_PSYCHOTROPICS_EXCL_SEDATIVES_HYPNOTICS | Seed 3: ATT = 1.3375, SE = 0.6435, p = 0.10621\n",
      "✅ CAT_ALL_PSYCHOTROPICS_EXCL_SEDATIVES_HYPNOTICS | Seed 4: ATT = 1.6692, SE = 0.5891, p = 0.04717\n",
      "✅ CAT_ALL_PSYCHOTROPICS_EXCL_SEDATIVES_HYPNOTICS | Seed 5: ATT = 1.0292, SE = 0.6630, p = 0.19555\n",
      "✅ CAT_ALL_PSYCHOTROPICS_EXCL_SEDATIVES_HYPNOTICS | Seed 6: ATT = 1.2702, SE = 0.7303, p = 0.15695\n",
      "✅ CAT_ALL_PSYCHOTROPICS_EXCL_SEDATIVES_HYPNOTICS | Seed 7: ATT = 1.6119, SE = 0.8447, p = 0.12902\n",
      "✅ CAT_ALL_PSYCHOTROPICS_EXCL_SEDATIVES_HYPNOTICS | Seed 8: ATT = 1.7196, SE = 0.6377, p = 0.05428\n",
      "✅ CAT_ALL_PSYCHOTROPICS_EXCL_SEDATIVES_HYPNOTICS | Seed 9: ATT = 1.4281, SE = 0.7341, p = 0.12360\n",
      "✅ CAT_ALL_PSYCHOTROPICS_EXCL_SEDATIVES_HYPNOTICS | Seed 10: ATT = 1.4018, SE = 0.9808, p = 0.22616\n",
      "📊 Diagnostic plots saved for CAT_ALL_PSYCHOTROPICS_EXCL_SEDATIVES_HYPNOTICS\n",
      "🏆 Best result for CAT_ALL_PSYCHOTROPICS_EXCL_SEDATIVES_HYPNOTICS → Seed 4 | SE = 0.5891\n",
      "\n",
      "🚀 Running OLS for CAT_ALL_PSYCHOTROPICS\n",
      "✅ CAT_ALL_PSYCHOTROPICS | Seed 1: ATT = 1.5728, SE = 0.6631, p = 0.07667\n",
      "✅ CAT_ALL_PSYCHOTROPICS | Seed 2: ATT = 1.6213, SE = 0.7059, p = 0.08322\n",
      "✅ CAT_ALL_PSYCHOTROPICS | Seed 3: ATT = 2.0095, SE = 0.5781, p = 0.02544\n",
      "✅ CAT_ALL_PSYCHOTROPICS | Seed 4: ATT = 1.9064, SE = 0.5822, p = 0.03067\n",
      "✅ CAT_ALL_PSYCHOTROPICS | Seed 5: ATT = 1.8955, SE = 0.7494, p = 0.06471\n",
      "✅ CAT_ALL_PSYCHOTROPICS | Seed 6: ATT = 1.6331, SE = 0.7667, p = 0.10019\n",
      "✅ CAT_ALL_PSYCHOTROPICS | Seed 7: ATT = 1.6926, SE = 0.6729, p = 0.06568\n",
      "✅ CAT_ALL_PSYCHOTROPICS | Seed 8: ATT = 1.5924, SE = 0.7836, p = 0.11194\n",
      "✅ CAT_ALL_PSYCHOTROPICS | Seed 9: ATT = 1.8543, SE = 0.8584, p = 0.09686\n",
      "✅ CAT_ALL_PSYCHOTROPICS | Seed 10: ATT = 1.8350, SE = 0.6417, p = 0.04594\n",
      "📊 Diagnostic plots saved for CAT_ALL_PSYCHOTROPICS\n",
      "🏆 Best result for CAT_ALL_PSYCHOTROPICS → Seed 3 | SE = 0.5781\n",
      "\n",
      "🚀 Running OLS for CAT_ALL\n",
      "✅ CAT_ALL | Seed 1: ATT = 2.0362, SE = 1.0470, p = 0.12369\n",
      "✅ CAT_ALL | Seed 2: ATT = 1.5943, SE = 0.5511, p = 0.04443\n",
      "✅ CAT_ALL | Seed 3: ATT = 1.7431, SE = 0.6202, p = 0.04830\n",
      "✅ CAT_ALL | Seed 4: ATT = 1.9036, SE = 0.9301, p = 0.11012\n",
      "✅ CAT_ALL | Seed 5: ATT = 1.9350, SE = 1.0178, p = 0.13006\n",
      "✅ CAT_ALL | Seed 6: ATT = 1.8633, SE = 0.7296, p = 0.06305\n",
      "✅ CAT_ALL | Seed 7: ATT = 1.9698, SE = 0.8581, p = 0.08336\n",
      "✅ CAT_ALL | Seed 8: ATT = 2.3800, SE = 0.6821, p = 0.02514\n",
      "✅ CAT_ALL | Seed 9: ATT = 2.3659, SE = 0.7029, p = 0.02815\n",
      "✅ CAT_ALL | Seed 10: ATT = 2.2462, SE = 0.6340, p = 0.02396\n",
      "📊 Diagnostic plots saved for CAT_ALL\n",
      "🏆 Best result for CAT_ALL → Seed 2 | SE = 0.5511\n",
      "\n",
      "🎯 All summary files saved.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import KFold\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import t, probplot\n",
    "import xgboost as xgb\n",
    "\n",
    "# -----------------------------\n",
    "# Configuration\n",
    "# -----------------------------\n",
    "seeds = list(range(1, 11))\n",
    "imputations = 5\n",
    "output_folder = \"outputs\"\n",
    "\n",
    "# -----------------------------\n",
    "# Diagnostic Plotting Function\n",
    "# -----------------------------\n",
    "def create_diagnostic_plots(residuals_data, fitted_data, group_name):\n",
    "    \"\"\"Create 4 diagnostic plots for model validation\"\"\"\n",
    "    plots_dir = os.path.join(output_folder, \"plots\")\n",
    "    os.makedirs(plots_dir, exist_ok=True)\n",
    "    \n",
    "    # Flatten the collected data\n",
    "    all_residuals = np.concatenate(residuals_data)\n",
    "    all_fitted = np.concatenate(fitted_data)\n",
    "    \n",
    "    # Create figure with 2x2 subplots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    fig.suptitle(f'Diagnostic Plots - {group_name}', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Residuals vs Fitted\n",
    "    axes[0,0].scatter(all_fitted, all_residuals, alpha=0.6, s=20)\n",
    "    axes[0,0].axhline(y=0, color='red', linestyle='--', alpha=0.8)\n",
    "    axes[0,0].set_xlabel('Fitted Values')\n",
    "    axes[0,0].set_ylabel('Residuals')\n",
    "    axes[0,0].set_title('Residuals vs Fitted')\n",
    "    axes[0,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. QQ Plot\n",
    "    probplot(all_residuals, dist=\"norm\", plot=axes[0,1])\n",
    "    axes[0,1].set_title('Q-Q Plot (Normal)')\n",
    "    axes[0,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Residual Histogram\n",
    "    axes[1,0].hist(all_residuals, bins=30, density=True, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    axes[1,0].set_xlabel('Residuals')\n",
    "    axes[1,0].set_ylabel('Density')\n",
    "    axes[1,0].set_title('Residual Distribution')\n",
    "    axes[1,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Scale-Location Plot\n",
    "    sqrt_abs_residuals = np.sqrt(np.abs(all_residuals))\n",
    "    axes[1,1].scatter(all_fitted, sqrt_abs_residuals, alpha=0.6, s=20)\n",
    "    axes[1,1].set_xlabel('Fitted Values')\n",
    "    axes[1,1].set_ylabel('√|Residuals|')\n",
    "    axes[1,1].set_title('Scale-Location Plot')\n",
    "    axes[1,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save plot\n",
    "    plot_filename = os.path.join(plots_dir, f'{group_name}_unweighted.png')\n",
    "    plt.savefig(plot_filename, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"📊 Diagnostic plots saved for {group_name}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Rubin's Rule\n",
    "# -----------------------------\n",
    "def rubins_pool(estimates, ses):\n",
    "    m = len(estimates)\n",
    "    q_bar = np.mean(estimates)\n",
    "    u_bar = np.mean(np.square(ses))\n",
    "    b_m = np.var(estimates, ddof=1)\n",
    "    total_var = u_bar + ((1 + 1/m) * b_m)\n",
    "    total_se = np.sqrt(total_var)\n",
    "    ci_lower = q_bar - 1.96 * total_se\n",
    "    ci_upper = q_bar + 1.96 * total_se\n",
    "    p_value = 2 * (1 - t.cdf(np.abs(q_bar / total_se), df=m-1))\n",
    "    rounded_p = round(p_value, 5)\n",
    "    formatted_p = \"< 0.00001\" if rounded_p <= 0.00001 else f\"{rounded_p:.5f}\"\n",
    "    return q_bar, total_se, ci_lower, ci_upper, formatted_p\n",
    "\n",
    "# -----------------------------\n",
    "# SMD + Variance Ratio\n",
    "# -----------------------------\n",
    "def calculate_smd_vr(X, T):\n",
    "    treated = X[T == 1]\n",
    "    control = X[T == 0]\n",
    "    smd, vr = [], []\n",
    "    for col in X.columns:\n",
    "        try:\n",
    "            m1, m0 = treated[col].mean(), control[col].mean()\n",
    "            s1 = treated[col].std()\n",
    "            s0 = control[col].std()\n",
    "            pooled_sd = np.sqrt((s1 ** 2 + s0 ** 2) / 2)\n",
    "            smd.append((m1 - m0) / pooled_sd if pooled_sd > 0 else 0)\n",
    "            vr.append(s1**2 / s0**2 if s0**2 > 0 else 0)\n",
    "        except Exception:\n",
    "            smd.append(np.nan)\n",
    "            vr.append(np.nan)\n",
    "    return np.nanmean(smd), np.nanmean(vr)\n",
    "\n",
    "# -----------------------------\n",
    "# OLS Main Loop\n",
    "# -----------------------------\n",
    "def run_dml_with_trimmed_data(final_covariates_map):\n",
    "    att_results = []\n",
    "    balance_results = []\n",
    "\n",
    "    for group, covariates in final_covariates_map.items():\n",
    "        print(f\"\\n🚀 Running OLS for {group}\")\n",
    "        group_dir = os.path.join(output_folder, group)\n",
    "        os.makedirs(group_dir, exist_ok=True)\n",
    "\n",
    "        best_result = None\n",
    "        best_se = float(\"inf\")\n",
    "        \n",
    "        # Initialize lists to collect residuals and fitted values for diagnostic plots\n",
    "        group_residuals = []\n",
    "        group_fitted = []\n",
    "\n",
    "        for seed in seeds:\n",
    "            # Set random seed for this iteration\n",
    "            np.random.seed(seed)\n",
    "            \n",
    "            att_list, se_list, r2_list, rmse_list, smd_list, vr_list = [], [], [], [], [], []\n",
    "\n",
    "            for imp in range(1, imputations + 1):\n",
    "                file_path = os.path.join(group_dir, f\"trimmed_data_imp{imp}.pkl\")\n",
    "                if not os.path.exists(file_path):\n",
    "                    continue\n",
    "\n",
    "                df = pd.read_pickle(file_path)\n",
    "                if group not in df.columns or \"iptw\" not in df.columns:\n",
    "                    continue\n",
    "\n",
    "                # Add bootstrap sampling with seed-based randomization\n",
    "                n_samples = len(df)\n",
    "                bootstrap_idx = np.random.choice(n_samples, size=n_samples, replace=True)\n",
    "                df_bootstrap = df.iloc[bootstrap_idx].reset_index(drop=True)\n",
    "\n",
    "                X = df_bootstrap[covariates].copy()\n",
    "                T = df_bootstrap[group]\n",
    "                Y = df_bootstrap[\"caps5_change_baseline\"]\n",
    "                #W = df_bootstrap[\"iptw\"]\n",
    "\n",
    "                try:\n",
    "                    # Create design matrix with treatment variable and covariates\n",
    "                    X_ols = pd.concat([T, X], axis=1)\n",
    "                    X_ols = sm.add_constant(X_ols)\n",
    "                    \n",
    "                    # Fit OLS with robust standard errors (unweighted)\n",
    "                    ols_model = sm.OLS(Y, X_ols).fit(cov_type='HC1')\n",
    "                    \n",
    "                    # Extract treatment effect (coefficient of treatment variable)\n",
    "                    att = ols_model.params[group]  # Treatment coefficient\n",
    "                    se = ols_model.bse[group]  # Robust standard error for treatment\n",
    "                    \n",
    "                    att_list.append(att)\n",
    "                    se_list.append(se)\n",
    "\n",
    "                    # Calculate model fit statistics\n",
    "                    Y_pred = ols_model.fittedvalues\n",
    "                    residuals = ols_model.resid\n",
    "                    rmse = mean_squared_error(Y, Y_pred, squared=False)\n",
    "                    r2 = ols_model.rsquared\n",
    "                    r2_list.append(r2)\n",
    "                    rmse_list.append(rmse)\n",
    "                    \n",
    "                    # Collect residuals and fitted values for diagnostic plots\n",
    "                    group_residuals.append(residuals.values)\n",
    "                    group_fitted.append(Y_pred.values)\n",
    "\n",
    "                    smd, vr = calculate_smd_vr(X, T)\n",
    "                    smd_list.append(smd)\n",
    "                    vr_list.append(vr)\n",
    "                except Exception as e:\n",
    "                    print(f\"⚠️ Error in {group}, seed {seed}, imp {imp}: {e}\")\n",
    "\n",
    "            if att_list:\n",
    "                att, se, ci_l, ci_u, p_val = rubins_pool(att_list, se_list)\n",
    "                avg_r2 = np.mean(r2_list)\n",
    "                avg_rmse = np.mean(rmse_list)\n",
    "                avg_smd = np.mean(smd_list)\n",
    "                avg_vr = np.mean(vr_list)\n",
    "\n",
    "                balance_results.append({\n",
    "                    \"group\": group, \"seed\": seed, \"smd\": avg_smd, \"vr\": avg_vr\n",
    "                })\n",
    "\n",
    "                if se < best_se:\n",
    "                    best_se = se\n",
    "                    best_result = {\n",
    "                        \"group\": group, \"seed\": seed, \"att\": att, \"se\": se,\n",
    "                        \"ci_lower\": ci_l, \"ci_upper\": ci_u, \"p_value\": p_val,\n",
    "                        \"r2\": avg_r2, \"rmse\": avg_rmse\n",
    "                    }\n",
    "\n",
    "                print(f\"✅ {group} | Seed {seed}: ATT = {att:.4f}, SE = {se:.4f}, p = {p_val}\")\n",
    "            else:\n",
    "                print(f\"⚠️ No valid results for {group} | Seed {seed}\")\n",
    "\n",
    "        # Create diagnostic plots for this group\n",
    "        if group_residuals and group_fitted:\n",
    "            create_diagnostic_plots(group_residuals, group_fitted, group)\n",
    "\n",
    "        if best_result:\n",
    "            att_results.append(best_result)\n",
    "            print(f\"🏆 Best result for {group} → Seed {best_result['seed']} | SE = {best_result['se']:.4f}\")\n",
    "\n",
    "    # Save final output\n",
    "    pd.DataFrame(att_results).to_excel(\"ols_rubin_summary_cats_unweighted.xlsx\", index=False)\n",
    "    pd.DataFrame(balance_results).to_excel(\"smd_vr_summary_cats_unweighted.xlsx\", index=False)\n",
    "    print(\"\\n🎯 All summary files saved.\")\n",
    "\n",
    "run_dml_with_trimmed_data(final_covariates_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fc4dc9d3-d7a4-48e4-9c44-8c3e1c375a6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Final_ATT_Summary_Cat saved\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import sem, ttest_ind\n",
    "\n",
    "# ----------------------------------\n",
    "# File paths\n",
    "# ----------------------------------\n",
    "output_base = \"outputs\"\n",
    "att_file = \"ols_rubin_summary_cats.xlsx\"\n",
    "trimmed_file = \"trimmed_data_imp1.pkl\"\n",
    "auc_file = \"auc_scores.xlsx\"  # NEW\n",
    "\n",
    "# ----------------------------------\n",
    "# Load ATT Summary\n",
    "# ----------------------------------\n",
    "if os.path.exists(att_file):\n",
    "    att_df = pd.read_excel(att_file)\n",
    "else:\n",
    "    raise FileNotFoundError(\"❌ ATT summary file not found: ols_rubin_summary_cats.xlsx\")\n",
    "\n",
    "summary_rows = []\n",
    "\n",
    "# ----------------------------------\n",
    "# Loop over medication groups\n",
    "# ----------------------------------\n",
    "groups = [g for g in medication_groups if os.path.isdir(os.path.join(output_base, g))]\n",
    "\n",
    "for med in groups:\n",
    "    try:\n",
    "        group_path = os.path.join(output_base, med)\n",
    "\n",
    "        # Load trimmed data\n",
    "        df = pd.read_pickle(os.path.join(group_path, trimmed_file))\n",
    "\n",
    "        # Detect treatment column\n",
    "        treatment_cols = [col for col in df.columns if col.upper() == med.upper()]\n",
    "        if not treatment_cols:\n",
    "            print(f\"⚠️ Treatment column {med} not found in trimmed data. Skipping.\")\n",
    "            continue\n",
    "        treatment_var = treatment_cols[0]\n",
    "\n",
    "        # Extract treatment and outcome\n",
    "        T = df[treatment_var]\n",
    "        Y = df[\"caps5_change_baseline\"]\n",
    "\n",
    "        # Treated and control stats\n",
    "        treated = Y[T == 1]\n",
    "        control = Y[T == 0]\n",
    "\n",
    "        mean_treat = treated.mean()\n",
    "        se_treat = sem(treated) if len(treated) > 1 else np.nan\n",
    "\n",
    "        mean_ctrl = control.mean()\n",
    "        se_ctrl = sem(control) if len(control) > 1 else np.nan\n",
    "\n",
    "        # Cohen's d (unadjusted)\n",
    "        pooled_sd = np.sqrt(((treated.std() ** 2) + (control.std() ** 2)) / 2)\n",
    "        cohen_d = (mean_treat - mean_ctrl) / pooled_sd if pooled_sd > 0 else np.nan\n",
    "\n",
    "        # E-value (unadjusted)\n",
    "        delta = mean_treat - mean_ctrl\n",
    "        E = delta / abs(mean_ctrl) * 100 if mean_ctrl != 0 else np.nan\n",
    "\n",
    "        # Unadjusted p-value\n",
    "        try:\n",
    "            t_stat, p_val = ttest_ind(treated, control, equal_var=False, nan_policy=\"omit\")\n",
    "            rounded_p = round(p_val, 5)\n",
    "            formatted_p = \"< 0.00001\" if rounded_p < 0.00001 else rounded_p\n",
    "        except Exception:\n",
    "            formatted_p = np.nan\n",
    "\n",
    "        # AUC from new auc_scores.xlsx file\n",
    "        auc_val = np.nan\n",
    "        auc_path = os.path.join(group_path, auc_file)\n",
    "        if os.path.exists(auc_path):\n",
    "            auc_df = pd.read_excel(auc_path)\n",
    "            if \"AUC\" in auc_df.columns:\n",
    "                auc_val = auc_df[\"AUC\"].dropna().mean()\n",
    "\n",
    "        # Adjusted stats from Rubin summary\n",
    "        att_row = att_df[att_df[\"group\"].str.strip().str.upper() == med.strip().upper()]\n",
    "        if not att_row.empty:\n",
    "            att = att_row.iloc[0][\"att\"]\n",
    "            att_se = att_row.iloc[0][\"se\"]\n",
    "            att_p_val = att_row.iloc[0][\"p_value\"]\n",
    "            r2 = att_row.iloc[0][\"r2\"]\n",
    "            rmse = att_row.iloc[0][\"rmse\"]\n",
    "\n",
    "            try:\n",
    "                rounded_att_p = round(float(att_p_val), 5)\n",
    "                formatted_att_p = \"< 0.00001\" if rounded_att_p < 0.00001 else rounded_att_p\n",
    "            except:\n",
    "                formatted_att_p = att_p_val\n",
    "        else:\n",
    "            att, att_se, formatted_att_p, r2, rmse = np.nan, np.nan, np.nan, np.nan, np.nan\n",
    "\n",
    "        # Append full row\n",
    "        summary_rows.append({\n",
    "            'Medication Group': med,\n",
    "            'Mean Treated': mean_treat,\n",
    "            'SE Treated': se_treat,\n",
    "            'Mean Control': mean_ctrl,\n",
    "            'SE Control': se_ctrl,\n",
    "            'Cohen d': cohen_d,\n",
    "            'E (Unadjusted)': E,\n",
    "            'n Treated': len(treated),\n",
    "            'n Control': len(control),\n",
    "            #'Unadjusted p-value': formatted_p,\n",
    "            'ATT Estimate': att,\n",
    "            'ATT SE (Robust)': att_se,\n",
    "            'ATT p-value': formatted_att_p,\n",
    "            'R²': r2,\n",
    "            'RMSE': rmse,\n",
    "            'AUC': auc_val\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error in {med}: {e}\")\n",
    "\n",
    "# ----------------------------------\n",
    "# Save final summary\n",
    "# ----------------------------------\n",
    "summary_df = pd.DataFrame(summary_rows)\n",
    "summary_df = summary_df.sort_values(\"Medication Group\")\n",
    "summary_df.to_excel(\"Final_ATT_Summary_Cat.xlsx\", index=False)\n",
    "print(\"✅ Final_ATT_Summary_Cat saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "98d3f0ff-8ebf-4379-b380-afb212a49f8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ ols_att_barplot_cat saved\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "# ✅ Load the final summary table\n",
    "final_df = pd.read_excel(\"Final_ATT_Summary_Cat.xlsx\")\n",
    "\n",
    "# ✅ Parse DML p-values (handle \"< 0.00001\")\n",
    "def parse_pval(p):\n",
    "    try:\n",
    "        if isinstance(p, str) and \"<\" in p:\n",
    "            return 0.000001\n",
    "        return float(p)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "final_df['ATT p-value'] = final_df['ATT p-value'].apply(parse_pval)\n",
    "\n",
    "# ✅ Plot settings\n",
    "width = 0.35\n",
    "\n",
    "# ✅ Plotting function for a single medication group\n",
    "def plot_single_group(row):\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    bars1 = ax.bar(-width/2, row['Mean Control'], width, \n",
    "                   yerr=row['SE Control'], label='Control', hatch='//', color='gray', capsize=5)\n",
    "    bars2 = ax.bar(+width/2, row['Mean Treated'], width, \n",
    "                   yerr=row['SE Treated'], label='Treated', color='steelblue', capsize=5)\n",
    "\n",
    "    label = (\n",
    "        f\"ATT = {row['ATT Estimate']:.2f}\\n\"\n",
    "        f\"d = {row['Cohen d']:.2f}, p = {row['ATT p-value']:.3f}\\n\"\n",
    "        f\"nT = {row['n Treated']}, nC = {row['n Control']}\\n\"\n",
    "        f\"E = {row['E (Unadjusted)']:.1f}%\"\n",
    "    )\n",
    "    max_y = max(row['Mean Control'], row['Mean Treated']) + 1.5\n",
    "    ax.text(0, max_y, label, ha='center', va='bottom', fontsize=9, color='#FFD700')\n",
    "\n",
    "    ax.axhline(0, color='black', linewidth=0.8)\n",
    "    ax.set_xticks([-width/2, +width/2])\n",
    "    ax.set_xticklabels(['Control', 'Treated'])\n",
    "    ax.set_title(f\"Group: {row['Medication Group']}\", fontsize=12, weight='bold')\n",
    "    ax.set_ylabel(\"CAPS5 Change Score\")\n",
    "    ax.set_ylim(bottom=0, top=max_y + 2)\n",
    "    ax.legend()\n",
    "    fig.tight_layout()\n",
    "    return fig\n",
    "\n",
    "# ✅ Generate and save all plots into a multi-page PDF\n",
    "with PdfPages(\"ols_att_barplot_cat.pdf\") as pdf:\n",
    "    for idx, row in final_df.iterrows():\n",
    "        fig = plot_single_group(row)\n",
    "        pdf.savefig(fig, dpi=300, bbox_inches='tight')\n",
    "        plt.close(fig)\n",
    "print(\"✅ ols_att_barplot_cat saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a3e741d8-2d5c-4c48-997c-a50d1f651061",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Love plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "47b4d0de-9bff-4bf7-a497-ead963faa064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Processing CAT_Aceetanilidederivaten...\n",
      "📊 Exported numeric summary to: outputs\\CAT_Aceetanilidederivaten\\covariate_balance_table_CAT_Aceetanilidederivaten.xlsx\n",
      "✅ Saved love plot: outputs\\CAT_Aceetanilidederivaten\\love_plot_CAT_Aceetanilidederivaten.pdf\n",
      "📏 Max weighted SMD for CAT_Aceetanilidederivaten: 0.536\n",
      "\n",
      "🔍 Processing CAT_Adhd...\n",
      "📊 Exported numeric summary to: outputs\\CAT_Adhd\\covariate_balance_table_CAT_Adhd.xlsx\n",
      "✅ Saved love plot: outputs\\CAT_Adhd\\love_plot_CAT_Adhd.pdf\n",
      "📏 Max weighted SMD for CAT_Adhd: 0.433\n",
      "\n",
      "🔍 Processing CAT_All...\n",
      "📊 Exported numeric summary to: outputs\\CAT_All\\covariate_balance_table_CAT_All.xlsx\n",
      "✅ Saved love plot: outputs\\CAT_All\\love_plot_CAT_All.pdf\n",
      "📏 Max weighted SMD for CAT_All: 0.100\n",
      "\n",
      "🔍 Processing CAT_All_Psychotropics...\n",
      "📊 Exported numeric summary to: outputs\\CAT_All_Psychotropics\\covariate_balance_table_CAT_All_Psychotropics.xlsx\n",
      "✅ Saved love plot: outputs\\CAT_All_Psychotropics\\love_plot_CAT_All_Psychotropics.pdf\n",
      "📏 Max weighted SMD for CAT_All_Psychotropics: 0.106\n",
      "\n",
      "🔍 Processing CAT_All_Psychotropics_Excl_Benzo...\n",
      "📊 Exported numeric summary to: outputs\\CAT_All_Psychotropics_Excl_Benzo\\covariate_balance_table_CAT_All_Psychotropics_Excl_Benzo.xlsx\n",
      "✅ Saved love plot: outputs\\CAT_All_Psychotropics_Excl_Benzo\\love_plot_CAT_All_Psychotropics_Excl_Benzo.pdf\n",
      "📏 Max weighted SMD for CAT_All_Psychotropics_Excl_Benzo: 0.171\n",
      "\n",
      "🔍 Processing CAT_All_Psychotropics_Excl_Sedatives_Hypnotics...\n",
      "📊 Exported numeric summary to: outputs\\CAT_All_Psychotropics_Excl_Sedatives_Hypnotics\\covariate_balance_table_CAT_All_Psychotropics_Excl_Sedatives_Hypnotics.xlsx\n",
      "✅ Saved love plot: outputs\\CAT_All_Psychotropics_Excl_Sedatives_Hypnotics\\love_plot_CAT_All_Psychotropics_Excl_Sedatives_Hypnotics.pdf\n",
      "📏 Max weighted SMD for CAT_All_Psychotropics_Excl_Sedatives_Hypnotics: 0.142\n",
      "\n",
      "🔍 Processing CAT_Antidepressiva...\n",
      "📊 Exported numeric summary to: outputs\\CAT_Antidepressiva\\covariate_balance_table_CAT_Antidepressiva.xlsx\n",
      "✅ Saved love plot: outputs\\CAT_Antidepressiva\\love_plot_CAT_Antidepressiva.pdf\n",
      "📏 Max weighted SMD for CAT_Antidepressiva: 0.127\n",
      "\n",
      "🔍 Processing CAT_Antihistaminica...\n",
      "📊 Exported numeric summary to: outputs\\CAT_Antihistaminica\\covariate_balance_table_CAT_Antihistaminica.xlsx\n",
      "✅ Saved love plot: outputs\\CAT_Antihistaminica\\love_plot_CAT_Antihistaminica.pdf\n",
      "📏 Max weighted SMD for CAT_Antihistaminica: 0.565\n",
      "\n",
      "🔍 Processing CAT_Antihypertensiva...\n",
      "📊 Exported numeric summary to: outputs\\CAT_Antihypertensiva\\covariate_balance_table_CAT_Antihypertensiva.xlsx\n",
      "✅ Saved love plot: outputs\\CAT_Antihypertensiva\\love_plot_CAT_Antihypertensiva.pdf\n",
      "📏 Max weighted SMD for CAT_Antihypertensiva: 0.746\n",
      "\n",
      "🔍 Processing CAT_Antipsychotica...\n",
      "📊 Exported numeric summary to: outputs\\CAT_Antipsychotica\\covariate_balance_table_CAT_Antipsychotica.xlsx\n",
      "✅ Saved love plot: outputs\\CAT_Antipsychotica\\love_plot_CAT_Antipsychotica.pdf\n",
      "📏 Max weighted SMD for CAT_Antipsychotica: 0.163\n",
      "\n",
      "🔍 Processing CAT_Anti_Epileptica...\n",
      "📊 Exported numeric summary to: outputs\\CAT_Anti_Epileptica\\covariate_balance_table_CAT_Anti_Epileptica.xlsx\n",
      "✅ Saved love plot: outputs\\CAT_Anti_Epileptica\\love_plot_CAT_Anti_Epileptica.pdf\n",
      "📏 Max weighted SMD for CAT_Anti_Epileptica: 0.388\n",
      "\n",
      "🔍 Processing CAT_Benzodiazepine...\n",
      "📊 Exported numeric summary to: outputs\\CAT_Benzodiazepine\\covariate_balance_table_CAT_Benzodiazepine.xlsx\n",
      "✅ Saved love plot: outputs\\CAT_Benzodiazepine\\love_plot_CAT_Benzodiazepine.pdf\n",
      "📏 Max weighted SMD for CAT_Benzodiazepine: 0.168\n",
      "\n",
      "🔍 Processing CAT_Nsaids...\n",
      "📊 Exported numeric summary to: outputs\\CAT_Nsaids\\covariate_balance_table_CAT_Nsaids.xlsx\n",
      "✅ Saved love plot: outputs\\CAT_Nsaids\\love_plot_CAT_Nsaids.pdf\n",
      "📏 Max weighted SMD for CAT_Nsaids: 0.572\n",
      "\n",
      "🔍 Processing CAT_Opioden...\n",
      "📊 Exported numeric summary to: outputs\\CAT_Opioden\\covariate_balance_table_CAT_Opioden.xlsx\n",
      "✅ Saved love plot: outputs\\CAT_Opioden\\love_plot_CAT_Opioden.pdf\n",
      "📏 Max weighted SMD for CAT_Opioden: 0.396\n",
      "\n",
      "🔍 Processing CAT_Z_Drugs...\n",
      "📊 Exported numeric summary to: outputs\\CAT_Z_Drugs\\covariate_balance_table_CAT_Z_Drugs.xlsx\n",
      "✅ Saved love plot: outputs\\CAT_Z_Drugs\\love_plot_CAT_Z_Drugs.pdf\n",
      "📏 Max weighted SMD for CAT_Z_Drugs: 0.360\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "# ----------------------------------------\n",
    "# Functions to calculate balance\n",
    "# ----------------------------------------\n",
    "def calculate_smd(x1, x2, w1=None, w2=None):\n",
    "    def weighted_mean(x, w): return np.average(x, weights=w)\n",
    "    def weighted_var(x, w):\n",
    "        m = np.average(x, weights=w)\n",
    "        return np.average((x - m) ** 2, weights=w)\n",
    "    \n",
    "    m1 = weighted_mean(x1, w1) if w1 is not None else np.mean(x1)\n",
    "    m2 = weighted_mean(x2, w2) if w2 is not None else np.mean(x2)\n",
    "    v1 = weighted_var(x1, w1) if w1 is not None else np.var(x1, ddof=1)\n",
    "    v2 = weighted_var(x2, w2) if w2 is not None else np.var(x2, ddof=1)\n",
    "    \n",
    "    pooled_sd = np.sqrt((v1 + v2) / 2)\n",
    "    return np.abs(m1 - m2) / pooled_sd if pooled_sd > 0 else 0\n",
    "\n",
    "def variance_ratio(x1, x2, w1=None, w2=None):\n",
    "    def weighted_var(x, w):\n",
    "        m = np.average(x, weights=w)\n",
    "        return np.average((x - m) ** 2, weights=w)\n",
    "    \n",
    "    v1 = weighted_var(x1, w1) if w1 is not None else np.var(x1, ddof=1)\n",
    "    v2 = weighted_var(x2, w2) if w2 is not None else np.var(x2, ddof=1)\n",
    "    \n",
    "    return max(v1 / v2, v2 / v1) if v1 > 0 and v2 > 0 else 1\n",
    "\n",
    "# ----------------------------------------\n",
    "# Setup\n",
    "# ----------------------------------------\n",
    "output_base = \"outputs\"\n",
    "groups = [g for g in os.listdir(output_base) if os.path.isdir(os.path.join(output_base, g))]\n",
    "\n",
    "# Create a case-insensitive mapping\n",
    "final_covariates_map_lower = {k.lower(): v for k, v in final_covariates_map.items()}\n",
    "\n",
    "# ----------------------------------------\n",
    "# Main Loop\n",
    "# ----------------------------------------\n",
    "for group in groups:\n",
    "    if group.lower() not in final_covariates_map_lower:\n",
    "        continue\n",
    "\n",
    "    print(f\"\\n🔍 Processing {group}...\")\n",
    "\n",
    "    try:\n",
    "        group_path = os.path.join(output_base, group)\n",
    "        covariates = final_covariates_map_lower[group.lower()]\n",
    "        \n",
    "        column_name = None\n",
    "        for col in pd.read_pickle(os.path.join(group_path, \"trimmed_data_imp1.pkl\")).columns:\n",
    "            if col.lower() == group.lower():\n",
    "                column_name = col\n",
    "                break\n",
    "        if column_name is None:\n",
    "            print(f\"⚠️ Column not found for {group}, skipping.\")\n",
    "            continue\n",
    "\n",
    "        smd_unw_all, smd_w_all = [], []\n",
    "        vr_unw_all, vr_w_all = [], []\n",
    "\n",
    "        for i in range(1, 6):\n",
    "            df_path = os.path.join(group_path, f\"trimmed_data_imp{i}.pkl\")\n",
    "            iptw_path = os.path.join(group_path, \"iptw_weights.xlsx\")\n",
    "\n",
    "            if not os.path.exists(df_path) or not os.path.exists(iptw_path):\n",
    "                print(f\"⚠️ Missing data for {group} imp{i}, skipping.\")\n",
    "                continue\n",
    "\n",
    "            df = pd.read_pickle(df_path)\n",
    "            iptw_df = pd.read_excel(iptw_path, index_col=0)\n",
    "            T = df[column_name]\n",
    "            W = iptw_df.loc[df.index, \"iptw_mean\"]\n",
    "\n",
    "            smd_unw_i, smd_w_i, vr_unw_i, vr_w_i = [], [], [], []\n",
    "\n",
    "            for cov in covariates:\n",
    "                x1, x0 = df.loc[T == 1, cov], df.loc[T == 0, cov]\n",
    "                w1, w0 = W[T == 1], W[T == 0]\n",
    "\n",
    "                su = calculate_smd(x1, x0)\n",
    "                sw = calculate_smd(x1, x0, w1, w0)\n",
    "\n",
    "                vu = variance_ratio(x1, x0) if cov in ['treatmentdurationdays', 'CAPS5score_baseline', 'age'] else np.nan\n",
    "                vw = variance_ratio(x1, x0, w1, w0) if cov in ['treatmentdurationdays', 'CAPS5score_baseline', 'age'] else np.nan\n",
    "\n",
    "                smd_unw_i.append(su)\n",
    "                smd_w_i.append(sw)\n",
    "                vr_unw_i.append(vu)\n",
    "                vr_w_i.append(vw)\n",
    "\n",
    "            smd_unw_all.append(smd_unw_i)\n",
    "            smd_w_all.append(smd_w_i)\n",
    "            vr_unw_all.append(vr_unw_i)\n",
    "            vr_w_all.append(vr_w_i)\n",
    "\n",
    "        smd_unw = np.mean(smd_unw_all, axis=0)\n",
    "        smd_w = np.mean(smd_w_all, axis=0)\n",
    "        vr_unw = np.nanmean(vr_unw_all, axis=0)\n",
    "        vr_w = np.nanmean(vr_w_all, axis=0)\n",
    "\n",
    "        severity = []\n",
    "        for sw in smd_w:\n",
    "            if sw <= 0.1:\n",
    "                severity.append(\"Good\")\n",
    "            elif sw <= 0.2:\n",
    "                severity.append(\"Moderate\")\n",
    "            else:\n",
    "                severity.append(\"Poor\")\n",
    "\n",
    "        covariate_names = covariates\n",
    "        numeric_df = pd.DataFrame({\n",
    "            \"Covariate\": covariate_names,\n",
    "            \"SMD_Unweighted\": smd_unw,\n",
    "            \"SMD_Weighted\": smd_w,\n",
    "            \"Imbalance_Severity\": severity,\n",
    "            \"VR_Unweighted\": vr_unw,\n",
    "            \"VR_Weighted\": vr_w\n",
    "        })\n",
    "\n",
    "        numeric_path = os.path.join(group_path, f\"covariate_balance_table_{group}.xlsx\")\n",
    "        numeric_df.to_excel(numeric_path, index=False)\n",
    "        print(f\"📊 Exported numeric summary to: {numeric_path}\")\n",
    "\n",
    "        # -------------------------\n",
    "        # Plot\n",
    "        # -------------------------\n",
    "        labels = covariates\n",
    "        y_pos = np.arange(len(labels))\n",
    "\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(18, len(labels) * 0.45))\n",
    "\n",
    "        axes[0].scatter(smd_unw, y_pos, color='red', label=\"Unweighted\")\n",
    "        axes[0].scatter(smd_w, y_pos, color='blue', label=\"Weighted\")\n",
    "        axes[0].axvline(0.1, color='gray', linestyle='--', label=\"Threshold 0.1\")\n",
    "        axes[0].axvline(0.2, color='black', linestyle='--', label=\"Threshold 0.2\")\n",
    "        axes[0].set_xlim(0, max(max(smd_unw), max(smd_w), 0.25) + 0.05)\n",
    "        axes[0].set_yticks(y_pos)\n",
    "        axes[0].set_yticklabels(labels)\n",
    "        axes[0].invert_yaxis()\n",
    "        axes[0].set_title(\"Standardized Mean Differences (SMD)\")\n",
    "        axes[0].legend(loc=\"upper right\")\n",
    "        axes[0].grid(True)\n",
    "\n",
    "        vr_mask = [cov in ['treatmentdurationdays', 'CAPS5score_baseline', 'age'] for cov in covariates]\n",
    "        filtered_y = [i for i, b in enumerate(vr_mask) if b]\n",
    "        filtered_labels = [labels[i] for i in filtered_y]\n",
    "        filtered_vr_unw = [vr_unw[i] for i in filtered_y]\n",
    "        filtered_vr_w = [vr_w[i] for i in filtered_y]\n",
    "\n",
    "        axes[1].scatter(filtered_vr_unw, filtered_y, color='blue', marker='o', label=\"Unweighted\")\n",
    "        axes[1].scatter(filtered_vr_w, filtered_y, color='red', marker='x', label=\"Weighted\")\n",
    "        axes[1].axvline(2, color='gray', linestyle='--')\n",
    "        axes[1].axvline(0.5, color='gray', linestyle='--')\n",
    "        axes[1].set_xlim(0, max(filtered_vr_unw + filtered_vr_w + [2.5]) + 0.5)\n",
    "        axes[1].set_yticks(filtered_y)\n",
    "        axes[1].set_yticklabels(filtered_labels)\n",
    "        axes[1].invert_yaxis()\n",
    "        axes[1].set_title(\"Variance Ratio (VR)\")\n",
    "        axes[1].legend()\n",
    "        axes[1].grid(True)\n",
    "\n",
    "        fig.suptitle(f\"Covariate Balance for {group.replace('CAT_', '')}\", fontsize=14, weight='bold')\n",
    "        fig.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "        plot_path = os.path.join(group_path, f\"love_plot_{group}.pdf\")\n",
    "        fig.savefig(plot_path, dpi=300)\n",
    "        plt.close()\n",
    "        print(f\"✅ Saved love plot: {plot_path}\")\n",
    "        print(f\"📏 Max weighted SMD for {group}: {np.max(smd_w):.3f}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error in {group}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "70cac837-729f-49b1-b10d-d0ef9c0992a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fbcc8601-6f31-421d-bdf4-6da70b6d6101",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== Creating Heatmap for CAT_ADHD ==========\n",
      "✅ Heatmap saved: outputs\\CAT_ADHD\\heatmap_smd_CAT_ADHD.png\n",
      "\n",
      "========== Creating Heatmap for CAT_Aceetanilidederivaten ==========\n",
      "✅ Heatmap saved: outputs\\CAT_Aceetanilidederivaten\\heatmap_smd_CAT_Aceetanilidederivaten.png\n",
      "\n",
      "========== Creating Heatmap for CAT_Z_drugs ==========\n",
      "✅ Heatmap saved: outputs\\CAT_Z_drugs\\heatmap_smd_CAT_Z_drugs.png\n",
      "\n",
      "========== Creating Heatmap for CAT_Opioden ==========\n",
      "✅ Heatmap saved: outputs\\CAT_Opioden\\heatmap_smd_CAT_Opioden.png\n",
      "\n",
      "========== Creating Heatmap for CAT_NSAIDs ==========\n",
      "✅ Heatmap saved: outputs\\CAT_NSAIDs\\heatmap_smd_CAT_NSAIDs.png\n",
      "\n",
      "========== Creating Heatmap for CAT_Benzodiazepine ==========\n",
      "✅ Heatmap saved: outputs\\CAT_Benzodiazepine\\heatmap_smd_CAT_Benzodiazepine.png\n",
      "\n",
      "========== Creating Heatmap for CAT_Antihypertensiva ==========\n",
      "✅ Heatmap saved: outputs\\CAT_Antihypertensiva\\heatmap_smd_CAT_Antihypertensiva.png\n",
      "\n",
      "========== Creating Heatmap for CAT_Antihistaminica ==========\n",
      "✅ Heatmap saved: outputs\\CAT_Antihistaminica\\heatmap_smd_CAT_Antihistaminica.png\n",
      "\n",
      "========== Creating Heatmap for CAT_Anti_epileptica ==========\n",
      "✅ Heatmap saved: outputs\\CAT_Anti_epileptica\\heatmap_smd_CAT_Anti_epileptica.png\n",
      "\n",
      "========== Creating Heatmap for CAT_Antidepressiva ==========\n",
      "✅ Heatmap saved: outputs\\CAT_Antidepressiva\\heatmap_smd_CAT_Antidepressiva.png\n",
      "\n",
      "========== Creating Heatmap for CAT_Antipsychotica ==========\n",
      "✅ Heatmap saved: outputs\\CAT_Antipsychotica\\heatmap_smd_CAT_Antipsychotica.png\n",
      "\n",
      "========== Creating Heatmap for CAT_ALL_PSYCHOTROPICS_EXCL_BENZO ==========\n",
      "✅ Heatmap saved: outputs\\CAT_ALL_PSYCHOTROPICS_EXCL_BENZO\\heatmap_smd_CAT_ALL_PSYCHOTROPICS_EXCL_BENZO.png\n",
      "\n",
      "========== Creating Heatmap for CAT_ALL_PSYCHOTROPICS_EXCL_SEDATIVES_HYPNOTICS ==========\n",
      "✅ Heatmap saved: outputs\\CAT_ALL_PSYCHOTROPICS_EXCL_SEDATIVES_HYPNOTICS\\heatmap_smd_CAT_ALL_PSYCHOTROPICS_EXCL_SEDATIVES_HYPNOTICS.png\n",
      "\n",
      "========== Creating Heatmap for CAT_ALL_PSYCHOTROPICS ==========\n",
      "✅ Heatmap saved: outputs\\CAT_ALL_PSYCHOTROPICS\\heatmap_smd_CAT_ALL_PSYCHOTROPICS.png\n",
      "\n",
      "========== Creating Heatmap for CAT_ALL ==========\n",
      "✅ Heatmap saved: outputs\\CAT_ALL\\heatmap_smd_CAT_ALL.png\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "#-----------------------------\n",
    "# Generate heatmaps\n",
    "# -------------------------------\n",
    "for treatment_var in medication_groups:\n",
    "    print(f\"\\n========== Creating Heatmap for {treatment_var} ==========\")\n",
    "\n",
    "    try:\n",
    "        output_folder = os.path.join('outputs', treatment_var)\n",
    "        balance_path = os.path.join(output_folder, f'covariate_balance_table_{treatment_var}.xlsx')\n",
    "\n",
    "        if not os.path.exists(balance_path):\n",
    "            print(f\"❌ Balance file not found: {balance_path}\")\n",
    "            continue\n",
    "\n",
    "        balance_df = pd.read_excel(balance_path)\n",
    "\n",
    "        # ✅ Use finalized covariates + 'Propensity Score'\n",
    "        covariates = final_covariates_map[treatment_var] + ['Propensity Score']\n",
    "        balance_df = balance_df[balance_df['Covariate'].isin(covariates)]\n",
    "\n",
    "        # ✅ Check for CAPS5score_baseline\n",
    "        highlight_caps = 'CAPS5score_baseline' in balance_df['Covariate'].values\n",
    "\n",
    "        # ✅ Format for heatmap\n",
    "        heatmap_df = balance_df[['Covariate', 'SMD_Unweighted', 'SMD_Weighted']].copy()\n",
    "        heatmap_df.columns = ['Covariate', 'Unweighted', 'Weighted']\n",
    "        heatmap_df = heatmap_df.set_index('Covariate')\n",
    "        heatmap_df = heatmap_df.sort_values(by='Unweighted', ascending=False)\n",
    "\n",
    "        # ✅ Plot\n",
    "        plt.figure(figsize=(12, max(10, len(heatmap_df) * 0.35)))\n",
    "        ax = sns.heatmap(\n",
    "            heatmap_df,\n",
    "            cmap=\"coolwarm\",\n",
    "            annot=True,\n",
    "            fmt=\".2f\",\n",
    "            linewidths=0.6,\n",
    "            linecolor='gray',\n",
    "            cbar_kws={\"label\": \"Standardized Mean Difference\"}\n",
    "        )\n",
    "\n",
    "        plt.title(f\"Covariate Balance Heatmap (Rubin IPTW)\\n{treatment_var}\", fontsize=15, weight='bold')\n",
    "        plt.xlabel(\"Condition\")\n",
    "        plt.ylabel(\"Covariate\")\n",
    "\n",
    "        # ✅ Bold CAPS5score_baseline if present\n",
    "        if highlight_caps:\n",
    "            ylabels = [label.get_text() for label in ax.get_yticklabels()]\n",
    "            ax.set_yticklabels([\n",
    "                f\"{label} ←\" if label == 'CAPS5score_baseline' else label for label in ylabels\n",
    "            ])\n",
    "\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # ✅ Save image\n",
    "        save_path = os.path.join(output_folder, f'heatmap_smd_{treatment_var}.png')\n",
    "        plt.savefig(save_path, dpi=300)\n",
    "        plt.close()\n",
    "\n",
    "        print(f\"✅ Heatmap saved: {save_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error processing {treatment_var}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f20d353-a1a6-41d1-a07e-d83cd907131e",
   "metadata": {},
   "source": [
    "### Subcat analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "74cf789e-8d11-46b8-9fc6-4c93ac6375d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "covariates_SUBCAT_Antipsychotica_atypisch = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline',\n",
    "    'CAT_Antidepressiva', 'CAT_Antipsychotica',\n",
    "    'DIAGNOSIS_CHILDHOOD_TRAUMA', 'DIAGNOSIS_CPTSD', 'DIAGNOSIS_SEXUAL_TRAUMA',\n",
    "    'DIAGNOSIS_SUICIDALITY', 'age',\n",
    "    'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"Bipolar_and_Mood_disorder\", \"ethnicity_Dutch\", \"ethnicity_other\"\n",
    "]\n",
    "\n",
    "covariates_SUBCAT_TCA = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline',\n",
    "    'CAT_Antipsychotica',\n",
    "    'DIAGNOSIS_CHILDHOOD_TRAUMA', 'DIAGNOSIS_CPTSD', 'DIAGNOSIS_SEXUAL_TRAUMA',\n",
    "    'DIAGNOSIS_SUICIDALITY', 'age',\n",
    "    'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"Bipolar_and_Mood_disorder\", \"ethnicity_Dutch\", \"ethnicity_other\"\n",
    "]\n",
    "\n",
    "covariates_SUBCAT_SSRI = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline',\n",
    "    'CAT_Antipsychotica',\n",
    "    'DIAGNOSIS_CHILDHOOD_TRAUMA', 'DIAGNOSIS_CPTSD', 'DIAGNOSIS_SEXUAL_TRAUMA',\n",
    "    'DIAGNOSIS_SUICIDALITY', 'age',\n",
    "    'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"Bipolar_and_Mood_disorder\", \"ethnicity_Dutch\", \"ethnicity_other\"\n",
    "]\n",
    "\n",
    "covariates_SUBCAT_SNRI = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline',\n",
    "    'CAT_Antipsychotica',\n",
    "    'DIAGNOSIS_CHILDHOOD_TRAUMA', 'DIAGNOSIS_CPTSD', 'DIAGNOSIS_SEXUAL_TRAUMA',\n",
    "    'DIAGNOSIS_SUICIDALITY', 'age',\n",
    "    'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"Bipolar_and_Mood_disorder\", \"ethnicity_Dutch\", \"ethnicity_other\"\n",
    "]\n",
    "\n",
    "\n",
    "covariates_SUBCAT_Tetracyclische_antidepressiva = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline',\n",
    "    'CAT_Antipsychotica',\n",
    "    'DIAGNOSIS_CHILDHOOD_TRAUMA', 'DIAGNOSIS_CPTSD', 'DIAGNOSIS_SEXUAL_TRAUMA',\n",
    "    'DIAGNOSIS_SUICIDALITY', 'age',\n",
    "    'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"Bipolar_and_Mood_disorder\", \"ethnicity_Dutch\", \"ethnicity_other\"\n",
    "]\n",
    "\n",
    "covariates_SUBCAT_Antidepressiva_overige = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline',\n",
    "    'CAT_Antipsychotica',\n",
    "    'DIAGNOSIS_CHILDHOOD_TRAUMA', 'DIAGNOSIS_CPTSD', 'DIAGNOSIS_SEXUAL_TRAUMA',\n",
    "    'DIAGNOSIS_SUICIDALITY', 'age',\n",
    "    'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"Bipolar_and_Mood_disorder\", \"ethnicity_Dutch\", \"ethnicity_other\"\n",
    "]\n",
    "\n",
    "covariates_SUBCAT_Systemische_antihistaminica = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline',\n",
    "    'CAT_Antidepressiva', 'CAT_Antipsychotica',\n",
    "    'DIAGNOSIS_CHILDHOOD_TRAUMA', 'DIAGNOSIS_CPTSD', 'DIAGNOSIS_SEXUAL_TRAUMA',\n",
    "    'DIAGNOSIS_SUICIDALITY', 'age',\n",
    "    'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"Bipolar_and_Mood_disorder\", \"ethnicity_Dutch\", \"ethnicity_other\"\n",
    "]\n",
    "\n",
    "\n",
    "covariates_SUBCAT_anxiolytica_Benzodiazepine = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline',\n",
    "    'CAT_Antidepressiva',\n",
    "    'CAT_Antipsychotica',\n",
    "    'DIAGNOSIS_CHILDHOOD_TRAUMA', 'DIAGNOSIS_CPTSD', 'DIAGNOSIS_SEXUAL_TRAUMA',\n",
    "    'DIAGNOSIS_SUICIDALITY', 'age',\n",
    "    'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"Bipolar_and_Mood_disorder\", \"ethnicity_Dutch\", \"ethnicity_other\"\n",
    "]\n",
    "\n",
    "covariates_SUBCAT_hypnotica_Benzodiazepine = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline',\n",
    "    'CAT_Antidepressiva',\n",
    "    'CAT_Antipsychotica',\n",
    "    'DIAGNOSIS_CHILDHOOD_TRAUMA', 'DIAGNOSIS_CPTSD', 'DIAGNOSIS_SEXUAL_TRAUMA',\n",
    "    'DIAGNOSIS_SUICIDALITY', 'age',\n",
    "    'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"Bipolar_and_Mood_disorder\", \"ethnicity_Dutch\", \"ethnicity_other\"\n",
    "]\n",
    "\n",
    "\n",
    "covariates_SUBCAT_Amfetaminen = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline', 'CAT_Antidepressiva', 'CAT_Antipsychotica',\n",
    "    'DIAGNOSIS_CHILDHOOD_TRAUMA', 'DIAGNOSIS_CPTSD',\n",
    "    'DIAGNOSIS_SEXUAL_TRAUMA', 'DIAGNOSIS_SUICIDALITY',\n",
    "    'age',\n",
    "    'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"Bipolar_and_Mood_disorder\", \"ethnicity_Dutch\", \"ethnicity_other\"\n",
    "]\n",
    "\n",
    "\n",
    "covariates_SUBCAT_Systemische_betablokkers = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline',\n",
    "    'CAT_Antidepressiva', 'CAT_Antipsychotica',\n",
    "    'DIAGNOSIS_CHILDHOOD_TRAUMA', 'DIAGNOSIS_CPTSD', 'DIAGNOSIS_SEXUAL_TRAUMA',\n",
    "    'DIAGNOSIS_SUICIDALITY', 'age',\n",
    "    'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"Bipolar_and_Mood_disorder\", \"ethnicity_Dutch\", \"ethnicity_other\"\n",
    "]\n",
    "\n",
    "\n",
    "covariates_SUBCAT_Paracetamol_mono = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline',\n",
    "    'CAT_Antidepressiva', 'CAT_Antipsychotica',\n",
    "    'DIAGNOSIS_CHILDHOOD_TRAUMA', 'DIAGNOSIS_CPTSD', 'DIAGNOSIS_SEXUAL_TRAUMA',\n",
    "    'DIAGNOSIS_SUICIDALITY', 'age',\n",
    "    'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"Bipolar_and_Mood_disorder\", \"ethnicity_Dutch\", \"ethnicity_other\"\n",
    "]\n",
    "\n",
    "\n",
    "covariates_SUBCAT_Anti_epileptica_stemmingsstabilisatoren = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline',\n",
    "    'CAT_Antidepressiva',\n",
    "    'CAT_Antipsychotica',\n",
    "    'DIAGNOSIS_CHILDHOOD_TRAUMA', 'DIAGNOSIS_CPTSD', 'DIAGNOSIS_SEXUAL_TRAUMA',\n",
    "    'DIAGNOSIS_SUICIDALITY', 'age', \n",
    "    'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"Bipolar_and_Mood_disorder\", \"ethnicity_Dutch\", \"ethnicity_other\"\n",
    "]\n",
    "\n",
    "covariates_SUBCAT_Opioden = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline',\n",
    "    'CAT_Antidepressiva',\n",
    "    'CAT_Antipsychotica',\n",
    "    'DIAGNOSIS_CHILDHOOD_TRAUMA', 'DIAGNOSIS_CPTSD', 'DIAGNOSIS_SEXUAL_TRAUMA',\n",
    "    'DIAGNOSIS_SUICIDALITY', 'age',\n",
    "    'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"Bipolar_and_Mood_disorder\", \"ethnicity_Dutch\", \"ethnicity_other\"\n",
    "]\n",
    "\n",
    "covariates_SUBCAT_Z_drugs = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline',\n",
    "    'CAT_Antidepressiva', 'CAT_Antipsychotica',\n",
    "    'DIAGNOSIS_CHILDHOOD_TRAUMA', 'DIAGNOSIS_CPTSD', 'DIAGNOSIS_SEXUAL_TRAUMA',\n",
    "    'DIAGNOSIS_SUICIDALITY', 'age',\n",
    "    'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"Bipolar_and_Mood_disorder\", \"ethnicity_Dutch\", \"ethnicity_other\"\n",
    "]\n",
    "\n",
    "covariates_SUBCAT_NSAIDs = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline',\n",
    "    'CAT_Antidepressiva',\n",
    "    'CAT_Antipsychotica',\n",
    "    'DIAGNOSIS_CHILDHOOD_TRAUMA', 'DIAGNOSIS_CPTSD', 'DIAGNOSIS_SEXUAL_TRAUMA',\n",
    "    'DIAGNOSIS_SUICIDALITY', 'age',\n",
    "    'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"Bipolar_and_Mood_disorder\", \"ethnicity_Dutch\", \"ethnicity_other\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0a20737a-265c-405f-ad41-e29a7dca8e23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Groups found: ['SUBCAT_Antipsychotica_atypisch', 'SUBCAT_TCA', 'SUBCAT_SSRI', 'SUBCAT_SNRI', 'SUBCAT_Tetracyclische_antidepressiva', 'SUBCAT_Antidepressiva_overige', 'SUBCAT_Systemische_antihistaminica', 'SUBCAT_anxiolytica_Benzodiazepine', 'SUBCAT_hypnotica_Benzodiazepine', 'SUBCAT_Amfetaminen', 'SUBCAT_Systemische_betablokkers', 'SUBCAT_Paracetamol_mono', 'SUBCAT_Anti_epileptica_stemmingsstabilisatoren', 'SUBCAT_Opioden', 'SUBCAT_Z_drugs', 'SUBCAT_NSAIDs']\n",
      "['SUBCAT_Antipsychotica_atypisch', 'SUBCAT_TCA', 'SUBCAT_SSRI', 'SUBCAT_SNRI', 'SUBCAT_Tetracyclische_antidepressiva', 'SUBCAT_Antidepressiva_overige', 'SUBCAT_Systemische_antihistaminica', 'SUBCAT_anxiolytica_Benzodiazepine', 'SUBCAT_hypnotica_Benzodiazepine', 'SUBCAT_Amfetaminen', 'SUBCAT_Systemische_betablokkers', 'SUBCAT_Paracetamol_mono', 'SUBCAT_Anti_epileptica_stemmingsstabilisatoren', 'SUBCAT_Opioden', 'SUBCAT_Z_drugs', 'SUBCAT_NSAIDs']\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# This finds all variables that start with covariates_SUBCAT_\n",
    "final_covariates_map = defaultdict(list)\n",
    "final_covariates_map.update({\n",
    "    var.replace(\"covariates_\", \"\"): val\n",
    "    for var, val in globals().items()\n",
    "    if var.lower().startswith(\"covariates_subcat_\") and isinstance(val, list)\n",
    "})\n",
    "\n",
    "# Show detected group names\n",
    "print(\"Groups found:\", list(final_covariates_map.keys()))\n",
    "medication_groups = list(final_covariates_map.keys())\n",
    "print(medication_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0c5d1b8a-3cb1-4a30-af1a-a414bccd3b5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Starting analysis for all SUBCAT groups\n",
      "\n",
      " Processing SUBCAT_Antipsychotica_Atypisch...\n",
      "  → Using imputation 1\n",
      "  → Using imputation 2\n",
      "  → Using imputation 3\n",
      "  → Using imputation 4\n",
      "  → Using imputation 5\n",
      " Done: SUBCAT_Antipsychotica_Atypisch\n",
      "\n",
      " Processing SUBCAT_Tca...\n",
      "  → Using imputation 1\n",
      "  → Using imputation 2\n",
      "  → Using imputation 3\n",
      "  → Using imputation 4\n",
      "  → Using imputation 5\n",
      " Done: SUBCAT_Tca\n",
      "\n",
      " Processing SUBCAT_Ssri...\n",
      "  → Using imputation 1\n",
      "  → Using imputation 2\n",
      "  → Using imputation 3\n",
      "  → Using imputation 4\n",
      "  → Using imputation 5\n",
      " Done: SUBCAT_Ssri\n",
      "\n",
      " Processing SUBCAT_Snri...\n",
      "  → Using imputation 1\n",
      "  → Using imputation 2\n",
      "  → Using imputation 3\n",
      "  → Using imputation 4\n",
      "  → Using imputation 5\n",
      " Done: SUBCAT_Snri\n",
      "\n",
      " Processing SUBCAT_Tetracyclische_Antidepressiva...\n",
      "  → Using imputation 1\n",
      "  → Using imputation 2\n",
      "  → Using imputation 3\n",
      "  → Using imputation 4\n",
      "  → Using imputation 5\n",
      " Done: SUBCAT_Tetracyclische_Antidepressiva\n",
      "\n",
      " Processing SUBCAT_Antidepressiva_Overige...\n",
      "  → Using imputation 1\n",
      "  → Using imputation 2\n",
      "  → Using imputation 3\n",
      "  → Using imputation 4\n",
      "  → Using imputation 5\n",
      " Done: SUBCAT_Antidepressiva_Overige\n",
      "\n",
      " Processing SUBCAT_Systemische_Antihistaminica...\n",
      "  → Using imputation 1\n",
      "  → Using imputation 2\n",
      "  → Using imputation 3\n",
      "  → Using imputation 4\n",
      "  → Using imputation 5\n",
      " Done: SUBCAT_Systemische_Antihistaminica\n",
      "\n",
      " Processing SUBCAT_Anxiolytica_Benzodiazepine...\n",
      "  → Using imputation 1\n",
      "  → Using imputation 2\n",
      "  → Using imputation 3\n",
      "  → Using imputation 4\n",
      "  → Using imputation 5\n",
      " Done: SUBCAT_Anxiolytica_Benzodiazepine\n",
      "\n",
      " Processing SUBCAT_Hypnotica_Benzodiazepine...\n",
      "  → Using imputation 1\n",
      "  → Using imputation 2\n",
      "  → Using imputation 3\n",
      "  → Using imputation 4\n",
      "  → Using imputation 5\n",
      " Done: SUBCAT_Hypnotica_Benzodiazepine\n",
      "\n",
      " Processing SUBCAT_Amfetaminen...\n",
      "  → Using imputation 1\n",
      "  → Using imputation 2\n",
      "  → Using imputation 3\n",
      "  → Using imputation 4\n",
      "  → Using imputation 5\n",
      " Done: SUBCAT_Amfetaminen\n",
      "\n",
      " Processing SUBCAT_Systemische_Betablokkers...\n",
      "  → Using imputation 1\n",
      "  → Using imputation 2\n",
      "  → Using imputation 3\n",
      "  → Using imputation 4\n",
      "  → Using imputation 5\n",
      " Done: SUBCAT_Systemische_Betablokkers\n",
      "\n",
      " Processing SUBCAT_Paracetamol_Mono...\n",
      "  → Using imputation 1\n",
      "  → Using imputation 2\n",
      "  → Using imputation 3\n",
      "  → Using imputation 4\n",
      "  → Using imputation 5\n",
      " Done: SUBCAT_Paracetamol_Mono\n",
      "\n",
      " Processing SUBCAT_Anti_Epileptica_Stemmingsstabilisatoren...\n",
      "  → Using imputation 1\n",
      "  → Using imputation 2\n",
      "  → Using imputation 3\n",
      "  → Using imputation 4\n",
      "  → Using imputation 5\n",
      " Done: SUBCAT_Anti_Epileptica_Stemmingsstabilisatoren\n",
      "\n",
      " Processing SUBCAT_Opioden...\n",
      "  → Using imputation 1\n",
      "  → Using imputation 2\n",
      "  → Using imputation 3\n",
      "  → Using imputation 4\n",
      "  → Using imputation 5\n",
      " Done: SUBCAT_Opioden\n",
      "\n",
      " Processing SUBCAT_Z_Drugs...\n",
      "  → Using imputation 1\n",
      "  → Using imputation 2\n",
      "  → Using imputation 3\n",
      "  → Using imputation 4\n",
      "  → Using imputation 5\n",
      " Done: SUBCAT_Z_Drugs\n",
      "\n",
      " Processing SUBCAT_Nsaids...\n",
      "  → Using imputation 1\n",
      "  → Using imputation 2\n",
      "  → Using imputation 3\n",
      "  → Using imputation 4\n",
      "  → Using imputation 5\n",
      " Done: SUBCAT_Nsaids\n",
      "\n",
      " All SUBCAT group analyses complete.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "def run_all_SUBCAT_group_models(imputed_dfs):\n",
    "    \"\"\"\n",
    "    Runs downstream analysis for each SUBCAT medisubcation group using imputed datasets.\n",
    "\n",
    "    Parameters:\n",
    "    - imputed_dfs: list of 5 imputed DataFrames (from df_imputed_final_imp1.pkl ... imp5.pkl)\n",
    "    \n",
    "    Notes:\n",
    "    - Covariate lists must be defined as global variables: covariates_subcat_<group>\n",
    "    - Outputs are saved in: outputs/SUBCAT_<GROUP>/\n",
    "    \"\"\"\n",
    "\n",
    "    print(\" Starting analysis for all SUBCAT groups\")\n",
    "\n",
    "    for var_name in globals():\n",
    "        if var_name.lower().startswith(\"covariates_subcat_\") and isinstance(globals()[var_name], list):\n",
    "            group_name = var_name.replace(\"covariates_\", \"\")\n",
    "            group_name = group_name.replace(\"_\", \" \").title().replace(\" \", \"_\")  # e.g., subcat_z_drugs → Subcat_Z_Drugs\n",
    "            group_name = group_name.replace(\"Subcat_\", \"SUBCAT_\")  # force prefix to uppercase\n",
    "\n",
    "            covariates = globals()[var_name]\n",
    "            output_dir = f\"outputs/{group_name}\"\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "            print(f\"\\n Processing {group_name}...\")\n",
    "\n",
    "            for k, df_imp in enumerate(imputed_dfs):\n",
    "                print(f\"  → Using imputation {k+1}\")\n",
    "\n",
    "                # Define X and Y\n",
    "                X = df_imp[covariates]\n",
    "                Y = df_imp[\"caps5_change_baseline\"]\n",
    "\n",
    "                # === Save X and Y as placeholder (replace with modeling later)\n",
    "                X.to_csv(f\"{output_dir}/X_imp{k+1}.csv\", index=False)\n",
    "                Y.to_frame(name=\"Y\").to_csv(f\"{output_dir}/Y_imp{k+1}.csv\", index=False)\n",
    "\n",
    "            print(f\" Done: {group_name}\")\n",
    "\n",
    "    print(\"\\n All SUBCAT group analyses complete.\")\n",
    "\n",
    "# ========= STEP 4: Execute ========= #\n",
    "run_all_SUBCAT_group_models(imputed_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "675d93ec-45cf-4e6d-ae5e-c44c514bf83d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " SUBCAT_Antipsychotica_atypisch\n",
      "  Imp 1: Treated = 355, Control = 5770, Missing = 0\n",
      "  Imp 2: Treated = 355, Control = 5770, Missing = 0\n",
      "  Imp 3: Treated = 355, Control = 5770, Missing = 0\n",
      "  Imp 4: Treated = 355, Control = 5770, Missing = 0\n",
      "  Imp 5: Treated = 355, Control = 5770, Missing = 0\n",
      "\n",
      " SUBCAT_TCA\n",
      "  Imp 1: Treated = 117, Control = 6008, Missing = 0\n",
      "  Imp 2: Treated = 117, Control = 6008, Missing = 0\n",
      "  Imp 3: Treated = 117, Control = 6008, Missing = 0\n",
      "  Imp 4: Treated = 117, Control = 6008, Missing = 0\n",
      "  Imp 5: Treated = 117, Control = 6008, Missing = 0\n",
      "\n",
      " SUBCAT_SSRI\n",
      "  Imp 1: Treated = 555, Control = 5570, Missing = 0\n",
      "  Imp 2: Treated = 555, Control = 5570, Missing = 0\n",
      "  Imp 3: Treated = 555, Control = 5570, Missing = 0\n",
      "  Imp 4: Treated = 555, Control = 5570, Missing = 0\n",
      "  Imp 5: Treated = 555, Control = 5570, Missing = 0\n",
      "\n",
      " SUBCAT_SNRI\n",
      "  Imp 1: Treated = 106, Control = 6019, Missing = 0\n",
      "  Imp 2: Treated = 106, Control = 6019, Missing = 0\n",
      "  Imp 3: Treated = 106, Control = 6019, Missing = 0\n",
      "  Imp 4: Treated = 106, Control = 6019, Missing = 0\n",
      "  Imp 5: Treated = 106, Control = 6019, Missing = 0\n",
      "\n",
      " SUBCAT_Tetracyclische_antidepressiva\n",
      "  Imp 1: Treated = 115, Control = 6010, Missing = 0\n",
      "  Imp 2: Treated = 115, Control = 6010, Missing = 0\n",
      "  Imp 3: Treated = 115, Control = 6010, Missing = 0\n",
      "  Imp 4: Treated = 115, Control = 6010, Missing = 0\n",
      "  Imp 5: Treated = 115, Control = 6010, Missing = 0\n",
      "\n",
      " SUBCAT_Antidepressiva_overige\n",
      "  Imp 1: Treated = 60, Control = 6065, Missing = 0\n",
      "  Imp 2: Treated = 60, Control = 6065, Missing = 0\n",
      "  Imp 3: Treated = 60, Control = 6065, Missing = 0\n",
      "  Imp 4: Treated = 60, Control = 6065, Missing = 0\n",
      "  Imp 5: Treated = 60, Control = 6065, Missing = 0\n",
      "\n",
      " SUBCAT_Systemische_antihistaminica\n",
      "  Imp 1: Treated = 86, Control = 6039, Missing = 0\n",
      "  Imp 2: Treated = 86, Control = 6039, Missing = 0\n",
      "  Imp 3: Treated = 86, Control = 6039, Missing = 0\n",
      "  Imp 4: Treated = 86, Control = 6039, Missing = 0\n",
      "  Imp 5: Treated = 86, Control = 6039, Missing = 0\n",
      "\n",
      " SUBCAT_anxiolytica_Benzodiazepine\n",
      "  Imp 1: Treated = 442, Control = 5683, Missing = 0\n",
      "  Imp 2: Treated = 442, Control = 5683, Missing = 0\n",
      "  Imp 3: Treated = 442, Control = 5683, Missing = 0\n",
      "  Imp 4: Treated = 442, Control = 5683, Missing = 0\n",
      "  Imp 5: Treated = 442, Control = 5683, Missing = 0\n",
      "\n",
      " SUBCAT_hypnotica_Benzodiazepine\n",
      "  Imp 1: Treated = 185, Control = 5940, Missing = 0\n",
      "  Imp 2: Treated = 185, Control = 5940, Missing = 0\n",
      "  Imp 3: Treated = 185, Control = 5940, Missing = 0\n",
      "  Imp 4: Treated = 185, Control = 5940, Missing = 0\n",
      "  Imp 5: Treated = 185, Control = 5940, Missing = 0\n",
      "\n",
      " SUBCAT_Amfetaminen\n",
      "  Imp 1: Treated = 82, Control = 6043, Missing = 0\n",
      "  Imp 2: Treated = 82, Control = 6043, Missing = 0\n",
      "  Imp 3: Treated = 82, Control = 6043, Missing = 0\n",
      "  Imp 4: Treated = 82, Control = 6043, Missing = 0\n",
      "  Imp 5: Treated = 82, Control = 6043, Missing = 0\n",
      "\n",
      " SUBCAT_Systemische_betablokkers\n",
      "  Imp 1: Treated = 35, Control = 6090, Missing = 0\n",
      "  Imp 2: Treated = 35, Control = 6090, Missing = 0\n",
      "  Imp 3: Treated = 35, Control = 6090, Missing = 0\n",
      "  Imp 4: Treated = 35, Control = 6090, Missing = 0\n",
      "  Imp 5: Treated = 35, Control = 6090, Missing = 0\n",
      "\n",
      " SUBCAT_Paracetamol_mono\n",
      "  Imp 1: Treated = 67, Control = 6058, Missing = 0\n",
      "  Imp 2: Treated = 67, Control = 6058, Missing = 0\n",
      "  Imp 3: Treated = 67, Control = 6058, Missing = 0\n",
      "  Imp 4: Treated = 67, Control = 6058, Missing = 0\n",
      "  Imp 5: Treated = 67, Control = 6058, Missing = 0\n",
      "\n",
      " SUBCAT_Anti_epileptica_stemmingsstabilisatoren\n",
      "  Imp 1: Treated = 83, Control = 6042, Missing = 0\n",
      "  Imp 2: Treated = 83, Control = 6042, Missing = 0\n",
      "  Imp 3: Treated = 83, Control = 6042, Missing = 0\n",
      "  Imp 4: Treated = 83, Control = 6042, Missing = 0\n",
      "  Imp 5: Treated = 83, Control = 6042, Missing = 0\n",
      "\n",
      " SUBCAT_Opioden\n",
      "  Imp 1: Treated = 79, Control = 6046, Missing = 0\n",
      "  Imp 2: Treated = 79, Control = 6046, Missing = 0\n",
      "  Imp 3: Treated = 79, Control = 6046, Missing = 0\n",
      "  Imp 4: Treated = 79, Control = 6046, Missing = 0\n",
      "  Imp 5: Treated = 79, Control = 6046, Missing = 0\n",
      "\n",
      " SUBCAT_Z_drugs\n",
      "  Imp 1: Treated = 89, Control = 6036, Missing = 0\n",
      "  Imp 2: Treated = 89, Control = 6036, Missing = 0\n",
      "  Imp 3: Treated = 89, Control = 6036, Missing = 0\n",
      "  Imp 4: Treated = 89, Control = 6036, Missing = 0\n",
      "  Imp 5: Treated = 89, Control = 6036, Missing = 0\n",
      "\n",
      " SUBCAT_NSAIDs\n",
      "  Imp 1: Treated = 52, Control = 6073, Missing = 0\n",
      "  Imp 2: Treated = 52, Control = 6073, Missing = 0\n",
      "  Imp 3: Treated = 52, Control = 6073, Missing = 0\n",
      "  Imp 4: Treated = 52, Control = 6073, Missing = 0\n",
      "  Imp 5: Treated = 52, Control = 6073, Missing = 0\n"
     ]
    }
   ],
   "source": [
    "for treatment_var in medication_groups:\n",
    "    print(f\"\\n {treatment_var}\")\n",
    "    \n",
    "    for i, df in enumerate(imputed_dfs):\n",
    "        if treatment_var not in df.columns:\n",
    "            print(f\"  Imp {i+1}:  Not found in columns.\")\n",
    "            continue\n",
    "\n",
    "        treated = (df[treatment_var] == 1).sum()\n",
    "        control = (df[treatment_var] == 0).sum()\n",
    "        missing = df[treatment_var].isna().sum()\n",
    "\n",
    "        print(f\"  Imp {i+1}: Treated = {treated}, Control = {control}, Missing = {missing}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "56242839-e5b1-4303-b947-ddf075afb59d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Processing VIF for SUBCAT_Antipsychotica_atypisch\n",
      " ✅ Saved: outputs\\SUBCAT_Antipsychotica_atypisch/pooled_vif.csv\n",
      "\n",
      "🔍 Processing VIF for SUBCAT_TCA\n",
      " ✅ Saved: outputs\\SUBCAT_TCA/pooled_vif.csv\n",
      "\n",
      "🔍 Processing VIF for SUBCAT_SSRI\n",
      " ✅ Saved: outputs\\SUBCAT_SSRI/pooled_vif.csv\n",
      "\n",
      "🔍 Processing VIF for SUBCAT_SNRI\n",
      " ✅ Saved: outputs\\SUBCAT_SNRI/pooled_vif.csv\n",
      "\n",
      "🔍 Processing VIF for SUBCAT_Tetracyclische_antidepressiva\n",
      " ✅ Saved: outputs\\SUBCAT_Tetracyclische_antidepressiva/pooled_vif.csv\n",
      "\n",
      "🔍 Processing VIF for SUBCAT_Antidepressiva_overige\n",
      " ✅ Saved: outputs\\SUBCAT_Antidepressiva_overige/pooled_vif.csv\n",
      "\n",
      "🔍 Processing VIF for SUBCAT_Systemische_antihistaminica\n",
      " ✅ Saved: outputs\\SUBCAT_Systemische_antihistaminica/pooled_vif.csv\n",
      "\n",
      "🔍 Processing VIF for SUBCAT_anxiolytica_Benzodiazepine\n",
      " ✅ Saved: outputs\\SUBCAT_anxiolytica_Benzodiazepine/pooled_vif.csv\n",
      "\n",
      "🔍 Processing VIF for SUBCAT_hypnotica_Benzodiazepine\n",
      " ✅ Saved: outputs\\SUBCAT_hypnotica_Benzodiazepine/pooled_vif.csv\n",
      "\n",
      "🔍 Processing VIF for SUBCAT_Amfetaminen\n",
      " ✅ Saved: outputs\\SUBCAT_Amfetaminen/pooled_vif.csv\n",
      "\n",
      "🔍 Processing VIF for SUBCAT_Systemische_betablokkers\n",
      " ✅ Saved: outputs\\SUBCAT_Systemische_betablokkers/pooled_vif.csv\n",
      "\n",
      "🔍 Processing VIF for SUBCAT_Paracetamol_mono\n",
      " ✅ Saved: outputs\\SUBCAT_Paracetamol_mono/pooled_vif.csv\n",
      "\n",
      "🔍 Processing VIF for SUBCAT_Anti_epileptica_stemmingsstabilisatoren\n",
      " ✅ Saved: outputs\\SUBCAT_Anti_epileptica_stemmingsstabilisatoren/pooled_vif.csv\n",
      "\n",
      "🔍 Processing VIF for SUBCAT_Opioden\n",
      " ✅ Saved: outputs\\SUBCAT_Opioden/pooled_vif.csv\n",
      "\n",
      "🔍 Processing VIF for SUBCAT_Z_drugs\n",
      " ✅ Saved: outputs\\SUBCAT_Z_drugs/pooled_vif.csv\n",
      "\n",
      "🔍 Processing VIF for SUBCAT_NSAIDs\n",
      " ✅ Saved: outputs\\SUBCAT_NSAIDs/pooled_vif.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from collections import defaultdict\n",
    "\n",
    "# ✅ VIF computation function\n",
    "def compute_vif(X):\n",
    "    X = sm.add_constant(X, has_constant='add')\n",
    "    vif_df = pd.DataFrame()\n",
    "    vif_df[\"variable\"] = X.columns\n",
    "    vif_df[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "    return vif_df\n",
    "\n",
    "# ✅ Process each group\n",
    "for group in medication_groups:\n",
    "    print(f\"\\n🔍 Processing VIF for {group}\")\n",
    "\n",
    "    if group not in final_covariates_map:\n",
    "        print(f\" ⚠️ No covariates found for {group}. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    covariates = final_covariates_map[group]\n",
    "    vif_list = []\n",
    "\n",
    "    for i, df_imp in enumerate(imputed_dfs):\n",
    "        try:\n",
    "            X = df_imp[covariates].copy()\n",
    "            vif_df = compute_vif(X)\n",
    "            vif_df[\"imputation\"] = i + 1\n",
    "            vif_list.append(vif_df)\n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ Failed on imputation {i+1} for {group}: {e}\")\n",
    "\n",
    "    if vif_list:\n",
    "        all_vif = pd.concat(vif_list)\n",
    "        pooled_vif = all_vif.groupby(\"variable\")[\"VIF\"].mean().reset_index()\n",
    "        pooled_vif = pooled_vif.sort_values(by=\"VIF\", ascending=False)\n",
    "\n",
    "        output_folder = os.path.join(\"outputs\", group)\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "        pooled_vif.to_csv(os.path.join(output_folder, \"pooled_vif.csv\"), index=False)\n",
    "\n",
    "        print(f\" ✅ Saved: {output_folder}/pooled_vif.csv\")\n",
    "    else:\n",
    "        print(f\" ⚠️ Skipped {group}: No valid imputations.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ca4774b3-375f-4f4f-af16-075947427200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Running PS estimation for SUBCAT_Antipsychotica_atypisch\n",
      "   Imp 1: AUC = 0.998, ROC saved.\n",
      "   Imp 2: AUC = 0.998, ROC saved.\n",
      "   Imp 3: AUC = 0.998, ROC saved.\n",
      "   Imp 4: AUC = 0.998, ROC saved.\n",
      "   Imp 5: AUC = 0.998, ROC saved.\n",
      " Composite PS + AUC saved for SUBCAT_Antipsychotica_atypisch\n",
      " Running PS estimation for SUBCAT_TCA\n",
      "   Imp 1: AUC = 0.717, ROC saved.\n",
      "   Imp 2: AUC = 0.700, ROC saved.\n",
      "   Imp 3: AUC = 0.687, ROC saved.\n",
      "   Imp 4: AUC = 0.700, ROC saved.\n",
      "   Imp 5: AUC = 0.716, ROC saved.\n",
      " Composite PS + AUC saved for SUBCAT_TCA\n",
      " Running PS estimation for SUBCAT_SSRI\n",
      "   Imp 1: AUC = 0.763, ROC saved.\n",
      "   Imp 2: AUC = 0.768, ROC saved.\n",
      "   Imp 3: AUC = 0.759, ROC saved.\n",
      "   Imp 4: AUC = 0.767, ROC saved.\n",
      "   Imp 5: AUC = 0.769, ROC saved.\n",
      " Composite PS + AUC saved for SUBCAT_SSRI\n",
      " Running PS estimation for SUBCAT_SNRI\n",
      "   Imp 1: AUC = 0.730, ROC saved.\n",
      "   Imp 2: AUC = 0.721, ROC saved.\n",
      "   Imp 3: AUC = 0.733, ROC saved.\n",
      "   Imp 4: AUC = 0.747, ROC saved.\n",
      "   Imp 5: AUC = 0.738, ROC saved.\n",
      " Composite PS + AUC saved for SUBCAT_SNRI\n",
      " Running PS estimation for SUBCAT_Tetracyclische_antidepressiva\n",
      "   Imp 1: AUC = 0.607, ROC saved.\n",
      "   Imp 2: AUC = 0.617, ROC saved.\n",
      "   Imp 3: AUC = 0.617, ROC saved.\n",
      "   Imp 4: AUC = 0.626, ROC saved.\n",
      "   Imp 5: AUC = 0.607, ROC saved.\n",
      " Composite PS + AUC saved for SUBCAT_Tetracyclische_antidepressiva\n",
      " Running PS estimation for SUBCAT_Antidepressiva_overige\n",
      "   Imp 1: AUC = 0.685, ROC saved.\n",
      "   Imp 2: AUC = 0.679, ROC saved.\n",
      "   Imp 3: AUC = 0.693, ROC saved.\n",
      "   Imp 4: AUC = 0.688, ROC saved.\n",
      "   Imp 5: AUC = 0.664, ROC saved.\n",
      " Composite PS + AUC saved for SUBCAT_Antidepressiva_overige\n",
      " Running PS estimation for SUBCAT_Systemische_antihistaminica\n",
      "   Imp 1: AUC = 0.837, ROC saved.\n",
      "   Imp 2: AUC = 0.830, ROC saved.\n",
      "   Imp 3: AUC = 0.850, ROC saved.\n",
      "   Imp 4: AUC = 0.845, ROC saved.\n",
      "   Imp 5: AUC = 0.824, ROC saved.\n",
      " Composite PS + AUC saved for SUBCAT_Systemische_antihistaminica\n",
      " Running PS estimation for SUBCAT_anxiolytica_Benzodiazepine\n",
      "   Imp 1: AUC = 0.790, ROC saved.\n",
      "   Imp 2: AUC = 0.786, ROC saved.\n",
      "   Imp 3: AUC = 0.788, ROC saved.\n",
      "   Imp 4: AUC = 0.790, ROC saved.\n",
      "   Imp 5: AUC = 0.784, ROC saved.\n",
      " Composite PS + AUC saved for SUBCAT_anxiolytica_Benzodiazepine\n",
      " Running PS estimation for SUBCAT_hypnotica_Benzodiazepine\n",
      "   Imp 1: AUC = 0.754, ROC saved.\n",
      "   Imp 2: AUC = 0.743, ROC saved.\n",
      "   Imp 3: AUC = 0.750, ROC saved.\n",
      "   Imp 4: AUC = 0.744, ROC saved.\n",
      "   Imp 5: AUC = 0.739, ROC saved.\n",
      " Composite PS + AUC saved for SUBCAT_hypnotica_Benzodiazepine\n",
      " Running PS estimation for SUBCAT_Amfetaminen\n",
      "   Imp 1: AUC = 0.592, ROC saved.\n",
      "   Imp 2: AUC = 0.615, ROC saved.\n",
      "   Imp 3: AUC = 0.624, ROC saved.\n",
      "   Imp 4: AUC = 0.632, ROC saved.\n",
      "   Imp 5: AUC = 0.556, ROC saved.\n",
      " Composite PS + AUC saved for SUBCAT_Amfetaminen\n",
      " Running PS estimation for SUBCAT_Systemische_betablokkers\n",
      "   Imp 1: AUC = 0.776, ROC saved.\n",
      "   Imp 2: AUC = 0.715, ROC saved.\n",
      "   Imp 3: AUC = 0.760, ROC saved.\n",
      "   Imp 4: AUC = 0.781, ROC saved.\n",
      "   Imp 5: AUC = 0.747, ROC saved.\n",
      " Composite PS + AUC saved for SUBCAT_Systemische_betablokkers\n",
      " Running PS estimation for SUBCAT_Paracetamol_mono\n",
      "   Imp 1: AUC = 0.690, ROC saved.\n",
      "   Imp 2: AUC = 0.734, ROC saved.\n",
      "   Imp 3: AUC = 0.667, ROC saved.\n",
      "   Imp 4: AUC = 0.711, ROC saved.\n",
      "   Imp 5: AUC = 0.688, ROC saved.\n",
      " Composite PS + AUC saved for SUBCAT_Paracetamol_mono\n",
      " Running PS estimation for SUBCAT_Anti_epileptica_stemmingsstabilisatoren\n",
      "   Imp 1: AUC = 0.847, ROC saved.\n",
      "   Imp 2: AUC = 0.847, ROC saved.\n",
      "   Imp 3: AUC = 0.852, ROC saved.\n",
      "   Imp 4: AUC = 0.847, ROC saved.\n",
      "   Imp 5: AUC = 0.846, ROC saved.\n",
      " Composite PS + AUC saved for SUBCAT_Anti_epileptica_stemmingsstabilisatoren\n",
      " Running PS estimation for SUBCAT_Opioden\n",
      "   Imp 1: AUC = 0.672, ROC saved.\n",
      "   Imp 2: AUC = 0.669, ROC saved.\n",
      "   Imp 3: AUC = 0.699, ROC saved.\n",
      "   Imp 4: AUC = 0.715, ROC saved.\n",
      "   Imp 5: AUC = 0.683, ROC saved.\n",
      " Composite PS + AUC saved for SUBCAT_Opioden\n",
      " Running PS estimation for SUBCAT_Z_drugs\n",
      "   Imp 1: AUC = 0.763, ROC saved.\n",
      "   Imp 2: AUC = 0.761, ROC saved.\n",
      "   Imp 3: AUC = 0.773, ROC saved.\n",
      "   Imp 4: AUC = 0.761, ROC saved.\n",
      "   Imp 5: AUC = 0.752, ROC saved.\n",
      " Composite PS + AUC saved for SUBCAT_Z_drugs\n",
      " Running PS estimation for SUBCAT_NSAIDs\n",
      "   Imp 1: AUC = 0.721, ROC saved.\n",
      "   Imp 2: AUC = 0.694, ROC saved.\n",
      "   Imp 3: AUC = 0.691, ROC saved.\n",
      "   Imp 4: AUC = 0.718, ROC saved.\n",
      "   Imp 5: AUC = 0.625, ROC saved.\n",
      " Composite PS + AUC saved for SUBCAT_NSAIDs\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ---------- PS Estimation Function ----------\n",
    "def run_logistic_ps_modeling(imputed_dfs, medication_groups, final_covariates_map):\n",
    "    for group in medication_groups:\n",
    "        print(f\" Running PS estimation for {group}\")\n",
    "\n",
    "        if group not in final_covariates_map:\n",
    "            print(f\" No covariates found for {group}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        output_folder = os.path.join(\"outputs\", group)\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "        covariates = final_covariates_map[group]\n",
    "        ps_matrix = pd.DataFrame()\n",
    "        auc_list = []\n",
    "\n",
    "        for i, df_imp in enumerate(imputed_dfs):\n",
    "            if group not in df_imp.columns:\n",
    "                print(f\" {group} not found in imputed dataset {i+1}. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            X = df_imp[covariates].copy()\n",
    "            T = df_imp[group]\n",
    "\n",
    "            # Drop missing treatment rows\n",
    "            valid_idx = T.dropna().index\n",
    "            X = X.loc[valid_idx]\n",
    "            T = T.loc[valid_idx]\n",
    "\n",
    "            # Train-test split for ROC\n",
    "            X_train, X_test, T_train, T_test = train_test_split(\n",
    "                X, T, stratify=T, test_size=0.3, random_state=42\n",
    "            )\n",
    "\n",
    "            try:\n",
    "                model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "                model.fit(X_train, T_train)\n",
    "\n",
    "                ps_scores = model.predict_proba(X)[:, 1]\n",
    "                ps_matrix[f\"ps_imp{i+1}\"] = pd.Series(ps_scores, index=valid_idx)\n",
    "\n",
    "                # ROC & AUC\n",
    "                auc = roc_auc_score(T_test, model.predict_proba(X_test)[:, 1])\n",
    "                auc_list.append(auc)\n",
    "\n",
    "                fpr, tpr, _ = roc_curve(T_test, model.predict_proba(X_test)[:, 1])\n",
    "                plt.figure()\n",
    "                plt.plot(fpr, tpr, label=f\"AUC = {auc:.3f}\")\n",
    "                plt.plot([0, 1], [0, 1], 'k--')\n",
    "                plt.xlabel(\"False Positive Rate\")\n",
    "                plt.ylabel(\"True Positive Rate\")\n",
    "                plt.title(f\"ROC Curve - {group} (Imp {i+1})\")\n",
    "                plt.legend()\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(os.path.join(output_folder, f\"roc_curve_imp{i+1}.png\"))\n",
    "                plt.close()\n",
    "                print(f\"   Imp {i+1}: AUC = {auc:.3f}, ROC saved.\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"   Error in {group} (imp {i+1}): {e}\")\n",
    "\n",
    "        # Save AUCs and Composite PS\n",
    "        if not ps_matrix.empty:\n",
    "            # Fill NaN rows (from dropped subjects in some imputations) with mean\n",
    "            ps_matrix[\"composite_ps\"] = ps_matrix.mean(axis=1)\n",
    "            ps_matrix.to_excel(os.path.join(output_folder, \"propensity_scores.xlsx\"))\n",
    "\n",
    "            auc_df = pd.DataFrame({\n",
    "                \"imputation\": [f\"imp{i+1}\" for i in range(len(auc_list))],\n",
    "                \"AUC\": auc_list\n",
    "            })\n",
    "            auc_df.loc[len(auc_df.index)] = [\"mean\", np.mean(auc_list) if auc_list else np.nan]\n",
    "            auc_df.to_excel(os.path.join(output_folder, \"auc_scores.xlsx\"), index=False)\n",
    "\n",
    "            print(f\" Composite PS + AUC saved for {group}\")\n",
    "        else:\n",
    "            print(f\" No valid PS scores generated for {group}\")\n",
    "\n",
    "# ---------- Run ----------\n",
    "run_logistic_ps_modeling(imputed_dfs, medication_groups, final_covariates_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "38f7f379-7ca4-4f21-8cdc-767c8814a4ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Computing feature importance for SUBCAT_Antipsychotica_atypisch\n",
      " Saved feature importance plot and CSV for SUBCAT_Antipsychotica_atypisch\n",
      "\n",
      " Computing feature importance for SUBCAT_TCA\n",
      " Saved feature importance plot and CSV for SUBCAT_TCA\n",
      "\n",
      " Computing feature importance for SUBCAT_SSRI\n",
      " Saved feature importance plot and CSV for SUBCAT_SSRI\n",
      "\n",
      " Computing feature importance for SUBCAT_SNRI\n",
      " Saved feature importance plot and CSV for SUBCAT_SNRI\n",
      "\n",
      " Computing feature importance for SUBCAT_Tetracyclische_antidepressiva\n",
      " Saved feature importance plot and CSV for SUBCAT_Tetracyclische_antidepressiva\n",
      "\n",
      " Computing feature importance for SUBCAT_Antidepressiva_overige\n",
      " Saved feature importance plot and CSV for SUBCAT_Antidepressiva_overige\n",
      "\n",
      " Computing feature importance for SUBCAT_Systemische_antihistaminica\n",
      " Saved feature importance plot and CSV for SUBCAT_Systemische_antihistaminica\n",
      "\n",
      " Computing feature importance for SUBCAT_anxiolytica_Benzodiazepine\n",
      " Saved feature importance plot and CSV for SUBCAT_anxiolytica_Benzodiazepine\n",
      "\n",
      " Computing feature importance for SUBCAT_hypnotica_Benzodiazepine\n",
      " Saved feature importance plot and CSV for SUBCAT_hypnotica_Benzodiazepine\n",
      "\n",
      " Computing feature importance for SUBCAT_Amfetaminen\n",
      " Saved feature importance plot and CSV for SUBCAT_Amfetaminen\n",
      "\n",
      " Computing feature importance for SUBCAT_Systemische_betablokkers\n",
      " Saved feature importance plot and CSV for SUBCAT_Systemische_betablokkers\n",
      "\n",
      " Computing feature importance for SUBCAT_Paracetamol_mono\n",
      " Saved feature importance plot and CSV for SUBCAT_Paracetamol_mono\n",
      "\n",
      " Computing feature importance for SUBCAT_Anti_epileptica_stemmingsstabilisatoren\n",
      " Saved feature importance plot and CSV for SUBCAT_Anti_epileptica_stemmingsstabilisatoren\n",
      "\n",
      " Computing feature importance for SUBCAT_Opioden\n",
      " Saved feature importance plot and CSV for SUBCAT_Opioden\n",
      "\n",
      " Computing feature importance for SUBCAT_Z_drugs\n",
      " Saved feature importance plot and CSV for SUBCAT_Z_drugs\n",
      "\n",
      " Computing feature importance for SUBCAT_NSAIDs\n",
      " Saved feature importance plot and CSV for SUBCAT_NSAIDs\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def compute_feature_importance(imputed_dfs, medication_groups, final_covariates_map):\n",
    "    for group in medication_groups:\n",
    "        print(f\"\\n Computing feature importance for {group}\")\n",
    "\n",
    "        if group not in final_covariates_map:\n",
    "            print(f\" No covariates found for {group}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        covariates = final_covariates_map[group]\n",
    "        output_folder = os.path.join(\"outputs\", group)\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "        importance_df_list = []\n",
    "\n",
    "        for i, df_imp in enumerate(imputed_dfs):\n",
    "            if group not in df_imp.columns:\n",
    "                print(f\" {group} not in imputed dataset {i+1}. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            X = df_imp[covariates].copy()\n",
    "            T = df_imp[group]\n",
    "\n",
    "            # Drop NaNs in treatment\n",
    "            valid_idx = T.dropna().index\n",
    "            X = X.loc[valid_idx]\n",
    "            T = T.loc[valid_idx]\n",
    "\n",
    "            try:\n",
    "                model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "                model.fit(X, T)\n",
    "\n",
    "                # Get feature importance (absolute coefficients)\n",
    "                importances = np.abs(model.coef_[0])\n",
    "                importance_dict = dict(zip(X.columns, importances))\n",
    "                df_feat = pd.DataFrame.from_dict(importance_dict, orient='index', columns=[f\"imp{i+1}\"])\n",
    "                df_feat.index.name = 'feature'\n",
    "                importance_df_list.append(df_feat)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"   Error during modeling: {e}\")\n",
    "\n",
    "        if importance_df_list:\n",
    "            # Combine and average\n",
    "            all_feat = pd.concat(importance_df_list, axis=1).fillna(0)\n",
    "            all_feat[\"mean_importance\"] = all_feat.mean(axis=1)\n",
    "\n",
    "            # Filter top 30 non-zero\n",
    "            non_zero = all_feat[all_feat[\"mean_importance\"] > 0]\n",
    "            top30 = non_zero.sort_values(by=\"mean_importance\", ascending=False).head(30)\n",
    "\n",
    "            # Save to CSV\n",
    "            top30.to_csv(os.path.join(output_folder, \"feature_importance.csv\"))\n",
    "\n",
    "            # Plot\n",
    "            plt.figure(figsize=(10, 8))\n",
    "            plt.barh(top30.index[::-1], top30[\"mean_importance\"][::-1])  # plot top → bottom\n",
    "            plt.xlabel(\"Mean Gain Importance\")\n",
    "            plt.title(f\"Top 30 Feature Importance - {group}\")\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(output_folder, \"feature_importance_top30.png\"))\n",
    "            plt.close()\n",
    "\n",
    "            print(f\" Saved feature importance plot and CSV for {group}\")\n",
    "        else:\n",
    "            print(f\" No valid models for {group}\")\n",
    "\n",
    "#  Run\n",
    "compute_feature_importance(imputed_dfs, medication_groups, final_covariates_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "61c1cc1f-9ec7-402b-a19a-534d28c833ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Processing IPTW + trimming + clipping for SUBCAT_Antipsychotica_atypisch\n",
      "✅ Saved IPTW weights for SUBCAT_Antipsychotica_atypisch\n",
      "    ℹ️ Retained 232/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SUBCAT_Antipsychotica_atypisch/trimmed_data_imp1.*\n",
      "    ℹ️ Retained 233/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SUBCAT_Antipsychotica_atypisch/trimmed_data_imp2.*\n",
      "    ℹ️ Retained 232/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SUBCAT_Antipsychotica_atypisch/trimmed_data_imp3.*\n",
      "    ℹ️ Retained 230/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SUBCAT_Antipsychotica_atypisch/trimmed_data_imp4.*\n",
      "    ℹ️ Retained 238/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SUBCAT_Antipsychotica_atypisch/trimmed_data_imp5.*\n",
      "\n",
      "🔍 Processing IPTW + trimming + clipping for SUBCAT_TCA\n",
      "✅ Saved IPTW weights for SUBCAT_TCA\n",
      "    ℹ️ Retained 356/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SUBCAT_TCA/trimmed_data_imp1.*\n",
      "    ℹ️ Retained 384/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SUBCAT_TCA/trimmed_data_imp2.*\n",
      "    ℹ️ Retained 371/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SUBCAT_TCA/trimmed_data_imp3.*\n",
      "    ℹ️ Retained 364/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SUBCAT_TCA/trimmed_data_imp4.*\n",
      "    ℹ️ Retained 382/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SUBCAT_TCA/trimmed_data_imp5.*\n",
      "\n",
      "🔍 Processing IPTW + trimming + clipping for SUBCAT_SSRI\n",
      "✅ Saved IPTW weights for SUBCAT_SSRI\n",
      "    ℹ️ Retained 3356/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SUBCAT_SSRI/trimmed_data_imp1.*\n",
      "    ℹ️ Retained 3294/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SUBCAT_SSRI/trimmed_data_imp2.*\n",
      "    ℹ️ Retained 3362/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SUBCAT_SSRI/trimmed_data_imp3.*\n",
      "    ℹ️ Retained 3286/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SUBCAT_SSRI/trimmed_data_imp4.*\n",
      "    ℹ️ Retained 3354/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SUBCAT_SSRI/trimmed_data_imp5.*\n",
      "\n",
      "🔍 Processing IPTW + trimming + clipping for SUBCAT_SNRI\n",
      "✅ Saved IPTW weights for SUBCAT_SNRI\n",
      "    ℹ️ Retained 363/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SUBCAT_SNRI/trimmed_data_imp1.*\n",
      "    ℹ️ Retained 372/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SUBCAT_SNRI/trimmed_data_imp2.*\n",
      "    ℹ️ Retained 372/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SUBCAT_SNRI/trimmed_data_imp3.*\n",
      "    ℹ️ Retained 363/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SUBCAT_SNRI/trimmed_data_imp4.*\n",
      "    ℹ️ Retained 377/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SUBCAT_SNRI/trimmed_data_imp5.*\n",
      "\n",
      "🔍 Processing IPTW + trimming + clipping for SUBCAT_Tetracyclische_antidepressiva\n",
      "✅ Saved IPTW weights for SUBCAT_Tetracyclische_antidepressiva\n",
      "    ℹ️ Retained 365/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SUBCAT_Tetracyclische_antidepressiva/trimmed_data_imp1.*\n",
      "    ℹ️ Retained 389/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SUBCAT_Tetracyclische_antidepressiva/trimmed_data_imp2.*\n",
      "    ℹ️ Retained 377/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SUBCAT_Tetracyclische_antidepressiva/trimmed_data_imp3.*\n",
      "    ℹ️ Retained 378/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SUBCAT_Tetracyclische_antidepressiva/trimmed_data_imp4.*\n",
      "    ℹ️ Retained 358/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SUBCAT_Tetracyclische_antidepressiva/trimmed_data_imp5.*\n",
      "\n",
      "🔍 Processing IPTW + trimming + clipping for SUBCAT_Antidepressiva_overige\n",
      "✅ Saved IPTW weights for SUBCAT_Antidepressiva_overige\n",
      "    ℹ️ Retained 72/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SUBCAT_Antidepressiva_overige/trimmed_data_imp1.*\n",
      "    ℹ️ Retained 79/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SUBCAT_Antidepressiva_overige/trimmed_data_imp2.*\n",
      "    ℹ️ Retained 71/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SUBCAT_Antidepressiva_overige/trimmed_data_imp3.*\n",
      "    ℹ️ Retained 90/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SUBCAT_Antidepressiva_overige/trimmed_data_imp4.*\n",
      "    ℹ️ Retained 66/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SUBCAT_Antidepressiva_overige/trimmed_data_imp5.*\n",
      "\n",
      "🔍 Processing IPTW + trimming + clipping for SUBCAT_Systemische_antihistaminica\n",
      "✅ Saved IPTW weights for SUBCAT_Systemische_antihistaminica\n",
      "    ℹ️ Retained 241/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SUBCAT_Systemische_antihistaminica/trimmed_data_imp1.*\n",
      "    ℹ️ Retained 236/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SUBCAT_Systemische_antihistaminica/trimmed_data_imp2.*\n",
      "    ℹ️ Retained 240/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SUBCAT_Systemische_antihistaminica/trimmed_data_imp3.*\n",
      "    ℹ️ Retained 228/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SUBCAT_Systemische_antihistaminica/trimmed_data_imp4.*\n",
      "    ℹ️ Retained 240/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SUBCAT_Systemische_antihistaminica/trimmed_data_imp5.*\n",
      "\n",
      "🔍 Processing IPTW + trimming + clipping for SUBCAT_anxiolytica_Benzodiazepine\n",
      "✅ Saved IPTW weights for SUBCAT_anxiolytica_Benzodiazepine\n",
      "    ℹ️ Retained 2124/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SUBCAT_anxiolytica_Benzodiazepine/trimmed_data_imp1.*\n",
      "    ℹ️ Retained 2129/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SUBCAT_anxiolytica_Benzodiazepine/trimmed_data_imp2.*\n",
      "    ℹ️ Retained 2142/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SUBCAT_anxiolytica_Benzodiazepine/trimmed_data_imp3.*\n",
      "    ℹ️ Retained 2153/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SUBCAT_anxiolytica_Benzodiazepine/trimmed_data_imp4.*\n",
      "    ℹ️ Retained 2111/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SUBCAT_anxiolytica_Benzodiazepine/trimmed_data_imp5.*\n",
      "\n",
      "🔍 Processing IPTW + trimming + clipping for SUBCAT_hypnotica_Benzodiazepine\n",
      "✅ Saved IPTW weights for SUBCAT_hypnotica_Benzodiazepine\n",
      "    ℹ️ Retained 878/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SUBCAT_hypnotica_Benzodiazepine/trimmed_data_imp1.*\n",
      "    ℹ️ Retained 891/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SUBCAT_hypnotica_Benzodiazepine/trimmed_data_imp2.*\n",
      "    ℹ️ Retained 878/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SUBCAT_hypnotica_Benzodiazepine/trimmed_data_imp3.*\n",
      "    ℹ️ Retained 881/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SUBCAT_hypnotica_Benzodiazepine/trimmed_data_imp4.*\n",
      "    ℹ️ Retained 882/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SUBCAT_hypnotica_Benzodiazepine/trimmed_data_imp5.*\n",
      "\n",
      "🔍 Processing IPTW + trimming + clipping for SUBCAT_Amfetaminen\n",
      "✅ Saved IPTW weights for SUBCAT_Amfetaminen\n",
      "    ℹ️ Retained 161/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SUBCAT_Amfetaminen/trimmed_data_imp1.*\n",
      "    ℹ️ Retained 160/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SUBCAT_Amfetaminen/trimmed_data_imp2.*\n",
      "    ℹ️ Retained 170/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SUBCAT_Amfetaminen/trimmed_data_imp3.*\n",
      "    ℹ️ Retained 157/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SUBCAT_Amfetaminen/trimmed_data_imp4.*\n",
      "    ℹ️ Retained 189/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SUBCAT_Amfetaminen/trimmed_data_imp5.*\n",
      "\n",
      "🔍 Processing IPTW + trimming + clipping for SUBCAT_Systemische_betablokkers\n",
      "✅ Saved IPTW weights for SUBCAT_Systemische_betablokkers\n",
      "    ℹ️ Retained 70/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SUBCAT_Systemische_betablokkers/trimmed_data_imp1.*\n",
      "    ℹ️ Retained 71/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SUBCAT_Systemische_betablokkers/trimmed_data_imp2.*\n",
      "    ℹ️ Retained 63/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SUBCAT_Systemische_betablokkers/trimmed_data_imp3.*\n",
      "    ℹ️ Retained 64/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SUBCAT_Systemische_betablokkers/trimmed_data_imp4.*\n",
      "    ℹ️ Retained 63/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SUBCAT_Systemische_betablokkers/trimmed_data_imp5.*\n",
      "\n",
      "🔍 Processing IPTW + trimming + clipping for SUBCAT_Paracetamol_mono\n",
      "✅ Saved IPTW weights for SUBCAT_Paracetamol_mono\n",
      "    ℹ️ Retained 111/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SUBCAT_Paracetamol_mono/trimmed_data_imp1.*\n",
      "    ℹ️ Retained 95/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SUBCAT_Paracetamol_mono/trimmed_data_imp2.*\n",
      "    ℹ️ Retained 79/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SUBCAT_Paracetamol_mono/trimmed_data_imp3.*\n",
      "    ℹ️ Retained 107/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SUBCAT_Paracetamol_mono/trimmed_data_imp4.*\n",
      "    ℹ️ Retained 112/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SUBCAT_Paracetamol_mono/trimmed_data_imp5.*\n",
      "\n",
      "🔍 Processing IPTW + trimming + clipping for SUBCAT_Anti_epileptica_stemmingsstabilisatoren\n",
      "✅ Saved IPTW weights for SUBCAT_Anti_epileptica_stemmingsstabilisatoren\n",
      "    ℹ️ Retained 392/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SUBCAT_Anti_epileptica_stemmingsstabilisatoren/trimmed_data_imp1.*\n",
      "    ℹ️ Retained 370/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SUBCAT_Anti_epileptica_stemmingsstabilisatoren/trimmed_data_imp2.*\n",
      "    ℹ️ Retained 376/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SUBCAT_Anti_epileptica_stemmingsstabilisatoren/trimmed_data_imp3.*\n",
      "    ℹ️ Retained 380/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SUBCAT_Anti_epileptica_stemmingsstabilisatoren/trimmed_data_imp4.*\n",
      "    ℹ️ Retained 378/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SUBCAT_Anti_epileptica_stemmingsstabilisatoren/trimmed_data_imp5.*\n",
      "\n",
      "🔍 Processing IPTW + trimming + clipping for SUBCAT_Opioden\n",
      "✅ Saved IPTW weights for SUBCAT_Opioden\n",
      "    ℹ️ Retained 227/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SUBCAT_Opioden/trimmed_data_imp1.*\n",
      "    ℹ️ Retained 233/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SUBCAT_Opioden/trimmed_data_imp2.*\n",
      "    ℹ️ Retained 229/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SUBCAT_Opioden/trimmed_data_imp3.*\n",
      "    ℹ️ Retained 227/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SUBCAT_Opioden/trimmed_data_imp4.*\n",
      "    ℹ️ Retained 226/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SUBCAT_Opioden/trimmed_data_imp5.*\n",
      "\n",
      "🔍 Processing IPTW + trimming + clipping for SUBCAT_Z_drugs\n",
      "✅ Saved IPTW weights for SUBCAT_Z_drugs\n",
      "    ℹ️ Retained 309/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SUBCAT_Z_drugs/trimmed_data_imp1.*\n",
      "    ℹ️ Retained 319/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SUBCAT_Z_drugs/trimmed_data_imp2.*\n",
      "    ℹ️ Retained 318/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SUBCAT_Z_drugs/trimmed_data_imp3.*\n",
      "    ℹ️ Retained 317/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SUBCAT_Z_drugs/trimmed_data_imp4.*\n",
      "    ℹ️ Retained 306/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SUBCAT_Z_drugs/trimmed_data_imp5.*\n",
      "\n",
      "🔍 Processing IPTW + trimming + clipping for SUBCAT_NSAIDs\n",
      "✅ Saved IPTW weights for SUBCAT_NSAIDs\n",
      "    ℹ️ Retained 91/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SUBCAT_NSAIDs/trimmed_data_imp1.*\n",
      "    ℹ️ Retained 97/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SUBCAT_NSAIDs/trimmed_data_imp2.*\n",
      "    ℹ️ Retained 86/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SUBCAT_NSAIDs/trimmed_data_imp3.*\n",
      "    ℹ️ Retained 94/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SUBCAT_NSAIDs/trimmed_data_imp4.*\n",
      "    ℹ️ Retained 92/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SUBCAT_NSAIDs/trimmed_data_imp5.*\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def compute_trimmed_clipped_iptw(ps_df, treatment, lower=0.05, upper=0.95, clip_max=10):\n",
    "    weights = []\n",
    "    keep_mask = (ps_df > lower) & (ps_df < upper)\n",
    "\n",
    "    for i in range(ps_df.shape[1]):\n",
    "        ps = ps_df.iloc[:, i].clip(lower=1e-6, upper=1 - 1e-6)  # avoid div by zero\n",
    "        mask = keep_mask.iloc[:, i]\n",
    "        w = pd.Series(np.nan, index=ps.index)\n",
    "\n",
    "        w[mask & (treatment == 1)] = 1 / ps[mask & (treatment == 1)]\n",
    "        w[mask & (treatment == 0)] = 1 / (1 - ps[mask & (treatment == 0)])\n",
    "        w = w.clip(upper=clip_max)\n",
    "        weights.append(w)\n",
    "\n",
    "    return pd.concat(weights, axis=1)\n",
    "\n",
    "\n",
    "def apply_rubins_rule_to_iptw(iptw_matrix):\n",
    "    \"\"\"\n",
    "    Given an IPTW matrix (n rows × M imputations), return Rubin’s rule pooled mean, SD, SE.\n",
    "    \"\"\"\n",
    "    M = iptw_matrix.shape[1]\n",
    "    q_bar = iptw_matrix.mean(axis=1)\n",
    "    u_bar = iptw_matrix.var(axis=1, ddof=1)\n",
    "    B = iptw_matrix.apply(lambda x: x.mean(), axis=1).var(ddof=1)\n",
    "    total_var = u_bar + (1 + 1/M) * B\n",
    "    total_se = np.sqrt(total_var)\n",
    "    return q_bar, u_bar.pow(0.5), total_se\n",
    "\n",
    "\n",
    "def run_trim_clip_save_all(imputed_dfs, medication_groups, final_covariates_map):\n",
    "    for group in medication_groups:\n",
    "        print(f\"\\n🔍 Processing IPTW + trimming + clipping for {group}\")\n",
    "        output_folder = os.path.join(\"outputs\", group)\n",
    "        ps_path = os.path.join(output_folder, \"propensity_scores.xlsx\")\n",
    "\n",
    "        if not os.path.exists(ps_path):\n",
    "            print(f\"⚠️ Missing PS file: {ps_path}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            ps_all = pd.read_excel(ps_path, index_col=0)\n",
    "            ps_cols = [col for col in ps_all.columns if col.startswith(\"ps_imp\")]\n",
    "            composite_index = ps_all.index\n",
    "\n",
    "            # Get treatment from one imputed dataset\n",
    "            T_full = None\n",
    "            for df in imputed_dfs:\n",
    "                if group in df.columns:\n",
    "                    T_full = df.loc[composite_index, group]\n",
    "                    break\n",
    "\n",
    "            if T_full is None:\n",
    "                print(f\"❌ Treatment column {group} not found in any imputed dataset.\")\n",
    "                continue\n",
    "\n",
    "            # Compute IPTW matrix (shape: n × M)\n",
    "            iptw_matrix = compute_trimmed_clipped_iptw(ps_all[ps_cols], T_full)\n",
    "            iptw_matrix.columns = [f\"iptw_imp{i+1}\" for i in range(iptw_matrix.shape[1])]\n",
    "\n",
    "            # Apply Rubin’s Rule for mean, SD, SE\n",
    "            iptw_matrix[\"iptw_mean\"], iptw_matrix[\"iptw_sd\"], iptw_matrix[\"iptw_se\"] = apply_rubins_rule_to_iptw(\n",
    "                iptw_matrix[[f\"iptw_imp{i+1}\" for i in range(iptw_matrix.shape[1])]]\n",
    "            )\n",
    "\n",
    "            # Save IPTW matrix separately\n",
    "            iptw_matrix.to_excel(os.path.join(output_folder, \"iptw_weights.xlsx\"))\n",
    "            print(f\"✅ Saved IPTW weights for {group}\")\n",
    "\n",
    "            # Save trimmed & clipped imputed datasets with IPTW\n",
    "            for i in range(5):\n",
    "                df = imputed_dfs[i].copy()\n",
    "                if group not in df.columns:\n",
    "                    continue\n",
    "\n",
    "                trimmed_idx = iptw_matrix.index.intersection(df.index)\n",
    "                needed_cols = final_covariates_map[group] + [group, \"caps5_change_baseline\"]\n",
    "\n",
    "                # Select only necessary columns\n",
    "                df_trimmed = df.loc[trimmed_idx, needed_cols].copy()\n",
    "                df_trimmed[\"iptw\"] = iptw_matrix[f\"iptw_imp{i+1}\"].loc[trimmed_idx]\n",
    "\n",
    "                # ✅ DROP rows with missing IPTW values\n",
    "                before = len(df_trimmed)\n",
    "                df_trimmed = df_trimmed.dropna(subset=[\"iptw\"])\n",
    "                after = len(df_trimmed)\n",
    "                print(f\"    ℹ️ Retained {after}/{before} rows after IPTW NaN drop.\")\n",
    "\n",
    "                # Save to .pkl\n",
    "                df_trimmed.to_pickle(os.path.join(output_folder, f\"trimmed_data_imp{i+1}.pkl\"))\n",
    "                print(f\"  💾 Saved trimmed dataset: {output_folder}/trimmed_data_imp{i+1}.*\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error in {group}: {e}\")\n",
    "\n",
    "\n",
    "run_trim_clip_save_all(imputed_dfs, medication_groups, final_covariates_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2e57891b-20aa-401e-b95e-019da87c3dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Plotting PS overlap for SUBCAT_Antipsychotica_atypisch\n",
      "✅ Saved unweighted and weighted PS plots for SUBCAT_Antipsychotica_atypisch\n",
      "\n",
      "📊 Plotting PS overlap for SUBCAT_TCA\n",
      "✅ Saved unweighted and weighted PS plots for SUBCAT_TCA\n",
      "\n",
      "📊 Plotting PS overlap for SUBCAT_SSRI\n",
      "✅ Saved unweighted and weighted PS plots for SUBCAT_SSRI\n",
      "\n",
      "📊 Plotting PS overlap for SUBCAT_SNRI\n",
      "✅ Saved unweighted and weighted PS plots for SUBCAT_SNRI\n",
      "\n",
      "📊 Plotting PS overlap for SUBCAT_Tetracyclische_antidepressiva\n",
      "✅ Saved unweighted and weighted PS plots for SUBCAT_Tetracyclische_antidepressiva\n",
      "\n",
      "📊 Plotting PS overlap for SUBCAT_Antidepressiva_overige\n",
      "✅ Saved unweighted and weighted PS plots for SUBCAT_Antidepressiva_overige\n",
      "\n",
      "📊 Plotting PS overlap for SUBCAT_Systemische_antihistaminica\n",
      "✅ Saved unweighted and weighted PS plots for SUBCAT_Systemische_antihistaminica\n",
      "\n",
      "📊 Plotting PS overlap for SUBCAT_anxiolytica_Benzodiazepine\n",
      "✅ Saved unweighted and weighted PS plots for SUBCAT_anxiolytica_Benzodiazepine\n",
      "\n",
      "📊 Plotting PS overlap for SUBCAT_hypnotica_Benzodiazepine\n",
      "✅ Saved unweighted and weighted PS plots for SUBCAT_hypnotica_Benzodiazepine\n",
      "\n",
      "📊 Plotting PS overlap for SUBCAT_Amfetaminen\n",
      "✅ Saved unweighted and weighted PS plots for SUBCAT_Amfetaminen\n",
      "\n",
      "📊 Plotting PS overlap for SUBCAT_Systemische_betablokkers\n",
      "✅ Saved unweighted and weighted PS plots for SUBCAT_Systemische_betablokkers\n",
      "\n",
      "📊 Plotting PS overlap for SUBCAT_Paracetamol_mono\n",
      "✅ Saved unweighted and weighted PS plots for SUBCAT_Paracetamol_mono\n",
      "\n",
      "📊 Plotting PS overlap for SUBCAT_Anti_epileptica_stemmingsstabilisatoren\n",
      "✅ Saved unweighted and weighted PS plots for SUBCAT_Anti_epileptica_stemmingsstabilisatoren\n",
      "\n",
      "📊 Plotting PS overlap for SUBCAT_Opioden\n",
      "✅ Saved unweighted and weighted PS plots for SUBCAT_Opioden\n",
      "\n",
      "📊 Plotting PS overlap for SUBCAT_Z_drugs\n",
      "✅ Saved unweighted and weighted PS plots for SUBCAT_Z_drugs\n",
      "\n",
      "📊 Plotting PS overlap for SUBCAT_NSAIDs\n",
      "✅ Saved unweighted and weighted PS plots for SUBCAT_NSAIDs\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_ps_overlap_all_groups(medication_groups):\n",
    "    for group in medication_groups:\n",
    "        print(f\"\\n📊 Plotting PS overlap for {group}\")\n",
    "\n",
    "        folder = os.path.join(\"outputs\", group)\n",
    "        ps_file = os.path.join(folder, \"propensity_scores.xlsx\")\n",
    "        iptw_file = os.path.join(folder, \"iptw_weights.xlsx\")\n",
    "        trimmed_file = os.path.join(folder, \"trimmed_data_imp1.pkl\")\n",
    "\n",
    "        if not all(os.path.exists(f) for f in [ps_file, iptw_file, trimmed_file]):\n",
    "            print(f\"⚠️ Missing required files for {group}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            ps_df = pd.read_excel(ps_file, index_col=0)\n",
    "            iptw_df = pd.read_excel(iptw_file, index_col=0)\n",
    "            trimmed_df = pd.read_pickle(trimmed_file)\n",
    "\n",
    "            # Extract\n",
    "            ps = ps_df[\"composite_ps\"].reindex(trimmed_df.index)\n",
    "            w = iptw_df[\"iptw_mean\"].reindex(trimmed_df.index)\n",
    "            T = trimmed_df[group]\n",
    "\n",
    "            # Masks to remove NaNs\n",
    "            treated_mask = (T == 1) & ps.notna() & w.notna()\n",
    "            control_mask = (T == 0) & ps.notna() & w.notna()\n",
    "\n",
    "            treated = ps[treated_mask]\n",
    "            treated_w = w[treated_mask]\n",
    "\n",
    "            control = ps[control_mask]\n",
    "            control_w = w[control_mask]\n",
    "\n",
    "            # === Unweighted Plot ===\n",
    "            plt.figure(figsize=(8, 5))\n",
    "            plt.hist([treated, control], bins=25, label=[\"Treated\", \"Control\"], alpha=0.6)\n",
    "            plt.title(f\"Unweighted PS Overlap - {group}\")\n",
    "            plt.xlabel(\"Composite Propensity Score\")\n",
    "            plt.ylabel(\"Count\")\n",
    "            plt.legend()\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(folder, \"ps_overlap_unweighted.png\"))\n",
    "            plt.close()\n",
    "\n",
    "            # === Weighted Plot ===\n",
    "            plt.figure(figsize=(8, 5))\n",
    "            plt.hist([treated, control], bins=25, weights=[treated_w, control_w], label=[\"Treated\", \"Control\"], alpha=0.6)\n",
    "            plt.title(f\"Weighted PS Overlap - {group}\")\n",
    "            plt.xlabel(\"Composite Propensity Score\")\n",
    "            plt.ylabel(\"Weighted Count\")\n",
    "            plt.legend()\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(folder, \"ps_overlap_weighted.png\"))\n",
    "            plt.close()\n",
    "\n",
    "            print(f\"✅ Saved unweighted and weighted PS plots for {group}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error processing {group}: {e}\")\n",
    "\n",
    "# 🔁 Run\n",
    "plot_ps_overlap_all_groups(medication_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1ff6b954-c0fd-4644-b88b-4d5e3073188a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ✅ Saved: outputs\\SUBCAT_Antipsychotica_atypisch\\four_panel_overlap_SUBCAT_Antipsychotica_atypisch.png\n",
      " ✅ Saved: outputs\\SUBCAT_TCA\\four_panel_overlap_SUBCAT_TCA.png\n",
      " ✅ Saved: outputs\\SUBCAT_SSRI\\four_panel_overlap_SUBCAT_SSRI.png\n",
      " ✅ Saved: outputs\\SUBCAT_SNRI\\four_panel_overlap_SUBCAT_SNRI.png\n",
      " ✅ Saved: outputs\\SUBCAT_Tetracyclische_antidepressiva\\four_panel_overlap_SUBCAT_Tetracyclische_antidepressiva.png\n",
      " ✅ Saved: outputs\\SUBCAT_Antidepressiva_overige\\four_panel_overlap_SUBCAT_Antidepressiva_overige.png\n",
      " ✅ Saved: outputs\\SUBCAT_Systemische_antihistaminica\\four_panel_overlap_SUBCAT_Systemische_antihistaminica.png\n",
      " ✅ Saved: outputs\\SUBCAT_anxiolytica_Benzodiazepine\\four_panel_overlap_SUBCAT_anxiolytica_Benzodiazepine.png\n",
      " ✅ Saved: outputs\\SUBCAT_hypnotica_Benzodiazepine\\four_panel_overlap_SUBCAT_hypnotica_Benzodiazepine.png\n",
      " ✅ Saved: outputs\\SUBCAT_Amfetaminen\\four_panel_overlap_SUBCAT_Amfetaminen.png\n",
      " ✅ Saved: outputs\\SUBCAT_Systemische_betablokkers\\four_panel_overlap_SUBCAT_Systemische_betablokkers.png\n",
      " ✅ Saved: outputs\\SUBCAT_Paracetamol_mono\\four_panel_overlap_SUBCAT_Paracetamol_mono.png\n",
      " ✅ Saved: outputs\\SUBCAT_Anti_epileptica_stemmingsstabilisatoren\\four_panel_overlap_SUBCAT_Anti_epileptica_stemmingsstabilisatoren.png\n",
      " ✅ Saved: outputs\\SUBCAT_Opioden\\four_panel_overlap_SUBCAT_Opioden.png\n",
      " ✅ Saved: outputs\\SUBCAT_Z_drugs\\four_panel_overlap_SUBCAT_Z_drugs.png\n",
      " ✅ Saved: outputs\\SUBCAT_NSAIDs\\four_panel_overlap_SUBCAT_NSAIDs.png\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set up base output folder\n",
    "output_base = \"outputs\"\n",
    "ps_file = \"propensity_scores.xlsx\"\n",
    "iptw_file = \"iptw_weights.xlsx\"\n",
    "trimmed_data_file = \"trimmed_data_imp1.pkl\"\n",
    "\n",
    "# Collect all treatment group folders\n",
    "groups = [g for g in medication_groups if os.path.isdir(os.path.join(output_base, g))]\n",
    "\n",
    "\n",
    "# Generate 4-panel overlap plots\n",
    "for group in groups:\n",
    "    group_path = os.path.join(output_base, group)\n",
    "    try:\n",
    "        # Load trimmed treatment info\n",
    "        trimmed_df = pd.read_pickle(os.path.join(group_path, trimmed_data_file))\n",
    "        index = trimmed_df.index\n",
    "\n",
    "        # Fix: case-insensitive match for treatment variable\n",
    "        possible_cols = [col for col in trimmed_df.columns if col.upper() == group.upper()]\n",
    "        if not possible_cols:\n",
    "            print(f\" Treatment variable {group} not found in {group}, skipping.\")\n",
    "            continue\n",
    "        treatment_var = possible_cols[0]\n",
    "        T = trimmed_df[treatment_var]\n",
    "\n",
    "        # Load composite PS (aligned to trimmed_df index)\n",
    "        ps_df = pd.read_excel(os.path.join(group_path, ps_file), index_col=0)\n",
    "        if 'composite_ps' not in ps_df.columns:\n",
    "            print(f\" Composite column missing in {ps_file}, skipping {group}.\")\n",
    "            continue\n",
    "        ps = ps_df.loc[index, 'composite_ps']\n",
    "\n",
    "        # Load IPTW weights (aligned to trimmed_df index)\n",
    "        weights_df = pd.read_excel(os.path.join(group_path, iptw_file), index_col=0)\n",
    "        if 'iptw_mean' not in weights_df.columns:\n",
    "            print(f\" IPTW weight column missing in {iptw_file}, skipping {group}.\")\n",
    "            continue\n",
    "        weights = weights_df.loc[index, 'iptw_mean']\n",
    "\n",
    "        # Prepare 4 datasets\n",
    "        raw_treated = ps[T == 1]\n",
    "        raw_control = ps[T == 0]\n",
    "        weighted_treated = (ps[T == 1], weights[T == 1])\n",
    "        weighted_control = (ps[T == 0], weights[T == 0])\n",
    "\n",
    "        # Create plot\n",
    "        fig, axs = plt.subplots(2, 2, figsize=(12, 8))\n",
    "        fig.suptitle(f\"Propensity Score Distribution - {group}\", fontsize=14)\n",
    "\n",
    "        axs[0, 0].hist(raw_treated, bins=20, alpha=0.7, color='blue')\n",
    "        axs[0, 0].set_title(\"Raw Treated\")\n",
    "\n",
    "        axs[0, 1].hist(raw_control, bins=20, alpha=0.7, color='green')\n",
    "        axs[0, 1].set_title(\"Raw Control\")\n",
    "\n",
    "        axs[1, 0].hist(weighted_treated[0], bins=20, weights=weighted_treated[1], alpha=0.7, color='blue')\n",
    "        axs[1, 0].set_title(\"Weighted Treated\")\n",
    "\n",
    "        axs[1, 1].hist(weighted_control[0], bins=20, weights=weighted_control[1], alpha=0.7, color='green')\n",
    "        axs[1, 1].set_title(\"Weighted Control\")\n",
    "\n",
    "        for ax in axs.flat:\n",
    "            ax.set_xlim(0, 1)\n",
    "            ax.set_xlabel(\"Propensity Score\")\n",
    "            ax.set_ylabel(\"Count\")\n",
    "\n",
    "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "\n",
    "        # Save figure\n",
    "        plot_path = os.path.join(group_path, f\"four_panel_overlap_{group}.png\")\n",
    "        plt.savefig(plot_path)\n",
    "        plt.close()\n",
    "        print(f\" ✅ Saved: {plot_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\" ❌ Error in {group}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "69b56397-7cf3-4956-94f0-b657d5eab983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ATT calculation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3e629fc1-6c45-4ea9-b8a0-b6491f004f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weighted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2580922c-2fd5-49b1-8083-ee1ddbfa0cfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🚀 Running OLS for SUBCAT_Antipsychotica_atypisch\n",
      "✅ SUBCAT_Antipsychotica_atypisch | Seed 1: ATT = 6.6376, SE = 8.4840, p = 0.47772\n",
      "✅ SUBCAT_Antipsychotica_atypisch | Seed 2: ATT = 6.1659, SE = 8.4248, p = 0.50482\n",
      "✅ SUBCAT_Antipsychotica_atypisch | Seed 3: ATT = 4.0529, SE = 5.1990, p = 0.47920\n",
      "✅ SUBCAT_Antipsychotica_atypisch | Seed 4: ATT = 7.3070, SE = 7.5990, p = 0.39071\n",
      "✅ SUBCAT_Antipsychotica_atypisch | Seed 5: ATT = 1.8224, SE = 4.3531, p = 0.69697\n",
      "✅ SUBCAT_Antipsychotica_atypisch | Seed 6: ATT = 5.3180, SE = 7.5841, p = 0.52183\n",
      "✅ SUBCAT_Antipsychotica_atypisch | Seed 7: ATT = 6.2190, SE = 7.7693, p = 0.46828\n",
      "✅ SUBCAT_Antipsychotica_atypisch | Seed 8: ATT = 5.0271, SE = 8.4432, p = 0.58362\n",
      "✅ SUBCAT_Antipsychotica_atypisch | Seed 9: ATT = 2.6389, SE = 7.1649, p = 0.73130\n",
      "✅ SUBCAT_Antipsychotica_atypisch | Seed 10: ATT = 5.5002, SE = 7.8783, p = 0.52354\n",
      "📊 Diagnostic plots saved for SUBCAT_Antipsychotica_atypisch\n",
      "🏆 Best result for SUBCAT_Antipsychotica_atypisch → Seed 5 | SE = 4.3531\n",
      "\n",
      "🚀 Running OLS for SUBCAT_TCA\n",
      "✅ SUBCAT_TCA | Seed 1: ATT = 3.6779, SE = 3.1162, p = 0.30329\n",
      "✅ SUBCAT_TCA | Seed 2: ATT = 1.5525, SE = 4.9746, p = 0.77056\n",
      "✅ SUBCAT_TCA | Seed 3: ATT = 2.5640, SE = 4.3959, p = 0.59102\n",
      "✅ SUBCAT_TCA | Seed 4: ATT = 2.8941, SE = 2.9352, p = 0.37995\n",
      "✅ SUBCAT_TCA | Seed 5: ATT = 1.4071, SE = 4.2544, p = 0.75743\n",
      "✅ SUBCAT_TCA | Seed 6: ATT = 4.8820, SE = 4.0056, p = 0.28988\n",
      "✅ SUBCAT_TCA | Seed 7: ATT = 1.9506, SE = 3.3613, p = 0.59282\n",
      "✅ SUBCAT_TCA | Seed 8: ATT = 3.0040, SE = 3.9661, p = 0.49097\n",
      "✅ SUBCAT_TCA | Seed 9: ATT = 2.7511, SE = 3.8466, p = 0.51401\n",
      "✅ SUBCAT_TCA | Seed 10: ATT = 2.6831, SE = 4.5595, p = 0.58784\n",
      "📊 Diagnostic plots saved for SUBCAT_TCA\n",
      "🏆 Best result for SUBCAT_TCA → Seed 4 | SE = 2.9352\n",
      "\n",
      "🚀 Running OLS for SUBCAT_SSRI\n",
      "✅ SUBCAT_SSRI | Seed 1: ATT = 0.9315, SE = 0.9299, p = 0.37320\n",
      "✅ SUBCAT_SSRI | Seed 2: ATT = 0.7214, SE = 0.7912, p = 0.41343\n",
      "✅ SUBCAT_SSRI | Seed 3: ATT = 1.1982, SE = 1.2546, p = 0.39362\n",
      "✅ SUBCAT_SSRI | Seed 4: ATT = 1.1976, SE = 0.9123, p = 0.25952\n",
      "✅ SUBCAT_SSRI | Seed 5: ATT = 1.1082, SE = 0.8538, p = 0.26407\n",
      "✅ SUBCAT_SSRI | Seed 6: ATT = 0.3895, SE = 1.2050, p = 0.76271\n",
      "✅ SUBCAT_SSRI | Seed 7: ATT = 1.1099, SE = 1.4324, p = 0.48167\n",
      "✅ SUBCAT_SSRI | Seed 8: ATT = 0.8027, SE = 0.9781, p = 0.45788\n",
      "✅ SUBCAT_SSRI | Seed 9: ATT = 0.5965, SE = 1.0200, p = 0.59008\n",
      "✅ SUBCAT_SSRI | Seed 10: ATT = 0.4849, SE = 1.2117, p = 0.70949\n",
      "📊 Diagnostic plots saved for SUBCAT_SSRI\n",
      "🏆 Best result for SUBCAT_SSRI → Seed 2 | SE = 0.7912\n",
      "\n",
      "🚀 Running OLS for SUBCAT_SNRI\n",
      "✅ SUBCAT_SNRI | Seed 1: ATT = 3.5876, SE = 4.4480, p = 0.46514\n",
      "✅ SUBCAT_SNRI | Seed 2: ATT = 5.7981, SE = 3.9413, p = 0.21522\n",
      "✅ SUBCAT_SNRI | Seed 3: ATT = 5.6459, SE = 4.3240, p = 0.26169\n",
      "✅ SUBCAT_SNRI | Seed 4: ATT = 3.9385, SE = 5.2616, p = 0.49575\n",
      "✅ SUBCAT_SNRI | Seed 5: ATT = 5.0936, SE = 3.2344, p = 0.19042\n",
      "✅ SUBCAT_SNRI | Seed 6: ATT = 4.7836, SE = 4.8709, p = 0.38167\n",
      "✅ SUBCAT_SNRI | Seed 7: ATT = 4.3074, SE = 4.9003, p = 0.42904\n",
      "✅ SUBCAT_SNRI | Seed 8: ATT = 5.5361, SE = 5.4056, p = 0.36366\n",
      "✅ SUBCAT_SNRI | Seed 9: ATT = 3.0186, SE = 5.3221, p = 0.60091\n",
      "✅ SUBCAT_SNRI | Seed 10: ATT = 2.4085, SE = 5.6989, p = 0.69429\n",
      "📊 Diagnostic plots saved for SUBCAT_SNRI\n",
      "🏆 Best result for SUBCAT_SNRI → Seed 5 | SE = 3.2344\n",
      "\n",
      "🚀 Running OLS for SUBCAT_Tetracyclische_antidepressiva\n",
      "✅ SUBCAT_Tetracyclische_antidepressiva | Seed 1: ATT = 4.6484, SE = 4.3529, p = 0.34573\n",
      "✅ SUBCAT_Tetracyclische_antidepressiva | Seed 2: ATT = 6.4589, SE = 3.8772, p = 0.17107\n",
      "✅ SUBCAT_Tetracyclische_antidepressiva | Seed 3: ATT = 3.5524, SE = 4.0562, p = 0.43059\n",
      "✅ SUBCAT_Tetracyclische_antidepressiva | Seed 4: ATT = 2.7583, SE = 4.7363, p = 0.59156\n",
      "✅ SUBCAT_Tetracyclische_antidepressiva | Seed 5: ATT = 5.0167, SE = 4.4153, p = 0.31933\n",
      "✅ SUBCAT_Tetracyclische_antidepressiva | Seed 6: ATT = 7.7360, SE = 4.6432, p = 0.17103\n",
      "✅ SUBCAT_Tetracyclische_antidepressiva | Seed 7: ATT = 10.3403, SE = 5.4683, p = 0.13161\n",
      "✅ SUBCAT_Tetracyclische_antidepressiva | Seed 8: ATT = 4.6896, SE = 4.1195, p = 0.31852\n",
      "✅ SUBCAT_Tetracyclische_antidepressiva | Seed 9: ATT = 5.1286, SE = 4.3514, p = 0.30388\n",
      "✅ SUBCAT_Tetracyclische_antidepressiva | Seed 10: ATT = 6.4554, SE = 3.4193, p = 0.13206\n",
      "📊 Diagnostic plots saved for SUBCAT_Tetracyclische_antidepressiva\n",
      "🏆 Best result for SUBCAT_Tetracyclische_antidepressiva → Seed 10 | SE = 3.4193\n",
      "\n",
      "🚀 Running OLS for SUBCAT_Antidepressiva_overige\n",
      "✅ SUBCAT_Antidepressiva_overige | Seed 1: ATT = 6.0223, SE = 7.4771, p = 0.46572\n",
      "✅ SUBCAT_Antidepressiva_overige | Seed 2: ATT = 12.2094, SE = 11.0450, p = 0.33099\n",
      "✅ SUBCAT_Antidepressiva_overige | Seed 3: ATT = 5.3967, SE = 6.3749, p = 0.44492\n",
      "✅ SUBCAT_Antidepressiva_overige | Seed 4: ATT = 3.9803, SE = 6.7362, p = 0.58637\n",
      "✅ SUBCAT_Antidepressiva_overige | Seed 5: ATT = 5.5652, SE = 9.2312, p = 0.57911\n",
      "✅ SUBCAT_Antidepressiva_overige | Seed 6: ATT = 4.0565, SE = 11.4460, p = 0.74093\n",
      "✅ SUBCAT_Antidepressiva_overige | Seed 7: ATT = 8.2312, SE = 10.8070, p = 0.48870\n",
      "✅ SUBCAT_Antidepressiva_overige | Seed 8: ATT = 9.1595, SE = 8.8262, p = 0.35799\n",
      "✅ SUBCAT_Antidepressiva_overige | Seed 9: ATT = 5.0203, SE = 9.3709, p = 0.62054\n",
      "✅ SUBCAT_Antidepressiva_overige | Seed 10: ATT = 3.6602, SE = 7.8753, p = 0.66627\n",
      "📊 Diagnostic plots saved for SUBCAT_Antidepressiva_overige\n",
      "🏆 Best result for SUBCAT_Antidepressiva_overige → Seed 3 | SE = 6.3749\n",
      "\n",
      "🚀 Running OLS for SUBCAT_Systemische_antihistaminica\n",
      "✅ SUBCAT_Systemische_antihistaminica | Seed 1: ATT = -0.2058, SE = 4.9538, p = 0.96885\n",
      "✅ SUBCAT_Systemische_antihistaminica | Seed 2: ATT = -0.4838, SE = 3.9168, p = 0.90766\n",
      "✅ SUBCAT_Systemische_antihistaminica | Seed 3: ATT = 1.3494, SE = 7.9259, p = 0.87308\n",
      "✅ SUBCAT_Systemische_antihistaminica | Seed 4: ATT = -1.0884, SE = 3.5209, p = 0.77265\n",
      "✅ SUBCAT_Systemische_antihistaminica | Seed 5: ATT = 1.3131, SE = 4.0831, p = 0.76387\n",
      "✅ SUBCAT_Systemische_antihistaminica | Seed 6: ATT = -2.1952, SE = 4.6698, p = 0.66278\n",
      "✅ SUBCAT_Systemische_antihistaminica | Seed 7: ATT = 2.3068, SE = 6.4754, p = 0.73965\n",
      "✅ SUBCAT_Systemische_antihistaminica | Seed 8: ATT = -0.3902, SE = 4.9751, p = 0.94125\n",
      "✅ SUBCAT_Systemische_antihistaminica | Seed 9: ATT = -1.8471, SE = 3.8420, p = 0.65580\n",
      "✅ SUBCAT_Systemische_antihistaminica | Seed 10: ATT = 0.7787, SE = 3.4504, p = 0.83251\n",
      "📊 Diagnostic plots saved for SUBCAT_Systemische_antihistaminica\n",
      "🏆 Best result for SUBCAT_Systemische_antihistaminica → Seed 10 | SE = 3.4504\n",
      "\n",
      "🚀 Running OLS for SUBCAT_anxiolytica_Benzodiazepine\n",
      "✅ SUBCAT_anxiolytica_Benzodiazepine | Seed 1: ATT = 0.4275, SE = 0.9715, p = 0.68263\n",
      "✅ SUBCAT_anxiolytica_Benzodiazepine | Seed 2: ATT = 1.0480, SE = 0.9503, p = 0.33203\n",
      "✅ SUBCAT_anxiolytica_Benzodiazepine | Seed 3: ATT = 1.0950, SE = 1.4503, p = 0.49226\n",
      "✅ SUBCAT_anxiolytica_Benzodiazepine | Seed 4: ATT = 0.7137, SE = 1.0737, p = 0.54259\n",
      "✅ SUBCAT_anxiolytica_Benzodiazepine | Seed 5: ATT = 0.6684, SE = 1.4533, p = 0.66948\n",
      "✅ SUBCAT_anxiolytica_Benzodiazepine | Seed 6: ATT = 0.7530, SE = 1.7774, p = 0.69359\n",
      "✅ SUBCAT_anxiolytica_Benzodiazepine | Seed 7: ATT = 1.1536, SE = 1.4645, p = 0.47492\n",
      "✅ SUBCAT_anxiolytica_Benzodiazepine | Seed 8: ATT = 0.7793, SE = 1.2967, p = 0.58025\n",
      "✅ SUBCAT_anxiolytica_Benzodiazepine | Seed 9: ATT = 0.1330, SE = 1.8791, p = 0.94698\n",
      "✅ SUBCAT_anxiolytica_Benzodiazepine | Seed 10: ATT = 1.2633, SE = 1.1603, p = 0.33748\n",
      "📊 Diagnostic plots saved for SUBCAT_anxiolytica_Benzodiazepine\n",
      "🏆 Best result for SUBCAT_anxiolytica_Benzodiazepine → Seed 2 | SE = 0.9503\n",
      "\n",
      "🚀 Running OLS for SUBCAT_hypnotica_Benzodiazepine\n",
      "✅ SUBCAT_hypnotica_Benzodiazepine | Seed 1: ATT = 0.6182, SE = 2.2472, p = 0.79686\n",
      "✅ SUBCAT_hypnotica_Benzodiazepine | Seed 2: ATT = 1.7324, SE = 3.7876, p = 0.67113\n",
      "✅ SUBCAT_hypnotica_Benzodiazepine | Seed 3: ATT = -0.2986, SE = 1.7505, p = 0.87283\n",
      "✅ SUBCAT_hypnotica_Benzodiazepine | Seed 4: ATT = 2.4695, SE = 2.4117, p = 0.36374\n",
      "✅ SUBCAT_hypnotica_Benzodiazepine | Seed 5: ATT = 1.4542, SE = 2.5011, p = 0.59214\n",
      "✅ SUBCAT_hypnotica_Benzodiazepine | Seed 6: ATT = 0.3815, SE = 2.6483, p = 0.89243\n",
      "✅ SUBCAT_hypnotica_Benzodiazepine | Seed 7: ATT = 0.9733, SE = 2.3717, p = 0.70255\n",
      "✅ SUBCAT_hypnotica_Benzodiazepine | Seed 8: ATT = 0.9263, SE = 1.6611, p = 0.60684\n",
      "✅ SUBCAT_hypnotica_Benzodiazepine | Seed 9: ATT = 2.0337, SE = 1.9513, p = 0.35615\n",
      "✅ SUBCAT_hypnotica_Benzodiazepine | Seed 10: ATT = 1.4956, SE = 1.7960, p = 0.45181\n",
      "📊 Diagnostic plots saved for SUBCAT_hypnotica_Benzodiazepine\n",
      "🏆 Best result for SUBCAT_hypnotica_Benzodiazepine → Seed 8 | SE = 1.6611\n",
      "\n",
      "🚀 Running OLS for SUBCAT_Amfetaminen\n",
      "✅ SUBCAT_Amfetaminen | Seed 1: ATT = -3.8149, SE = 4.6485, p = 0.45792\n",
      "✅ SUBCAT_Amfetaminen | Seed 2: ATT = -2.5264, SE = 8.6659, p = 0.78514\n",
      "✅ SUBCAT_Amfetaminen | Seed 3: ATT = -3.2744, SE = 4.3657, p = 0.49495\n",
      "✅ SUBCAT_Amfetaminen | Seed 4: ATT = -0.2864, SE = 7.2288, p = 0.97029\n",
      "✅ SUBCAT_Amfetaminen | Seed 5: ATT = -1.1949, SE = 5.9409, p = 0.85041\n",
      "✅ SUBCAT_Amfetaminen | Seed 6: ATT = 0.3259, SE = 10.1018, p = 0.97581\n",
      "✅ SUBCAT_Amfetaminen | Seed 7: ATT = -4.7954, SE = 6.3193, p = 0.49020\n",
      "✅ SUBCAT_Amfetaminen | Seed 8: ATT = -1.2326, SE = 7.7959, p = 0.88203\n",
      "✅ SUBCAT_Amfetaminen | Seed 9: ATT = -2.8717, SE = 8.4595, p = 0.75134\n",
      "✅ SUBCAT_Amfetaminen | Seed 10: ATT = 1.0968, SE = 9.9632, p = 0.91764\n",
      "📊 Diagnostic plots saved for SUBCAT_Amfetaminen\n",
      "🏆 Best result for SUBCAT_Amfetaminen → Seed 3 | SE = 4.3657\n",
      "\n",
      "🚀 Running OLS for SUBCAT_Systemische_betablokkers\n",
      "✅ SUBCAT_Systemische_betablokkers | Seed 1: ATT = 14.4154, SE = 13.6210, p = 0.34959\n",
      "✅ SUBCAT_Systemische_betablokkers | Seed 2: ATT = 13.0899, SE = 18.5079, p = 0.51843\n",
      "✅ SUBCAT_Systemische_betablokkers | Seed 3: ATT = 20.4576, SE = 8.3137, p = 0.06964\n",
      "✅ SUBCAT_Systemische_betablokkers | Seed 4: ATT = 20.6537, SE = 12.5396, p = 0.17489\n",
      "✅ SUBCAT_Systemische_betablokkers | Seed 5: ATT = 15.4531, SE = 8.3738, p = 0.13872\n",
      "✅ SUBCAT_Systemische_betablokkers | Seed 6: ATT = 9.1649, SE = 10.7251, p = 0.44097\n",
      "✅ SUBCAT_Systemische_betablokkers | Seed 7: ATT = 12.2238, SE = 23.1645, p = 0.62563\n",
      "✅ SUBCAT_Systemische_betablokkers | Seed 8: ATT = 7.1505, SE = 12.9188, p = 0.60940\n",
      "✅ SUBCAT_Systemische_betablokkers | Seed 9: ATT = 22.7783, SE = 14.1958, p = 0.18385\n",
      "✅ SUBCAT_Systemische_betablokkers | Seed 10: ATT = 9.3948, SE = 24.5593, p = 0.72152\n",
      "📊 Diagnostic plots saved for SUBCAT_Systemische_betablokkers\n",
      "🏆 Best result for SUBCAT_Systemische_betablokkers → Seed 3 | SE = 8.3137\n",
      "\n",
      "🚀 Running OLS for SUBCAT_Paracetamol_mono\n",
      "✅ SUBCAT_Paracetamol_mono | Seed 1: ATT = -5.8174, SE = 8.4584, p = 0.52940\n",
      "✅ SUBCAT_Paracetamol_mono | Seed 2: ATT = 1.2725, SE = 7.4501, p = 0.87267\n",
      "✅ SUBCAT_Paracetamol_mono | Seed 3: ATT = -6.6048, SE = 7.0501, p = 0.40188\n",
      "✅ SUBCAT_Paracetamol_mono | Seed 4: ATT = 5.6373, SE = 11.7779, p = 0.65719\n",
      "✅ SUBCAT_Paracetamol_mono | Seed 5: ATT = 0.2615, SE = 7.0806, p = 0.97231\n",
      "✅ SUBCAT_Paracetamol_mono | Seed 6: ATT = 7.5781, SE = 8.6740, p = 0.43162\n",
      "✅ SUBCAT_Paracetamol_mono | Seed 7: ATT = 2.5042, SE = 15.5587, p = 0.87993\n",
      "✅ SUBCAT_Paracetamol_mono | Seed 8: ATT = -2.6504, SE = 7.8507, p = 0.75264\n",
      "✅ SUBCAT_Paracetamol_mono | Seed 9: ATT = 1.4052, SE = 18.2014, p = 0.94217\n",
      "✅ SUBCAT_Paracetamol_mono | Seed 10: ATT = -2.5137, SE = 7.8518, p = 0.76489\n",
      "📊 Diagnostic plots saved for SUBCAT_Paracetamol_mono\n",
      "🏆 Best result for SUBCAT_Paracetamol_mono → Seed 3 | SE = 7.0501\n",
      "\n",
      "🚀 Running OLS for SUBCAT_Anti_epileptica_stemmingsstabilisatoren\n",
      "✅ SUBCAT_Anti_epileptica_stemmingsstabilisatoren | Seed 1: ATT = 4.8049, SE = 6.0295, p = 0.47014\n",
      "✅ SUBCAT_Anti_epileptica_stemmingsstabilisatoren | Seed 2: ATT = 3.0580, SE = 5.0345, p = 0.57638\n",
      "✅ SUBCAT_Anti_epileptica_stemmingsstabilisatoren | Seed 3: ATT = 4.1146, SE = 4.1600, p = 0.37861\n",
      "✅ SUBCAT_Anti_epileptica_stemmingsstabilisatoren | Seed 4: ATT = 4.2360, SE = 4.2783, p = 0.37817\n",
      "✅ SUBCAT_Anti_epileptica_stemmingsstabilisatoren | Seed 5: ATT = 1.6397, SE = 4.0057, p = 0.70325\n",
      "✅ SUBCAT_Anti_epileptica_stemmingsstabilisatoren | Seed 6: ATT = 1.7338, SE = 4.6690, p = 0.72922\n",
      "✅ SUBCAT_Anti_epileptica_stemmingsstabilisatoren | Seed 7: ATT = 4.5556, SE = 3.9001, p = 0.30765\n",
      "✅ SUBCAT_Anti_epileptica_stemmingsstabilisatoren | Seed 8: ATT = 3.9006, SE = 5.3421, p = 0.50575\n",
      "✅ SUBCAT_Anti_epileptica_stemmingsstabilisatoren | Seed 9: ATT = 6.3739, SE = 3.4708, p = 0.14018\n",
      "✅ SUBCAT_Anti_epileptica_stemmingsstabilisatoren | Seed 10: ATT = 3.0471, SE = 4.5802, p = 0.54227\n",
      "📊 Diagnostic plots saved for SUBCAT_Anti_epileptica_stemmingsstabilisatoren\n",
      "🏆 Best result for SUBCAT_Anti_epileptica_stemmingsstabilisatoren → Seed 9 | SE = 3.4708\n",
      "\n",
      "🚀 Running OLS for SUBCAT_Opioden\n",
      "✅ SUBCAT_Opioden | Seed 1: ATT = 1.4730, SE = 6.8932, p = 0.84124\n",
      "✅ SUBCAT_Opioden | Seed 2: ATT = -1.3265, SE = 6.1924, p = 0.84085\n",
      "✅ SUBCAT_Opioden | Seed 3: ATT = -2.8762, SE = 4.6596, p = 0.57047\n",
      "✅ SUBCAT_Opioden | Seed 4: ATT = -1.7383, SE = 7.7696, p = 0.83393\n",
      "✅ SUBCAT_Opioden | Seed 5: ATT = 0.9454, SE = 6.5765, p = 0.89265\n",
      "✅ SUBCAT_Opioden | Seed 6: ATT = 0.3572, SE = 7.5857, p = 0.96470\n",
      "✅ SUBCAT_Opioden | Seed 7: ATT = -2.1671, SE = 7.0435, p = 0.77369\n",
      "✅ SUBCAT_Opioden | Seed 8: ATT = -0.9435, SE = 6.5163, p = 0.89188\n",
      "✅ SUBCAT_Opioden | Seed 9: ATT = -0.6749, SE = 7.4158, p = 0.93186\n",
      "✅ SUBCAT_Opioden | Seed 10: ATT = 1.3258, SE = 4.9808, p = 0.80325\n",
      "📊 Diagnostic plots saved for SUBCAT_Opioden\n",
      "🏆 Best result for SUBCAT_Opioden → Seed 3 | SE = 4.6596\n",
      "\n",
      "🚀 Running OLS for SUBCAT_Z_drugs\n",
      "✅ SUBCAT_Z_drugs | Seed 1: ATT = 6.9640, SE = 3.2108, p = 0.09593\n",
      "✅ SUBCAT_Z_drugs | Seed 2: ATT = 8.2365, SE = 4.2027, p = 0.12158\n",
      "✅ SUBCAT_Z_drugs | Seed 3: ATT = 8.4024, SE = 4.1721, p = 0.11428\n",
      "✅ SUBCAT_Z_drugs | Seed 4: ATT = 8.4701, SE = 3.8693, p = 0.09379\n",
      "✅ SUBCAT_Z_drugs | Seed 5: ATT = 8.1796, SE = 3.6780, p = 0.09022\n",
      "✅ SUBCAT_Z_drugs | Seed 6: ATT = 8.7360, SE = 4.6221, p = 0.13174\n",
      "✅ SUBCAT_Z_drugs | Seed 7: ATT = 5.7859, SE = 4.7762, p = 0.29241\n",
      "✅ SUBCAT_Z_drugs | Seed 8: ATT = 3.9839, SE = 6.7435, p = 0.58644\n",
      "✅ SUBCAT_Z_drugs | Seed 9: ATT = 7.4880, SE = 3.9179, p = 0.12856\n",
      "✅ SUBCAT_Z_drugs | Seed 10: ATT = 9.7057, SE = 4.2148, p = 0.08269\n",
      "📊 Diagnostic plots saved for SUBCAT_Z_drugs\n",
      "🏆 Best result for SUBCAT_Z_drugs → Seed 1 | SE = 3.2108\n",
      "\n",
      "🚀 Running OLS for SUBCAT_NSAIDs\n",
      "✅ SUBCAT_NSAIDs | Seed 1: ATT = -7.4759, SE = 9.0959, p = 0.45731\n",
      "✅ SUBCAT_NSAIDs | Seed 2: ATT = -3.0223, SE = 17.2504, p = 0.86943\n",
      "✅ SUBCAT_NSAIDs | Seed 3: ATT = -9.2441, SE = 4.8183, p = 0.12749\n",
      "✅ SUBCAT_NSAIDs | Seed 4: ATT = -4.8147, SE = 8.6457, p = 0.60729\n",
      "✅ SUBCAT_NSAIDs | Seed 5: ATT = -9.6802, SE = 4.6801, p = 0.10743\n",
      "✅ SUBCAT_NSAIDs | Seed 6: ATT = -10.5089, SE = 5.2993, p = 0.11839\n",
      "✅ SUBCAT_NSAIDs | Seed 7: ATT = -5.1479, SE = 10.4856, p = 0.64918\n",
      "✅ SUBCAT_NSAIDs | Seed 8: ATT = -6.4224, SE = 7.7132, p = 0.45187\n",
      "✅ SUBCAT_NSAIDs | Seed 9: ATT = -1.3908, SE = 12.4287, p = 0.91629\n",
      "✅ SUBCAT_NSAIDs | Seed 10: ATT = -5.5607, SE = 11.6414, p = 0.65782\n",
      "📊 Diagnostic plots saved for SUBCAT_NSAIDs\n",
      "🏆 Best result for SUBCAT_NSAIDs → Seed 5 | SE = 4.6801\n",
      "\n",
      "🎯 All summary files saved.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import KFold\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import t, probplot\n",
    "import xgboost as xgb\n",
    "\n",
    "# -----------------------------\n",
    "# Configuration\n",
    "# -----------------------------\n",
    "seeds = list(range(1, 11))\n",
    "imputations = 5\n",
    "output_folder = \"outputs\"\n",
    "\n",
    "# -----------------------------\n",
    "# Diagnostic Plotting Function\n",
    "# -----------------------------\n",
    "def create_diagnostic_plots(residuals_data, fitted_data, group_name):\n",
    "    \"\"\"Create 4 diagnostic plots for model validation\"\"\"\n",
    "    plots_dir = os.path.join(output_folder, \"plots\")\n",
    "    os.makedirs(plots_dir, exist_ok=True)\n",
    "    \n",
    "    # Flatten the collected data\n",
    "    all_residuals = np.concatenate(residuals_data)\n",
    "    all_fitted = np.concatenate(fitted_data)\n",
    "    \n",
    "    # Create figure with 2x2 subplots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    fig.suptitle(f'Diagnostic Plots - {group_name}', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Residuals vs Fitted\n",
    "    axes[0,0].scatter(all_fitted, all_residuals, alpha=0.6, s=20)\n",
    "    axes[0,0].axhline(y=0, color='red', linestyle='--', alpha=0.8)\n",
    "    axes[0,0].set_xlabel('Fitted Values')\n",
    "    axes[0,0].set_ylabel('Residuals')\n",
    "    axes[0,0].set_title('Residuals vs Fitted')\n",
    "    axes[0,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. QQ Plot\n",
    "    probplot(all_residuals, dist=\"norm\", plot=axes[0,1])\n",
    "    axes[0,1].set_title('Q-Q Plot (Normal)')\n",
    "    axes[0,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Residual Histogram\n",
    "    axes[1,0].hist(all_residuals, bins=30, density=True, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    axes[1,0].set_xlabel('Residuals')\n",
    "    axes[1,0].set_ylabel('Density')\n",
    "    axes[1,0].set_title('Residual Distribution')\n",
    "    axes[1,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Scale-Location Plot\n",
    "    sqrt_abs_residuals = np.sqrt(np.abs(all_residuals))\n",
    "    axes[1,1].scatter(all_fitted, sqrt_abs_residuals, alpha=0.6, s=20)\n",
    "    axes[1,1].set_xlabel('Fitted Values')\n",
    "    axes[1,1].set_ylabel('√|Residuals|')\n",
    "    axes[1,1].set_title('Scale-Location Plot')\n",
    "    axes[1,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save plot\n",
    "    plot_filename = os.path.join(plots_dir, f'{group_name}.png')\n",
    "    plt.savefig(plot_filename, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"📊 Diagnostic plots saved for {group_name}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Rubin's Rule\n",
    "# -----------------------------\n",
    "def rubins_pool(estimates, ses):\n",
    "    m = len(estimates)\n",
    "    q_bar = np.mean(estimates)\n",
    "    u_bar = np.mean(np.square(ses))\n",
    "    b_m = np.var(estimates, ddof=1)\n",
    "    total_var = u_bar + ((1 + 1/m) * b_m)\n",
    "    total_se = np.sqrt(total_var)\n",
    "    ci_lower = q_bar - 1.96 * total_se\n",
    "    ci_upper = q_bar + 1.96 * total_se\n",
    "    p_value = 2 * (1 - t.cdf(np.abs(q_bar / total_se), df=m-1))\n",
    "    rounded_p = round(p_value, 5)\n",
    "    formatted_p = \"< 0.00001\" if rounded_p <= 0.00001 else f\"{rounded_p:.5f}\"\n",
    "    return q_bar, total_se, ci_lower, ci_upper, formatted_p\n",
    "\n",
    "# -----------------------------\n",
    "# SMD + Variance Ratio\n",
    "# -----------------------------\n",
    "def calculate_smd_vr(X, T, weights):\n",
    "    treated = X[T == 1]\n",
    "    control = X[T == 0]\n",
    "    w_treated = weights[T == 1]\n",
    "    w_control = weights[T == 0]\n",
    "    smd, vr = [], []\n",
    "    for col in X.columns:\n",
    "        try:\n",
    "            m1, m0 = np.average(treated[col], weights=w_treated), np.average(control[col], weights=w_control)\n",
    "            s1 = np.sqrt(np.average((treated[col] - m1) ** 2, weights=w_treated))\n",
    "            s0 = np.sqrt(np.average((control[col] - m0) ** 2, weights=w_control))\n",
    "            pooled_sd = np.sqrt((s1 ** 2 + s0 ** 2) / 2)\n",
    "            smd.append((m1 - m0) / pooled_sd if pooled_sd > 0 else 0)\n",
    "            vr.append(s1**2 / s0**2 if s0**2 > 0 else 0)\n",
    "        except Exception:\n",
    "            smd.append(np.nan)\n",
    "            vr.append(np.nan)\n",
    "    return np.nanmean(smd), np.nanmean(vr)\n",
    "\n",
    "# -----------------------------\n",
    "# OLS Main Loop\n",
    "# -----------------------------\n",
    "def run_dml_with_trimmed_data(final_covariates_map):\n",
    "    att_results = []\n",
    "    balance_results = []\n",
    "\n",
    "    for group, covariates in final_covariates_map.items():\n",
    "        print(f\"\\n🚀 Running OLS for {group}\")\n",
    "        group_dir = os.path.join(output_folder, group)\n",
    "        os.makedirs(group_dir, exist_ok=True)\n",
    "\n",
    "        best_result = None\n",
    "        best_se = float(\"inf\")\n",
    "        \n",
    "        # Initialize lists to collect residuals and fitted values for diagnostic plots\n",
    "        group_residuals = []\n",
    "        group_fitted = []\n",
    "\n",
    "        for seed in seeds:\n",
    "            # Set random seed for this iteration\n",
    "            np.random.seed(seed)\n",
    "            \n",
    "            att_list, se_list, r2_list, rmse_list, smd_list, vr_list = [], [], [], [], [], []\n",
    "\n",
    "            for imp in range(1, imputations + 1):\n",
    "                file_path = os.path.join(group_dir, f\"trimmed_data_imp{imp}.pkl\")\n",
    "                if not os.path.exists(file_path):\n",
    "                    continue\n",
    "\n",
    "                df = pd.read_pickle(file_path)\n",
    "                if group not in df.columns or \"iptw\" not in df.columns:\n",
    "                    continue\n",
    "\n",
    "                # Add bootstrap sampling with seed-based randomization\n",
    "                n_samples = len(df)\n",
    "                bootstrap_idx = np.random.choice(n_samples, size=n_samples, replace=True)\n",
    "                df_bootstrap = df.iloc[bootstrap_idx].reset_index(drop=True)\n",
    "\n",
    "                X = df_bootstrap[covariates].copy()\n",
    "                T = df_bootstrap[group]\n",
    "                Y = df_bootstrap[\"caps5_change_baseline\"]\n",
    "                W = df_bootstrap[\"iptw\"]\n",
    "\n",
    "                try:\n",
    "                    # Create design matrix with treatment variable and covariates\n",
    "                    X_ols = pd.concat([T, X], axis=1)\n",
    "                    X_ols = sm.add_constant(X_ols)\n",
    "                    \n",
    "                    # Fit weighted OLS with robust standard errors\n",
    "                    ols_model = sm.WLS(Y, X_ols, weights=W).fit(cov_type='HC1')\n",
    "                    \n",
    "                    # Extract treatment effect (coefficient of treatment variable)\n",
    "                    att = ols_model.params[group]  # Treatment coefficient\n",
    "                    se = ols_model.bse[group]  # Robust standard error for treatment\n",
    "                    \n",
    "                    att_list.append(att)\n",
    "                    se_list.append(se)\n",
    "\n",
    "                    # Calculate model fit statistics\n",
    "                    Y_pred = ols_model.fittedvalues\n",
    "                    residuals = ols_model.resid\n",
    "                    rmse = mean_squared_error(Y, Y_pred, squared=False)\n",
    "                    r2 = ols_model.rsquared\n",
    "                    r2_list.append(r2)\n",
    "                    rmse_list.append(rmse)\n",
    "                    \n",
    "                    # Collect residuals and fitted values for diagnostic plots\n",
    "                    group_residuals.append(residuals.values)\n",
    "                    group_fitted.append(Y_pred.values)\n",
    "\n",
    "                    smd, vr = calculate_smd_vr(X, T, W)\n",
    "                    smd_list.append(smd)\n",
    "                    vr_list.append(vr)\n",
    "                except Exception as e:\n",
    "                    print(f\"⚠️ Error in {group}, seed {seed}, imp {imp}: {e}\")\n",
    "\n",
    "            if att_list:\n",
    "                att, se, ci_l, ci_u, p_val = rubins_pool(att_list, se_list)\n",
    "                avg_r2 = np.mean(r2_list)\n",
    "                avg_rmse = np.mean(rmse_list)\n",
    "                avg_smd = np.mean(smd_list)\n",
    "                avg_vr = np.mean(vr_list)\n",
    "\n",
    "                balance_results.append({\n",
    "                    \"group\": group, \"seed\": seed, \"smd\": avg_smd, \"vr\": avg_vr\n",
    "                })\n",
    "\n",
    "                if se < best_se:\n",
    "                    best_se = se\n",
    "                    best_result = {\n",
    "                        \"group\": group, \"seed\": seed, \"att\": att, \"se\": se,\n",
    "                        \"ci_lower\": ci_l, \"ci_upper\": ci_u, \"p_value\": p_val,\n",
    "                        \"r2\": avg_r2, \"rmse\": avg_rmse\n",
    "                    }\n",
    "\n",
    "                print(f\"✅ {group} | Seed {seed}: ATT = {att:.4f}, SE = {se:.4f}, p = {p_val}\")\n",
    "            else:\n",
    "                print(f\"⚠️ No valid results for {group} | Seed {seed}\")\n",
    "\n",
    "        # Create diagnostic plots for this group\n",
    "        if group_residuals and group_fitted:\n",
    "            create_diagnostic_plots(group_residuals, group_fitted, group)\n",
    "\n",
    "        if best_result:\n",
    "            att_results.append(best_result)\n",
    "            print(f\"🏆 Best result for {group} → Seed {best_result['seed']} | SE = {best_result['se']:.4f}\")\n",
    "\n",
    "    # Save final output\n",
    "    pd.DataFrame(att_results).to_excel(\"ols_summary_subcats.xlsx\", index=False)\n",
    "    pd.DataFrame(balance_results).to_excel(\"smd_vr_summary_subcats.xlsx\", index=False)\n",
    "    print(\"\\n🎯 All summary files saved.\")\n",
    "\n",
    "run_dml_with_trimmed_data(final_covariates_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "77b5527d-29d9-44b7-8752-8a6b09a1f2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unweighted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7516480b-1b75-46c0-ba11-ddb32e50c668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🚀 Running OLS for SUBCAT_Antipsychotica_atypisch\n",
      "✅ SUBCAT_Antipsychotica_atypisch | Seed 1: ATT = 7.6283, SE = 7.2945, p = 0.35470\n",
      "✅ SUBCAT_Antipsychotica_atypisch | Seed 2: ATT = 6.2406, SE = 7.8969, p = 0.47359\n",
      "✅ SUBCAT_Antipsychotica_atypisch | Seed 3: ATT = 5.4737, SE = 5.9511, p = 0.40974\n",
      "✅ SUBCAT_Antipsychotica_atypisch | Seed 4: ATT = 6.9820, SE = 5.8724, p = 0.30021\n",
      "✅ SUBCAT_Antipsychotica_atypisch | Seed 5: ATT = 0.9987, SE = 5.7341, p = 0.87019\n",
      "✅ SUBCAT_Antipsychotica_atypisch | Seed 6: ATT = 5.6674, SE = 6.2234, p = 0.41399\n",
      "✅ SUBCAT_Antipsychotica_atypisch | Seed 7: ATT = 4.7487, SE = 7.0243, p = 0.53609\n",
      "✅ SUBCAT_Antipsychotica_atypisch | Seed 8: ATT = 5.9543, SE = 6.7890, p = 0.42998\n",
      "✅ SUBCAT_Antipsychotica_atypisch | Seed 9: ATT = 2.0668, SE = 7.6258, p = 0.79978\n",
      "✅ SUBCAT_Antipsychotica_atypisch | Seed 10: ATT = 7.5569, SE = 7.5702, p = 0.37465\n",
      "📊 Diagnostic plots saved for SUBCAT_Antipsychotica_atypisch\n",
      "🏆 Best result for SUBCAT_Antipsychotica_atypisch → Seed 5 | SE = 5.7341\n",
      "\n",
      "🚀 Running OLS for SUBCAT_TCA\n",
      "✅ SUBCAT_TCA | Seed 1: ATT = 3.5008, SE = 3.8384, p = 0.41334\n",
      "✅ SUBCAT_TCA | Seed 2: ATT = 1.0315, SE = 4.6579, p = 0.83559\n",
      "✅ SUBCAT_TCA | Seed 3: ATT = 2.4096, SE = 4.4870, p = 0.61974\n",
      "✅ SUBCAT_TCA | Seed 4: ATT = 3.6201, SE = 3.5325, p = 0.36339\n",
      "✅ SUBCAT_TCA | Seed 5: ATT = 1.3086, SE = 4.0075, p = 0.76039\n",
      "✅ SUBCAT_TCA | Seed 6: ATT = 4.6275, SE = 4.7047, p = 0.38101\n",
      "✅ SUBCAT_TCA | Seed 7: ATT = 1.6128, SE = 3.7233, p = 0.68723\n",
      "✅ SUBCAT_TCA | Seed 8: ATT = 2.2475, SE = 4.0196, p = 0.60590\n",
      "✅ SUBCAT_TCA | Seed 9: ATT = 2.3956, SE = 3.9989, p = 0.58141\n",
      "✅ SUBCAT_TCA | Seed 10: ATT = 2.6777, SE = 4.2406, p = 0.56204\n",
      "📊 Diagnostic plots saved for SUBCAT_TCA\n",
      "🏆 Best result for SUBCAT_TCA → Seed 4 | SE = 3.5325\n",
      "\n",
      "🚀 Running OLS for SUBCAT_SSRI\n",
      "✅ SUBCAT_SSRI | Seed 1: ATT = 0.2266, SE = 0.8700, p = 0.80741\n",
      "✅ SUBCAT_SSRI | Seed 2: ATT = 0.0104, SE = 0.7093, p = 0.98897\n",
      "✅ SUBCAT_SSRI | Seed 3: ATT = 0.5628, SE = 1.3431, p = 0.69673\n",
      "✅ SUBCAT_SSRI | Seed 4: ATT = 0.7087, SE = 1.0874, p = 0.55011\n",
      "✅ SUBCAT_SSRI | Seed 5: ATT = 0.5438, SE = 0.9714, p = 0.60549\n",
      "✅ SUBCAT_SSRI | Seed 6: ATT = -0.1075, SE = 1.0037, p = 0.91987\n",
      "✅ SUBCAT_SSRI | Seed 7: ATT = 0.3831, SE = 1.1795, p = 0.76161\n",
      "✅ SUBCAT_SSRI | Seed 8: ATT = 0.3077, SE = 0.8586, p = 0.73814\n",
      "✅ SUBCAT_SSRI | Seed 9: ATT = 0.0682, SE = 0.9098, p = 0.94384\n",
      "✅ SUBCAT_SSRI | Seed 10: ATT = -0.2268, SE = 1.0254, p = 0.83581\n",
      "📊 Diagnostic plots saved for SUBCAT_SSRI\n",
      "🏆 Best result for SUBCAT_SSRI → Seed 2 | SE = 0.7093\n",
      "\n",
      "🚀 Running OLS for SUBCAT_SNRI\n",
      "✅ SUBCAT_SNRI | Seed 1: ATT = 2.7995, SE = 4.2258, p = 0.54388\n",
      "✅ SUBCAT_SNRI | Seed 2: ATT = 5.4533, SE = 4.9283, p = 0.33056\n",
      "✅ SUBCAT_SNRI | Seed 3: ATT = 4.5380, SE = 3.4361, p = 0.25710\n",
      "✅ SUBCAT_SNRI | Seed 4: ATT = 3.8399, SE = 6.8635, p = 0.60569\n",
      "✅ SUBCAT_SNRI | Seed 5: ATT = 4.4168, SE = 4.0675, p = 0.33860\n",
      "✅ SUBCAT_SNRI | Seed 6: ATT = 3.6119, SE = 4.7260, p = 0.48731\n",
      "✅ SUBCAT_SNRI | Seed 7: ATT = 4.2935, SE = 4.4511, p = 0.38937\n",
      "✅ SUBCAT_SNRI | Seed 8: ATT = 5.7795, SE = 5.1564, p = 0.32510\n",
      "✅ SUBCAT_SNRI | Seed 9: ATT = 3.1401, SE = 5.3123, p = 0.58624\n",
      "✅ SUBCAT_SNRI | Seed 10: ATT = 1.3595, SE = 6.0125, p = 0.83220\n",
      "📊 Diagnostic plots saved for SUBCAT_SNRI\n",
      "🏆 Best result for SUBCAT_SNRI → Seed 3 | SE = 3.4361\n",
      "\n",
      "🚀 Running OLS for SUBCAT_Tetracyclische_antidepressiva\n",
      "✅ SUBCAT_Tetracyclische_antidepressiva | Seed 1: ATT = 4.8656, SE = 4.8754, p = 0.37476\n",
      "✅ SUBCAT_Tetracyclische_antidepressiva | Seed 2: ATT = 5.7893, SE = 4.1707, p = 0.23742\n",
      "✅ SUBCAT_Tetracyclische_antidepressiva | Seed 3: ATT = 2.0023, SE = 4.0070, p = 0.64353\n",
      "✅ SUBCAT_Tetracyclische_antidepressiva | Seed 4: ATT = 1.6794, SE = 5.0707, p = 0.75712\n",
      "✅ SUBCAT_Tetracyclische_antidepressiva | Seed 5: ATT = 4.9870, SE = 4.5397, p = 0.33365\n",
      "✅ SUBCAT_Tetracyclische_antidepressiva | Seed 6: ATT = 7.1505, SE = 4.9573, p = 0.22265\n",
      "✅ SUBCAT_Tetracyclische_antidepressiva | Seed 7: ATT = 9.6036, SE = 5.5891, p = 0.16088\n",
      "✅ SUBCAT_Tetracyclische_antidepressiva | Seed 8: ATT = 3.6324, SE = 4.4804, p = 0.46300\n",
      "✅ SUBCAT_Tetracyclische_antidepressiva | Seed 9: ATT = 3.2649, SE = 4.6087, p = 0.51778\n",
      "✅ SUBCAT_Tetracyclische_antidepressiva | Seed 10: ATT = 5.6007, SE = 3.7011, p = 0.20477\n",
      "📊 Diagnostic plots saved for SUBCAT_Tetracyclische_antidepressiva\n",
      "🏆 Best result for SUBCAT_Tetracyclische_antidepressiva → Seed 10 | SE = 3.7011\n",
      "\n",
      "🚀 Running OLS for SUBCAT_Antidepressiva_overige\n",
      "✅ SUBCAT_Antidepressiva_overige | Seed 1: ATT = 6.6073, SE = 7.6958, p = 0.43899\n",
      "✅ SUBCAT_Antidepressiva_overige | Seed 2: ATT = 12.5864, SE = 11.9107, p = 0.35023\n",
      "✅ SUBCAT_Antidepressiva_overige | Seed 3: ATT = 6.7177, SE = 8.1564, p = 0.45643\n",
      "✅ SUBCAT_Antidepressiva_overige | Seed 4: ATT = 4.0103, SE = 6.9895, p = 0.59684\n",
      "✅ SUBCAT_Antidepressiva_overige | Seed 5: ATT = 6.5980, SE = 8.7779, p = 0.49406\n",
      "✅ SUBCAT_Antidepressiva_overige | Seed 6: ATT = 2.8592, SE = 15.7185, p = 0.86451\n",
      "✅ SUBCAT_Antidepressiva_overige | Seed 7: ATT = 8.3452, SE = 10.1035, p = 0.45523\n",
      "✅ SUBCAT_Antidepressiva_overige | Seed 8: ATT = 7.4491, SE = 10.2770, p = 0.50869\n",
      "✅ SUBCAT_Antidepressiva_overige | Seed 9: ATT = 5.8398, SE = 9.1431, p = 0.55775\n",
      "✅ SUBCAT_Antidepressiva_overige | Seed 10: ATT = 3.8569, SE = 8.1294, p = 0.65993\n",
      "📊 Diagnostic plots saved for SUBCAT_Antidepressiva_overige\n",
      "🏆 Best result for SUBCAT_Antidepressiva_overige → Seed 4 | SE = 6.9895\n",
      "\n",
      "🚀 Running OLS for SUBCAT_Systemische_antihistaminica\n",
      "✅ SUBCAT_Systemische_antihistaminica | Seed 1: ATT = 0.1634, SE = 4.6341, p = 0.97357\n",
      "✅ SUBCAT_Systemische_antihistaminica | Seed 2: ATT = -0.8444, SE = 4.4890, p = 0.85996\n",
      "✅ SUBCAT_Systemische_antihistaminica | Seed 3: ATT = 1.3364, SE = 9.4058, p = 0.89389\n",
      "✅ SUBCAT_Systemische_antihistaminica | Seed 4: ATT = -0.9360, SE = 3.5355, p = 0.80428\n",
      "✅ SUBCAT_Systemische_antihistaminica | Seed 5: ATT = 1.5616, SE = 5.1870, p = 0.77837\n",
      "✅ SUBCAT_Systemische_antihistaminica | Seed 6: ATT = -2.5166, SE = 4.9418, p = 0.63738\n",
      "✅ SUBCAT_Systemische_antihistaminica | Seed 7: ATT = 2.5423, SE = 7.0848, p = 0.73785\n",
      "✅ SUBCAT_Systemische_antihistaminica | Seed 8: ATT = 0.0059, SE = 5.6190, p = 0.99921\n",
      "✅ SUBCAT_Systemische_antihistaminica | Seed 9: ATT = -1.2566, SE = 4.2816, p = 0.78375\n",
      "✅ SUBCAT_Systemische_antihistaminica | Seed 10: ATT = 1.2716, SE = 4.6039, p = 0.79607\n",
      "📊 Diagnostic plots saved for SUBCAT_Systemische_antihistaminica\n",
      "🏆 Best result for SUBCAT_Systemische_antihistaminica → Seed 4 | SE = 3.5355\n",
      "\n",
      "🚀 Running OLS for SUBCAT_anxiolytica_Benzodiazepine\n",
      "✅ SUBCAT_anxiolytica_Benzodiazepine | Seed 1: ATT = 0.3022, SE = 0.8938, p = 0.75233\n",
      "✅ SUBCAT_anxiolytica_Benzodiazepine | Seed 2: ATT = 1.0215, SE = 1.2040, p = 0.44401\n",
      "✅ SUBCAT_anxiolytica_Benzodiazepine | Seed 3: ATT = 0.7790, SE = 1.3960, p = 0.60656\n",
      "✅ SUBCAT_anxiolytica_Benzodiazepine | Seed 4: ATT = 0.6963, SE = 1.0062, p = 0.52701\n",
      "✅ SUBCAT_anxiolytica_Benzodiazepine | Seed 5: ATT = 0.4693, SE = 1.5305, p = 0.77442\n",
      "✅ SUBCAT_anxiolytica_Benzodiazepine | Seed 6: ATT = 0.8681, SE = 1.6503, p = 0.62670\n",
      "✅ SUBCAT_anxiolytica_Benzodiazepine | Seed 7: ATT = 1.0006, SE = 1.4204, p = 0.52001\n",
      "✅ SUBCAT_anxiolytica_Benzodiazepine | Seed 8: ATT = 0.6208, SE = 1.1849, p = 0.62803\n",
      "✅ SUBCAT_anxiolytica_Benzodiazepine | Seed 9: ATT = 0.0039, SE = 1.8723, p = 0.99846\n",
      "✅ SUBCAT_anxiolytica_Benzodiazepine | Seed 10: ATT = 0.9916, SE = 1.1608, p = 0.44111\n",
      "📊 Diagnostic plots saved for SUBCAT_anxiolytica_Benzodiazepine\n",
      "🏆 Best result for SUBCAT_anxiolytica_Benzodiazepine → Seed 1 | SE = 0.8938\n",
      "\n",
      "🚀 Running OLS for SUBCAT_hypnotica_Benzodiazepine\n",
      "✅ SUBCAT_hypnotica_Benzodiazepine | Seed 1: ATT = 0.0513, SE = 2.3020, p = 0.98328\n",
      "✅ SUBCAT_hypnotica_Benzodiazepine | Seed 2: ATT = 0.9657, SE = 3.3709, p = 0.78873\n",
      "✅ SUBCAT_hypnotica_Benzodiazepine | Seed 3: ATT = -0.8975, SE = 1.8437, p = 0.65187\n",
      "✅ SUBCAT_hypnotica_Benzodiazepine | Seed 4: ATT = 1.7612, SE = 2.3417, p = 0.49382\n",
      "✅ SUBCAT_hypnotica_Benzodiazepine | Seed 5: ATT = 0.7451, SE = 2.4570, p = 0.77681\n",
      "✅ SUBCAT_hypnotica_Benzodiazepine | Seed 6: ATT = -0.2672, SE = 2.4388, p = 0.91802\n",
      "✅ SUBCAT_hypnotica_Benzodiazepine | Seed 7: ATT = 0.4319, SE = 2.0729, p = 0.84513\n",
      "✅ SUBCAT_hypnotica_Benzodiazepine | Seed 8: ATT = -0.0175, SE = 1.8449, p = 0.99287\n",
      "✅ SUBCAT_hypnotica_Benzodiazepine | Seed 9: ATT = 1.3181, SE = 1.8993, p = 0.52590\n",
      "✅ SUBCAT_hypnotica_Benzodiazepine | Seed 10: ATT = 0.8178, SE = 1.7167, p = 0.65865\n",
      "📊 Diagnostic plots saved for SUBCAT_hypnotica_Benzodiazepine\n",
      "🏆 Best result for SUBCAT_hypnotica_Benzodiazepine → Seed 10 | SE = 1.7167\n",
      "\n",
      "🚀 Running OLS for SUBCAT_Amfetaminen\n",
      "✅ SUBCAT_Amfetaminen | Seed 1: ATT = -1.7601, SE = 7.4130, p = 0.82398\n",
      "✅ SUBCAT_Amfetaminen | Seed 2: ATT = -1.1191, SE = 9.4832, p = 0.91175\n",
      "✅ SUBCAT_Amfetaminen | Seed 3: ATT = -1.3905, SE = 6.1996, p = 0.83353\n",
      "✅ SUBCAT_Amfetaminen | Seed 4: ATT = 2.6717, SE = 7.0669, p = 0.72459\n",
      "✅ SUBCAT_Amfetaminen | Seed 5: ATT = -0.0870, SE = 6.4891, p = 0.98995\n",
      "✅ SUBCAT_Amfetaminen | Seed 6: ATT = 1.9934, SE = 10.6821, p = 0.86105\n",
      "✅ SUBCAT_Amfetaminen | Seed 7: ATT = -3.8213, SE = 6.1313, p = 0.56691\n",
      "✅ SUBCAT_Amfetaminen | Seed 8: ATT = 0.2273, SE = 8.4286, p = 0.97978\n",
      "✅ SUBCAT_Amfetaminen | Seed 9: ATT = -1.7217, SE = 9.8287, p = 0.86945\n",
      "✅ SUBCAT_Amfetaminen | Seed 10: ATT = 3.2301, SE = 9.5450, p = 0.75207\n",
      "📊 Diagnostic plots saved for SUBCAT_Amfetaminen\n",
      "🏆 Best result for SUBCAT_Amfetaminen → Seed 7 | SE = 6.1313\n",
      "\n",
      "🚀 Running OLS for SUBCAT_Systemische_betablokkers\n",
      "✅ SUBCAT_Systemische_betablokkers | Seed 1: ATT = 10.0957, SE = 18.8690, p = 0.62098\n",
      "✅ SUBCAT_Systemische_betablokkers | Seed 2: ATT = 9.8838, SE = 25.0658, p = 0.71347\n",
      "✅ SUBCAT_Systemische_betablokkers | Seed 3: ATT = 20.9645, SE = 9.1958, p = 0.08481\n",
      "✅ SUBCAT_Systemische_betablokkers | Seed 4: ATT = 17.8452, SE = 16.5856, p = 0.34252\n",
      "✅ SUBCAT_Systemische_betablokkers | Seed 5: ATT = 12.6560, SE = 11.5693, p = 0.33544\n",
      "✅ SUBCAT_Systemische_betablokkers | Seed 6: ATT = 5.1539, SE = 14.4630, p = 0.73958\n",
      "✅ SUBCAT_Systemische_betablokkers | Seed 7: ATT = 8.9524, SE = 24.7529, p = 0.73589\n",
      "✅ SUBCAT_Systemische_betablokkers | Seed 8: ATT = 3.3702, SE = 18.2116, p = 0.86219\n",
      "✅ SUBCAT_Systemische_betablokkers | Seed 9: ATT = 21.5931, SE = 16.8622, p = 0.26956\n",
      "✅ SUBCAT_Systemische_betablokkers | Seed 10: ATT = 4.1496, SE = 23.9025, p = 0.87061\n",
      "📊 Diagnostic plots saved for SUBCAT_Systemische_betablokkers\n",
      "🏆 Best result for SUBCAT_Systemische_betablokkers → Seed 3 | SE = 9.1958\n",
      "\n",
      "🚀 Running OLS for SUBCAT_Paracetamol_mono\n",
      "✅ SUBCAT_Paracetamol_mono | Seed 1: ATT = -6.4841, SE = 9.6044, p = 0.53661\n",
      "✅ SUBCAT_Paracetamol_mono | Seed 2: ATT = 3.0777, SE = 8.5234, p = 0.73630\n",
      "✅ SUBCAT_Paracetamol_mono | Seed 3: ATT = -9.0621, SE = 9.4936, p = 0.39386\n",
      "✅ SUBCAT_Paracetamol_mono | Seed 4: ATT = 1.9651, SE = 8.8039, p = 0.83431\n",
      "✅ SUBCAT_Paracetamol_mono | Seed 5: ATT = -0.6502, SE = 9.2971, p = 0.94760\n",
      "✅ SUBCAT_Paracetamol_mono | Seed 6: ATT = 5.9992, SE = 10.2139, p = 0.58852\n",
      "✅ SUBCAT_Paracetamol_mono | Seed 7: ATT = 3.6775, SE = 16.5181, p = 0.83472\n",
      "✅ SUBCAT_Paracetamol_mono | Seed 8: ATT = -4.2707, SE = 8.9665, p = 0.65871\n",
      "✅ SUBCAT_Paracetamol_mono | Seed 9: ATT = 1.3775, SE = 18.5433, p = 0.94435\n",
      "✅ SUBCAT_Paracetamol_mono | Seed 10: ATT = -3.7149, SE = 8.5687, p = 0.68698\n",
      "📊 Diagnostic plots saved for SUBCAT_Paracetamol_mono\n",
      "🏆 Best result for SUBCAT_Paracetamol_mono → Seed 2 | SE = 8.5234\n",
      "\n",
      "🚀 Running OLS for SUBCAT_Anti_epileptica_stemmingsstabilisatoren\n",
      "✅ SUBCAT_Anti_epileptica_stemmingsstabilisatoren | Seed 1: ATT = 3.9811, SE = 5.8325, p = 0.53236\n",
      "✅ SUBCAT_Anti_epileptica_stemmingsstabilisatoren | Seed 2: ATT = 2.2192, SE = 5.5956, p = 0.71191\n",
      "✅ SUBCAT_Anti_epileptica_stemmingsstabilisatoren | Seed 3: ATT = 3.3152, SE = 4.4139, p = 0.49438\n",
      "✅ SUBCAT_Anti_epileptica_stemmingsstabilisatoren | Seed 4: ATT = 3.6639, SE = 4.4560, p = 0.45713\n",
      "✅ SUBCAT_Anti_epileptica_stemmingsstabilisatoren | Seed 5: ATT = 1.0958, SE = 4.0143, p = 0.79839\n",
      "✅ SUBCAT_Anti_epileptica_stemmingsstabilisatoren | Seed 6: ATT = 1.3524, SE = 4.4864, p = 0.77809\n",
      "✅ SUBCAT_Anti_epileptica_stemmingsstabilisatoren | Seed 7: ATT = 3.6581, SE = 2.9862, p = 0.28778\n",
      "✅ SUBCAT_Anti_epileptica_stemmingsstabilisatoren | Seed 8: ATT = 3.8749, SE = 5.4932, p = 0.51948\n",
      "✅ SUBCAT_Anti_epileptica_stemmingsstabilisatoren | Seed 9: ATT = 4.5634, SE = 3.4710, p = 0.25891\n",
      "✅ SUBCAT_Anti_epileptica_stemmingsstabilisatoren | Seed 10: ATT = 3.3332, SE = 4.1869, p = 0.47055\n",
      "📊 Diagnostic plots saved for SUBCAT_Anti_epileptica_stemmingsstabilisatoren\n",
      "🏆 Best result for SUBCAT_Anti_epileptica_stemmingsstabilisatoren → Seed 7 | SE = 2.9862\n",
      "\n",
      "🚀 Running OLS for SUBCAT_Opioden\n",
      "✅ SUBCAT_Opioden | Seed 1: ATT = 0.9476, SE = 6.4058, p = 0.88956\n",
      "✅ SUBCAT_Opioden | Seed 2: ATT = -1.8448, SE = 6.5148, p = 0.79110\n",
      "✅ SUBCAT_Opioden | Seed 3: ATT = -3.0330, SE = 5.0038, p = 0.57714\n",
      "✅ SUBCAT_Opioden | Seed 4: ATT = -2.2549, SE = 7.6673, p = 0.78331\n",
      "✅ SUBCAT_Opioden | Seed 5: ATT = 1.0832, SE = 7.3481, p = 0.88994\n",
      "✅ SUBCAT_Opioden | Seed 6: ATT = -0.5315, SE = 7.7997, p = 0.94894\n",
      "✅ SUBCAT_Opioden | Seed 7: ATT = -2.5162, SE = 6.9425, p = 0.73537\n",
      "✅ SUBCAT_Opioden | Seed 8: ATT = -1.8451, SE = 7.0995, p = 0.80778\n",
      "✅ SUBCAT_Opioden | Seed 9: ATT = -0.8129, SE = 7.3236, p = 0.91696\n",
      "✅ SUBCAT_Opioden | Seed 10: ATT = 0.6687, SE = 5.3336, p = 0.90628\n",
      "📊 Diagnostic plots saved for SUBCAT_Opioden\n",
      "🏆 Best result for SUBCAT_Opioden → Seed 3 | SE = 5.0038\n",
      "\n",
      "🚀 Running OLS for SUBCAT_Z_drugs\n",
      "✅ SUBCAT_Z_drugs | Seed 1: ATT = 7.1463, SE = 3.8477, p = 0.13683\n",
      "✅ SUBCAT_Z_drugs | Seed 2: ATT = 7.8508, SE = 4.2754, p = 0.14020\n",
      "✅ SUBCAT_Z_drugs | Seed 3: ATT = 8.3281, SE = 4.9987, p = 0.17104\n",
      "✅ SUBCAT_Z_drugs | Seed 4: ATT = 8.4381, SE = 3.7183, p = 0.08579\n",
      "✅ SUBCAT_Z_drugs | Seed 5: ATT = 8.3415, SE = 4.1458, p = 0.11453\n",
      "✅ SUBCAT_Z_drugs | Seed 6: ATT = 8.3049, SE = 4.1990, p = 0.11910\n",
      "✅ SUBCAT_Z_drugs | Seed 7: ATT = 7.3644, SE = 5.2656, p = 0.23449\n",
      "✅ SUBCAT_Z_drugs | Seed 8: ATT = 4.3741, SE = 7.0219, p = 0.56709\n",
      "✅ SUBCAT_Z_drugs | Seed 9: ATT = 7.5993, SE = 4.2499, p = 0.14828\n",
      "✅ SUBCAT_Z_drugs | Seed 10: ATT = 9.7188, SE = 4.5065, p = 0.09725\n",
      "📊 Diagnostic plots saved for SUBCAT_Z_drugs\n",
      "🏆 Best result for SUBCAT_Z_drugs → Seed 4 | SE = 3.7183\n",
      "\n",
      "🚀 Running OLS for SUBCAT_NSAIDs\n",
      "✅ SUBCAT_NSAIDs | Seed 1: ATT = -4.4263, SE = 12.1924, p = 0.73495\n",
      "✅ SUBCAT_NSAIDs | Seed 2: ATT = 0.8085, SE = 20.6715, p = 0.97067\n",
      "✅ SUBCAT_NSAIDs | Seed 3: ATT = -6.8272, SE = 6.7974, p = 0.37202\n",
      "✅ SUBCAT_NSAIDs | Seed 4: ATT = -3.9193, SE = 10.6483, p = 0.73147\n",
      "✅ SUBCAT_NSAIDs | Seed 5: ATT = -8.0921, SE = 6.6972, p = 0.29348\n",
      "✅ SUBCAT_NSAIDs | Seed 6: ATT = -8.1807, SE = 7.8693, p = 0.35725\n",
      "✅ SUBCAT_NSAIDs | Seed 7: ATT = -2.2017, SE = 11.6716, p = 0.85956\n",
      "✅ SUBCAT_NSAIDs | Seed 8: ATT = -4.1016, SE = 9.9023, p = 0.69997\n",
      "✅ SUBCAT_NSAIDs | Seed 9: ATT = 0.8796, SE = 14.5802, p = 0.95479\n",
      "✅ SUBCAT_NSAIDs | Seed 10: ATT = -4.3702, SE = 11.7646, p = 0.72913\n",
      "📊 Diagnostic plots saved for SUBCAT_NSAIDs\n",
      "🏆 Best result for SUBCAT_NSAIDs → Seed 5 | SE = 6.6972\n",
      "\n",
      "🎯 All summary files saved.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import KFold\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import t, probplot\n",
    "import xgboost as xgb\n",
    "\n",
    "# -----------------------------\n",
    "# Configuration\n",
    "# -----------------------------\n",
    "seeds = list(range(1, 11))\n",
    "imputations = 5\n",
    "output_folder = \"outputs\"\n",
    "\n",
    "# -----------------------------\n",
    "# Diagnostic Plotting Function\n",
    "# -----------------------------\n",
    "def create_diagnostic_plots(residuals_data, fitted_data, group_name):\n",
    "    \"\"\"Create 4 diagnostic plots for model validation\"\"\"\n",
    "    plots_dir = os.path.join(output_folder, \"plots\")\n",
    "    os.makedirs(plots_dir, exist_ok=True)\n",
    "    \n",
    "    # Flatten the collected data\n",
    "    all_residuals = np.concatenate(residuals_data)\n",
    "    all_fitted = np.concatenate(fitted_data)\n",
    "    \n",
    "    # Create figure with 2x2 subplots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    fig.suptitle(f'Diagnostic Plots - {group_name}', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Residuals vs Fitted\n",
    "    axes[0,0].scatter(all_fitted, all_residuals, alpha=0.6, s=20)\n",
    "    axes[0,0].axhline(y=0, color='red', linestyle='--', alpha=0.8)\n",
    "    axes[0,0].set_xlabel('Fitted Values')\n",
    "    axes[0,0].set_ylabel('Residuals')\n",
    "    axes[0,0].set_title('Residuals vs Fitted')\n",
    "    axes[0,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. QQ Plot\n",
    "    probplot(all_residuals, dist=\"norm\", plot=axes[0,1])\n",
    "    axes[0,1].set_title('Q-Q Plot (Normal)')\n",
    "    axes[0,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Residual Histogram\n",
    "    axes[1,0].hist(all_residuals, bins=30, density=True, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    axes[1,0].set_xlabel('Residuals')\n",
    "    axes[1,0].set_ylabel('Density')\n",
    "    axes[1,0].set_title('Residual Distribution')\n",
    "    axes[1,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Scale-Location Plot\n",
    "    sqrt_abs_residuals = np.sqrt(np.abs(all_residuals))\n",
    "    axes[1,1].scatter(all_fitted, sqrt_abs_residuals, alpha=0.6, s=20)\n",
    "    axes[1,1].set_xlabel('Fitted Values')\n",
    "    axes[1,1].set_ylabel('√|Residuals|')\n",
    "    axes[1,1].set_title('Scale-Location Plot')\n",
    "    axes[1,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save plot\n",
    "    plot_filename = os.path.join(plots_dir, f'{group_name}_unweighted.png')\n",
    "    plt.savefig(plot_filename, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"📊 Diagnostic plots saved for {group_name}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Rubin's Rule\n",
    "# -----------------------------\n",
    "def rubins_pool(estimates, ses):\n",
    "    m = len(estimates)\n",
    "    q_bar = np.mean(estimates)\n",
    "    u_bar = np.mean(np.square(ses))\n",
    "    b_m = np.var(estimates, ddof=1)\n",
    "    total_var = u_bar + ((1 + 1/m) * b_m)\n",
    "    total_se = np.sqrt(total_var)\n",
    "    ci_lower = q_bar - 1.96 * total_se\n",
    "    ci_upper = q_bar + 1.96 * total_se\n",
    "    p_value = 2 * (1 - t.cdf(np.abs(q_bar / total_se), df=m-1))\n",
    "    rounded_p = round(p_value, 5)\n",
    "    formatted_p = \"< 0.00001\" if rounded_p <= 0.00001 else f\"{rounded_p:.5f}\"\n",
    "    return q_bar, total_se, ci_lower, ci_upper, formatted_p\n",
    "\n",
    "# -----------------------------\n",
    "# SMD + Variance Ratio\n",
    "# -----------------------------\n",
    "def calculate_smd_vr(X, T):\n",
    "    treated = X[T == 1]\n",
    "    control = X[T == 0]\n",
    "    smd, vr = [], []\n",
    "    for col in X.columns:\n",
    "        try:\n",
    "            m1, m0 = treated[col].mean(), control[col].mean()\n",
    "            s1 = treated[col].std()\n",
    "            s0 = control[col].std()\n",
    "            pooled_sd = np.sqrt((s1 ** 2 + s0 ** 2) / 2)\n",
    "            smd.append((m1 - m0) / pooled_sd if pooled_sd > 0 else 0)\n",
    "            vr.append(s1**2 / s0**2 if s0**2 > 0 else 0)\n",
    "        except Exception:\n",
    "            smd.append(np.nan)\n",
    "            vr.append(np.nan)\n",
    "    return np.nanmean(smd), np.nanmean(vr)\n",
    "\n",
    "# -----------------------------\n",
    "# OLS Main Loop\n",
    "# -----------------------------\n",
    "def run_dml_with_trimmed_data(final_covariates_map):\n",
    "    att_results = []\n",
    "    balance_results = []\n",
    "\n",
    "    for group, covariates in final_covariates_map.items():\n",
    "        print(f\"\\n🚀 Running OLS for {group}\")\n",
    "        group_dir = os.path.join(output_folder, group)\n",
    "        os.makedirs(group_dir, exist_ok=True)\n",
    "\n",
    "        best_result = None\n",
    "        best_se = float(\"inf\")\n",
    "        \n",
    "        # Initialize lists to collect residuals and fitted values for diagnostic plots\n",
    "        group_residuals = []\n",
    "        group_fitted = []\n",
    "\n",
    "        for seed in seeds:\n",
    "            # Set random seed for this iteration\n",
    "            np.random.seed(seed)\n",
    "            \n",
    "            att_list, se_list, r2_list, rmse_list, smd_list, vr_list = [], [], [], [], [], []\n",
    "\n",
    "            for imp in range(1, imputations + 1):\n",
    "                file_path = os.path.join(group_dir, f\"trimmed_data_imp{imp}.pkl\")\n",
    "                if not os.path.exists(file_path):\n",
    "                    continue\n",
    "\n",
    "                df = pd.read_pickle(file_path)\n",
    "                if group not in df.columns or \"iptw\" not in df.columns:\n",
    "                    continue\n",
    "\n",
    "                # Add bootstrap sampling with seed-based randomization\n",
    "                n_samples = len(df)\n",
    "                bootstrap_idx = np.random.choice(n_samples, size=n_samples, replace=True)\n",
    "                df_bootstrap = df.iloc[bootstrap_idx].reset_index(drop=True)\n",
    "\n",
    "                X = df_bootstrap[covariates].copy()\n",
    "                T = df_bootstrap[group]\n",
    "                Y = df_bootstrap[\"caps5_change_baseline\"]\n",
    "                #W = df_bootstrap[\"iptw\"]\n",
    "\n",
    "                try:\n",
    "                    # Create design matrix with treatment variable and covariates\n",
    "                    X_ols = pd.concat([T, X], axis=1)\n",
    "                    X_ols = sm.add_constant(X_ols)\n",
    "                    \n",
    "                    # Fit OLS with robust standard errors (unweighted)\n",
    "                    ols_model = sm.OLS(Y, X_ols).fit(cov_type='HC1')\n",
    "                    \n",
    "                    # Extract treatment effect (coefficient of treatment variable)\n",
    "                    att = ols_model.params[group]  # Treatment coefficient\n",
    "                    se = ols_model.bse[group]  # Robust standard error for treatment\n",
    "                    \n",
    "                    att_list.append(att)\n",
    "                    se_list.append(se)\n",
    "\n",
    "                    # Calculate model fit statistics\n",
    "                    Y_pred = ols_model.fittedvalues\n",
    "                    residuals = ols_model.resid\n",
    "                    rmse = mean_squared_error(Y, Y_pred, squared=False)\n",
    "                    r2 = ols_model.rsquared\n",
    "                    r2_list.append(r2)\n",
    "                    rmse_list.append(rmse)\n",
    "                    \n",
    "                    # Collect residuals and fitted values for diagnostic plots\n",
    "                    group_residuals.append(residuals.values)\n",
    "                    group_fitted.append(Y_pred.values)\n",
    "\n",
    "                    smd, vr = calculate_smd_vr(X, T)\n",
    "                    smd_list.append(smd)\n",
    "                    vr_list.append(vr)\n",
    "                except Exception as e:\n",
    "                    print(f\"⚠️ Error in {group}, seed {seed}, imp {imp}: {e}\")\n",
    "\n",
    "            if att_list:\n",
    "                att, se, ci_l, ci_u, p_val = rubins_pool(att_list, se_list)\n",
    "                avg_r2 = np.mean(r2_list)\n",
    "                avg_rmse = np.mean(rmse_list)\n",
    "                avg_smd = np.mean(smd_list)\n",
    "                avg_vr = np.mean(vr_list)\n",
    "\n",
    "                balance_results.append({\n",
    "                    \"group\": group, \"seed\": seed, \"smd\": avg_smd, \"vr\": avg_vr\n",
    "                })\n",
    "\n",
    "                if se < best_se:\n",
    "                    best_se = se\n",
    "                    best_result = {\n",
    "                        \"group\": group, \"seed\": seed, \"att\": att, \"se\": se,\n",
    "                        \"ci_lower\": ci_l, \"ci_upper\": ci_u, \"p_value\": p_val,\n",
    "                        \"r2\": avg_r2, \"rmse\": avg_rmse\n",
    "                    }\n",
    "\n",
    "                print(f\"✅ {group} | Seed {seed}: ATT = {att:.4f}, SE = {se:.4f}, p = {p_val}\")\n",
    "            else:\n",
    "                print(f\"⚠️ No valid results for {group} | Seed {seed}\")\n",
    "\n",
    "        # Create diagnostic plots for this group\n",
    "        if group_residuals and group_fitted:\n",
    "            create_diagnostic_plots(group_residuals, group_fitted, group)\n",
    "\n",
    "        if best_result:\n",
    "            att_results.append(best_result)\n",
    "            print(f\"🏆 Best result for {group} → Seed {best_result['seed']} | SE = {best_result['se']:.4f}\")\n",
    "\n",
    "    # Save final output\n",
    "    pd.DataFrame(att_results).to_excel(\"ols_rubin_summary_subcats_unweighted.xlsx\", index=False)\n",
    "    pd.DataFrame(balance_results).to_excel(\"smd_vr_summary_subcats_unweighted.xlsx\", index=False)\n",
    "    print(\"\\n🎯 All summary files saved.\")\n",
    "\n",
    "run_dml_with_trimmed_data(final_covariates_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9e3e1778-8b6a-4db4-915c-c8fc1a7d4992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Final_ATT_Summary_SubCat saved\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import sem, ttest_ind\n",
    "\n",
    "# ----------------------------------\n",
    "# File paths\n",
    "# ----------------------------------\n",
    "output_base = \"outputs\"\n",
    "att_file = \"ols_summary_subcats.xlsx\"\n",
    "trimmed_file = \"trimmed_data_imp1.pkl\"\n",
    "auc_file = \"auc_scores.xlsx\"  \n",
    "\n",
    "# ----------------------------------\n",
    "# Load ATT Summary\n",
    "# ----------------------------------\n",
    "if os.path.exists(att_file):\n",
    "    att_df = pd.read_excel(att_file)\n",
    "else:\n",
    "    raise FileNotFoundError(\"❌ ATT summary file not found: ols_summary_subcats.xlsx\")\n",
    "\n",
    "summary_rows = []\n",
    "\n",
    "# ----------------------------------\n",
    "# Loop over medication groups\n",
    "# ----------------------------------\n",
    "groups = [g for g in medication_groups if os.path.isdir(os.path.join(output_base, g))]\n",
    "\n",
    "for med in groups:\n",
    "    try:\n",
    "        group_path = os.path.join(output_base, med)\n",
    "\n",
    "        # Load trimmed data\n",
    "        df = pd.read_pickle(os.path.join(group_path, trimmed_file))\n",
    "\n",
    "        # Detect treatment column\n",
    "        treatment_cols = [col for col in df.columns if col.upper() == med.upper()]\n",
    "        if not treatment_cols:\n",
    "            print(f\"⚠️ Treatment column {med} not found in trimmed data. Skipping.\")\n",
    "            continue\n",
    "        treatment_var = treatment_cols[0]\n",
    "\n",
    "        # Extract treatment and outcome\n",
    "        T = df[treatment_var]\n",
    "        Y = df[\"caps5_change_baseline\"]\n",
    "\n",
    "        # Treated and control stats\n",
    "        treated = Y[T == 1]\n",
    "        control = Y[T == 0]\n",
    "\n",
    "        mean_treat = treated.mean()\n",
    "        se_treat = sem(treated) if len(treated) > 1 else np.nan\n",
    "\n",
    "        mean_ctrl = control.mean()\n",
    "        se_ctrl = sem(control) if len(control) > 1 else np.nan\n",
    "\n",
    "        # Cohen's d (unadjusted)\n",
    "        pooled_sd = np.sqrt(((treated.std() ** 2) + (control.std() ** 2)) / 2)\n",
    "        cohen_d = (mean_treat - mean_ctrl) / pooled_sd if pooled_sd > 0 else np.nan\n",
    "\n",
    "        # E-value (unadjusted)\n",
    "        delta = mean_treat - mean_ctrl\n",
    "        E = delta / abs(mean_ctrl) * 100 if mean_ctrl != 0 else np.nan\n",
    "\n",
    "        # Unadjusted p-value\n",
    "        try:\n",
    "            t_stat, p_val = ttest_ind(treated, control, equal_var=False, nan_policy=\"omit\")\n",
    "            rounded_p = round(p_val, 5)\n",
    "            formatted_p = \"< 0.00001\" if rounded_p < 0.00001 else rounded_p\n",
    "        except Exception:\n",
    "            formatted_p = np.nan\n",
    "\n",
    "        # AUC from new auc_scores.xlsx file\n",
    "        auc_val = np.nan\n",
    "        auc_path = os.path.join(group_path, auc_file)\n",
    "        if os.path.exists(auc_path):\n",
    "            auc_df = pd.read_excel(auc_path)\n",
    "            if \"AUC\" in auc_df.columns:\n",
    "                auc_val = auc_df[\"AUC\"].dropna().mean()\n",
    "\n",
    "        # Adjusted stats from Rubin summary\n",
    "        att_row = att_df[att_df[\"group\"].str.strip().str.upper() == med.strip().upper()]\n",
    "        if not att_row.empty:\n",
    "            att = att_row.iloc[0][\"att\"]\n",
    "            att_se = att_row.iloc[0][\"se\"]\n",
    "            att_p_val = att_row.iloc[0][\"p_value\"]\n",
    "            r2 = att_row.iloc[0][\"r2\"]\n",
    "            rmse = att_row.iloc[0][\"rmse\"]\n",
    "\n",
    "            try:\n",
    "                rounded_att_p = round(float(att_p_val), 5)\n",
    "                formatted_att_p = \"< 0.00001\" if rounded_att_p < 0.00001 else rounded_att_p\n",
    "            except:\n",
    "                formatted_att_p = att_p_val\n",
    "        else:\n",
    "            att, att_se, formatted_att_p, r2, rmse = np.nan, np.nan, np.nan, np.nan, np.nan\n",
    "\n",
    "        # Append full row\n",
    "        summary_rows.append({\n",
    "            'Medication Group': med,\n",
    "            'Mean Treated': mean_treat,\n",
    "            'SE Treated': se_treat,\n",
    "            'Mean Control': mean_ctrl,\n",
    "            'SE Control': se_ctrl,\n",
    "            'Cohen d': cohen_d,\n",
    "            'E (Unadjusted)': E,\n",
    "            'n Treated': len(treated),\n",
    "            'n Control': len(control),\n",
    "            #'Unadjusted p-value': formatted_p,\n",
    "            'ATT Estimate': att,\n",
    "            'ATT SE (Robust)': att_se,\n",
    "            'ATT p-value': formatted_att_p,\n",
    "            'R²': r2,\n",
    "            'RMSE': rmse,\n",
    "            'AUC': auc_val\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error in {med}: {e}\")\n",
    "\n",
    "# ----------------------------------\n",
    "# Save final summary\n",
    "# ----------------------------------\n",
    "summary_df = pd.DataFrame(summary_rows)\n",
    "summary_df = summary_df.sort_values(\"Medication Group\")\n",
    "summary_df.to_excel(\"Final_ATT_Summary_SubCat.xlsx\", index=False)\n",
    "print(\"✅ Final_ATT_Summary_SubCat saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "89c085a6-dd36-4cfd-9501-5a4dc75e036a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ ols_att_barplot_subcat saved\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "# ✅ Load the final summary table\n",
    "final_df = pd.read_excel(\"Final_ATT_Summary_SubCat.xlsx\")\n",
    "\n",
    "# ✅ Parse DML p-values (handle \"< 0.00001\")\n",
    "def parse_pval(p):\n",
    "    try:\n",
    "        if isinstance(p, str) and \"<\" in p:\n",
    "            return 0.000001\n",
    "        return float(p)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "final_df['ATT p-value'] = final_df['ATT p-value'].apply(parse_pval)\n",
    "\n",
    "# ✅ Plot settings\n",
    "width = 0.35\n",
    "\n",
    "# ✅ Plotting function for a single medication group\n",
    "def plot_single_group(row):\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    bars1 = ax.bar(-width/2, row['Mean Control'], width, \n",
    "                   yerr=row['SE Control'], label='Control', hatch='//', color='gray', capsize=5)\n",
    "    bars2 = ax.bar(+width/2, row['Mean Treated'], width, \n",
    "                   yerr=row['SE Treated'], label='Treated', color='steelblue', capsize=5)\n",
    "\n",
    "    label = (\n",
    "        f\"ATT = {row['ATT Estimate']:.2f}\\n\"\n",
    "        f\"d = {row['Cohen d']:.2f}, p = {row['ATT p-value']:.3f}\\n\"\n",
    "        f\"nT = {row['n Treated']}, nC = {row['n Control']}\\n\"\n",
    "        f\"E = {row['E (Unadjusted)']:.1f}%\"\n",
    "    )\n",
    "    max_y = max(row['Mean Control'], row['Mean Treated']) + 1.5\n",
    "    ax.text(0, max_y, label, ha='center', va='bottom', fontsize=9, color='#FFD700')\n",
    "\n",
    "    ax.axhline(0, color='black', linewidth=0.8)\n",
    "    ax.set_xticks([-width/2, +width/2])\n",
    "    ax.set_xticklabels(['Control', 'Treated'])\n",
    "    ax.set_title(f\"Group: {row['Medication Group']}\", fontsize=12, weight='bold')\n",
    "    ax.set_ylabel(\"CAPS5 Change Score\")\n",
    "    ax.set_ylim(bottom=0, top=max_y + 2)\n",
    "    ax.legend()\n",
    "    fig.tight_layout()\n",
    "    return fig\n",
    "\n",
    "# ✅ Generate and save all plots into a multi-page PDF\n",
    "with PdfPages(\"ols_att_barplot_subcat.pdf\") as pdf:\n",
    "    for idx, row in final_df.iterrows():\n",
    "        fig = plot_single_group(row)\n",
    "        pdf.savefig(fig, dpi=300, bbox_inches='tight')\n",
    "        plt.close(fig)\n",
    "print(\"✅ ols_att_barplot_subcat saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8088e735-50ea-4250-913d-d259ae773864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Love plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f84dd49c-2e5e-4323-926a-03c6450de68c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Processing SUBCAT_Amfetaminen...\n",
      "📊 Exported numeric summary to: outputs\\SUBCAT_Amfetaminen\\covariate_balance_table_SUBCAT_Amfetaminen.xlsx\n",
      "✅ Saved love plot: outputs\\SUBCAT_Amfetaminen\\love_plot_SUBCAT_Amfetaminen.pdf\n",
      "📏 Max weighted SMD for SUBCAT_Amfetaminen: 0.635\n",
      "\n",
      "🔍 Processing SUBCAT_Antidepressiva_Overige...\n",
      "📊 Exported numeric summary to: outputs\\SUBCAT_Antidepressiva_Overige\\covariate_balance_table_SUBCAT_Antidepressiva_Overige.xlsx\n",
      "✅ Saved love plot: outputs\\SUBCAT_Antidepressiva_Overige\\love_plot_SUBCAT_Antidepressiva_Overige.pdf\n",
      "📏 Max weighted SMD for SUBCAT_Antidepressiva_Overige: 0.798\n",
      "\n",
      "🔍 Processing SUBCAT_Antipsychotica_Atypisch...\n",
      "📊 Exported numeric summary to: outputs\\SUBCAT_Antipsychotica_Atypisch\\covariate_balance_table_SUBCAT_Antipsychotica_Atypisch.xlsx\n",
      "✅ Saved love plot: outputs\\SUBCAT_Antipsychotica_Atypisch\\love_plot_SUBCAT_Antipsychotica_Atypisch.pdf\n",
      "📏 Max weighted SMD for SUBCAT_Antipsychotica_Atypisch: 0.592\n",
      "\n",
      "🔍 Processing SUBCAT_Anti_Epileptica_Stemmingsstabilisatoren...\n",
      "📊 Exported numeric summary to: outputs\\SUBCAT_Anti_Epileptica_Stemmingsstabilisatoren\\covariate_balance_table_SUBCAT_Anti_Epileptica_Stemmingsstabilisatoren.xlsx\n",
      "✅ Saved love plot: outputs\\SUBCAT_Anti_Epileptica_Stemmingsstabilisatoren\\love_plot_SUBCAT_Anti_Epileptica_Stemmingsstabilisatoren.pdf\n",
      "📏 Max weighted SMD for SUBCAT_Anti_Epileptica_Stemmingsstabilisatoren: 0.280\n",
      "\n",
      "🔍 Processing SUBCAT_Anxiolytica_Benzodiazepine...\n",
      "📊 Exported numeric summary to: outputs\\SUBCAT_Anxiolytica_Benzodiazepine\\covariate_balance_table_SUBCAT_Anxiolytica_Benzodiazepine.xlsx\n",
      "✅ Saved love plot: outputs\\SUBCAT_Anxiolytica_Benzodiazepine\\love_plot_SUBCAT_Anxiolytica_Benzodiazepine.pdf\n",
      "📏 Max weighted SMD for SUBCAT_Anxiolytica_Benzodiazepine: 0.136\n",
      "\n",
      "🔍 Processing SUBCAT_Hypnotica_Benzodiazepine...\n",
      "📊 Exported numeric summary to: outputs\\SUBCAT_Hypnotica_Benzodiazepine\\covariate_balance_table_SUBCAT_Hypnotica_Benzodiazepine.xlsx\n",
      "✅ Saved love plot: outputs\\SUBCAT_Hypnotica_Benzodiazepine\\love_plot_SUBCAT_Hypnotica_Benzodiazepine.pdf\n",
      "📏 Max weighted SMD for SUBCAT_Hypnotica_Benzodiazepine: 0.282\n",
      "\n",
      "🔍 Processing SUBCAT_Nsaids...\n",
      "📊 Exported numeric summary to: outputs\\SUBCAT_Nsaids\\covariate_balance_table_SUBCAT_Nsaids.xlsx\n",
      "✅ Saved love plot: outputs\\SUBCAT_Nsaids\\love_plot_SUBCAT_Nsaids.pdf\n",
      "📏 Max weighted SMD for SUBCAT_Nsaids: 0.725\n",
      "\n",
      "🔍 Processing SUBCAT_Opioden...\n",
      "📊 Exported numeric summary to: outputs\\SUBCAT_Opioden\\covariate_balance_table_SUBCAT_Opioden.xlsx\n",
      "✅ Saved love plot: outputs\\SUBCAT_Opioden\\love_plot_SUBCAT_Opioden.pdf\n",
      "📏 Max weighted SMD for SUBCAT_Opioden: 0.539\n",
      "\n",
      "🔍 Processing SUBCAT_Paracetamol_Mono...\n",
      "📊 Exported numeric summary to: outputs\\SUBCAT_Paracetamol_Mono\\covariate_balance_table_SUBCAT_Paracetamol_Mono.xlsx\n",
      "✅ Saved love plot: outputs\\SUBCAT_Paracetamol_Mono\\love_plot_SUBCAT_Paracetamol_Mono.pdf\n",
      "📏 Max weighted SMD for SUBCAT_Paracetamol_Mono: 0.615\n",
      "\n",
      "🔍 Processing SUBCAT_Snri...\n",
      "📊 Exported numeric summary to: outputs\\SUBCAT_Snri\\covariate_balance_table_SUBCAT_Snri.xlsx\n",
      "✅ Saved love plot: outputs\\SUBCAT_Snri\\love_plot_SUBCAT_Snri.pdf\n",
      "📏 Max weighted SMD for SUBCAT_Snri: 0.268\n",
      "\n",
      "🔍 Processing SUBCAT_Ssri...\n",
      "📊 Exported numeric summary to: outputs\\SUBCAT_Ssri\\covariate_balance_table_SUBCAT_Ssri.xlsx\n",
      "✅ Saved love plot: outputs\\SUBCAT_Ssri\\love_plot_SUBCAT_Ssri.pdf\n",
      "📏 Max weighted SMD for SUBCAT_Ssri: 0.178\n",
      "\n",
      "🔍 Processing SUBCAT_Systemische_Antihistaminica...\n",
      "📊 Exported numeric summary to: outputs\\SUBCAT_Systemische_Antihistaminica\\covariate_balance_table_SUBCAT_Systemische_Antihistaminica.xlsx\n",
      "✅ Saved love plot: outputs\\SUBCAT_Systemische_Antihistaminica\\love_plot_SUBCAT_Systemische_Antihistaminica.pdf\n",
      "📏 Max weighted SMD for SUBCAT_Systemische_Antihistaminica: 0.313\n",
      "\n",
      "🔍 Processing SUBCAT_Systemische_Betablokkers...\n",
      "📊 Exported numeric summary to: outputs\\SUBCAT_Systemische_Betablokkers\\covariate_balance_table_SUBCAT_Systemische_Betablokkers.xlsx\n",
      "✅ Saved love plot: outputs\\SUBCAT_Systemische_Betablokkers\\love_plot_SUBCAT_Systemische_Betablokkers.pdf\n",
      "📏 Max weighted SMD for SUBCAT_Systemische_Betablokkers: 0.942\n",
      "\n",
      "🔍 Processing SUBCAT_Tca...\n",
      "📊 Exported numeric summary to: outputs\\SUBCAT_Tca\\covariate_balance_table_SUBCAT_Tca.xlsx\n",
      "✅ Saved love plot: outputs\\SUBCAT_Tca\\love_plot_SUBCAT_Tca.pdf\n",
      "📏 Max weighted SMD for SUBCAT_Tca: 0.467\n",
      "\n",
      "🔍 Processing SUBCAT_Tetracyclische_Antidepressiva...\n",
      "📊 Exported numeric summary to: outputs\\SUBCAT_Tetracyclische_Antidepressiva\\covariate_balance_table_SUBCAT_Tetracyclische_Antidepressiva.xlsx\n",
      "✅ Saved love plot: outputs\\SUBCAT_Tetracyclische_Antidepressiva\\love_plot_SUBCAT_Tetracyclische_Antidepressiva.pdf\n",
      "📏 Max weighted SMD for SUBCAT_Tetracyclische_Antidepressiva: 0.377\n",
      "\n",
      "🔍 Processing SUBCAT_Z_Drugs...\n",
      "📊 Exported numeric summary to: outputs\\SUBCAT_Z_Drugs\\covariate_balance_table_SUBCAT_Z_Drugs.xlsx\n",
      "✅ Saved love plot: outputs\\SUBCAT_Z_Drugs\\love_plot_SUBCAT_Z_Drugs.pdf\n",
      "📏 Max weighted SMD for SUBCAT_Z_Drugs: 0.311\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "# ----------------------------------------\n",
    "# Functions to calculate balance\n",
    "# ----------------------------------------\n",
    "def calculate_smd(x1, x2, w1=None, w2=None):\n",
    "    def weighted_mean(x, w): return np.average(x, weights=w)\n",
    "    def weighted_var(x, w):\n",
    "        m = np.average(x, weights=w)\n",
    "        return np.average((x - m) ** 2, weights=w)\n",
    "    \n",
    "    m1 = weighted_mean(x1, w1) if w1 is not None else np.mean(x1)\n",
    "    m2 = weighted_mean(x2, w2) if w2 is not None else np.mean(x2)\n",
    "    v1 = weighted_var(x1, w1) if w1 is not None else np.var(x1, ddof=1)\n",
    "    v2 = weighted_var(x2, w2) if w2 is not None else np.var(x2, ddof=1)\n",
    "    \n",
    "    pooled_sd = np.sqrt((v1 + v2) / 2)\n",
    "    return np.abs(m1 - m2) / pooled_sd if pooled_sd > 0 else 0\n",
    "\n",
    "def variance_ratio(x1, x2, w1=None, w2=None):\n",
    "    def weighted_var(x, w):\n",
    "        m = np.average(x, weights=w)\n",
    "        return np.average((x - m) ** 2, weights=w)\n",
    "    \n",
    "    v1 = weighted_var(x1, w1) if w1 is not None else np.var(x1, ddof=1)\n",
    "    v2 = weighted_var(x2, w2) if w2 is not None else np.var(x2, ddof=1)\n",
    "    \n",
    "    return max(v1 / v2, v2 / v1) if v1 > 0 and v2 > 0 else 1\n",
    "\n",
    "# ----------------------------------------\n",
    "# Setup\n",
    "# ----------------------------------------\n",
    "output_base = \"outputs\"\n",
    "groups = [g for g in os.listdir(output_base) if os.path.isdir(os.path.join(output_base, g))]\n",
    "\n",
    "# Create a case-insensitive mapping\n",
    "final_covariates_map_lower = {k.lower(): v for k, v in final_covariates_map.items()}\n",
    "\n",
    "# ----------------------------------------\n",
    "# Main Loop\n",
    "# ----------------------------------------\n",
    "for group in groups:\n",
    "    if group.lower() not in final_covariates_map_lower:\n",
    "        continue\n",
    "\n",
    "    print(f\"\\n🔍 Processing {group}...\")\n",
    "\n",
    "    try:\n",
    "        group_path = os.path.join(output_base, group)\n",
    "        covariates = final_covariates_map_lower[group.lower()]\n",
    "        \n",
    "        column_name = None\n",
    "        for col in pd.read_pickle(os.path.join(group_path, \"trimmed_data_imp1.pkl\")).columns:\n",
    "            if col.lower() == group.lower():\n",
    "                column_name = col\n",
    "                break\n",
    "        if column_name is None:\n",
    "            print(f\"⚠️ Column not found for {group}, skipping.\")\n",
    "            continue\n",
    "\n",
    "        smd_unw_all, smd_w_all = [], []\n",
    "        vr_unw_all, vr_w_all = [], []\n",
    "\n",
    "        for i in range(1, 6):\n",
    "            df_path = os.path.join(group_path, f\"trimmed_data_imp{i}.pkl\")\n",
    "            iptw_path = os.path.join(group_path, \"iptw_weights.xlsx\")\n",
    "\n",
    "            if not os.path.exists(df_path) or not os.path.exists(iptw_path):\n",
    "                print(f\"⚠️ Missing data for {group} imp{i}, skipping.\")\n",
    "                continue\n",
    "\n",
    "            df = pd.read_pickle(df_path)\n",
    "            iptw_df = pd.read_excel(iptw_path, index_col=0)\n",
    "            T = df[column_name]\n",
    "            W = iptw_df.loc[df.index, \"iptw_mean\"]\n",
    "\n",
    "            smd_unw_i, smd_w_i, vr_unw_i, vr_w_i = [], [], [], []\n",
    "\n",
    "            for cov in covariates:\n",
    "                x1, x0 = df.loc[T == 1, cov], df.loc[T == 0, cov]\n",
    "                w1, w0 = W[T == 1], W[T == 0]\n",
    "\n",
    "                su = calculate_smd(x1, x0)\n",
    "                sw = calculate_smd(x1, x0, w1, w0)\n",
    "\n",
    "                vu = variance_ratio(x1, x0) if cov in ['treatmentdurationdays', 'CAPS5score_baseline', 'age'] else np.nan\n",
    "                vw = variance_ratio(x1, x0, w1, w0) if cov in ['treatmentdurationdays', 'CAPS5score_baseline', 'age'] else np.nan\n",
    "\n",
    "                smd_unw_i.append(su)\n",
    "                smd_w_i.append(sw)\n",
    "                vr_unw_i.append(vu)\n",
    "                vr_w_i.append(vw)\n",
    "\n",
    "            smd_unw_all.append(smd_unw_i)\n",
    "            smd_w_all.append(smd_w_i)\n",
    "            vr_unw_all.append(vr_unw_i)\n",
    "            vr_w_all.append(vr_w_i)\n",
    "\n",
    "        smd_unw = np.mean(smd_unw_all, axis=0)\n",
    "        smd_w = np.mean(smd_w_all, axis=0)\n",
    "        vr_unw = np.nanmean(vr_unw_all, axis=0)\n",
    "        vr_w = np.nanmean(vr_w_all, axis=0)\n",
    "\n",
    "        severity = []\n",
    "        for sw in smd_w:\n",
    "            if sw <= 0.1:\n",
    "                severity.append(\"Good\")\n",
    "            elif sw <= 0.2:\n",
    "                severity.append(\"Moderate\")\n",
    "            else:\n",
    "                severity.append(\"Poor\")\n",
    "\n",
    "        covariate_names = covariates\n",
    "        numeric_df = pd.DataFrame({\n",
    "            \"Covariate\": covariate_names,\n",
    "            \"SMD_Unweighted\": smd_unw,\n",
    "            \"SMD_Weighted\": smd_w,\n",
    "            \"Imbalance_Severity\": severity,\n",
    "            \"VR_Unweighted\": vr_unw,\n",
    "            \"VR_Weighted\": vr_w\n",
    "        })\n",
    "\n",
    "        numeric_path = os.path.join(group_path, f\"covariate_balance_table_{group}.xlsx\")\n",
    "        numeric_df.to_excel(numeric_path, index=False)\n",
    "        print(f\"📊 Exported numeric summary to: {numeric_path}\")\n",
    "\n",
    "        # -------------------------\n",
    "        # Plot\n",
    "        # -------------------------\n",
    "        labels = covariates\n",
    "        y_pos = np.arange(len(labels))\n",
    "\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(18, len(labels) * 0.45))\n",
    "\n",
    "        axes[0].scatter(smd_unw, y_pos, color='red', label=\"Unweighted\")\n",
    "        axes[0].scatter(smd_w, y_pos, color='blue', label=\"Weighted\")\n",
    "        axes[0].axvline(0.1, color='gray', linestyle='--', label=\"Threshold 0.1\")\n",
    "        axes[0].axvline(0.2, color='black', linestyle='--', label=\"Threshold 0.2\")\n",
    "        axes[0].set_xlim(0, max(max(smd_unw), max(smd_w), 0.25) + 0.05)\n",
    "        axes[0].set_yticks(y_pos)\n",
    "        axes[0].set_yticklabels(labels)\n",
    "        axes[0].invert_yaxis()\n",
    "        axes[0].set_title(\"Standardized Mean Differences (SMD)\")\n",
    "        axes[0].legend(loc=\"upper right\")\n",
    "        axes[0].grid(True)\n",
    "\n",
    "        vr_mask = [cov in ['treatmentdurationdays', 'CAPS5score_baseline', 'age'] for cov in covariates]\n",
    "        filtered_y = [i for i, b in enumerate(vr_mask) if b]\n",
    "        filtered_labels = [labels[i] for i in filtered_y]\n",
    "        filtered_vr_unw = [vr_unw[i] for i in filtered_y]\n",
    "        filtered_vr_w = [vr_w[i] for i in filtered_y]\n",
    "\n",
    "        axes[1].scatter(filtered_vr_unw, filtered_y, color='blue', marker='o', label=\"Unweighted\")\n",
    "        axes[1].scatter(filtered_vr_w, filtered_y, color='red', marker='x', label=\"Weighted\")\n",
    "        axes[1].axvline(2, color='gray', linestyle='--')\n",
    "        axes[1].axvline(0.5, color='gray', linestyle='--')\n",
    "        axes[1].set_xlim(0, max(filtered_vr_unw + filtered_vr_w + [2.5]) + 0.5)\n",
    "        axes[1].set_yticks(filtered_y)\n",
    "        axes[1].set_yticklabels(filtered_labels)\n",
    "        axes[1].invert_yaxis()\n",
    "        axes[1].set_title(\"Variance Ratio (VR)\")\n",
    "        axes[1].legend()\n",
    "        axes[1].grid(True)\n",
    "\n",
    "        fig.suptitle(f\"Covariate Balance for {group.replace('CAT_', '')}\", fontsize=14, weight='bold')\n",
    "        fig.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "        plot_path = os.path.join(group_path, f\"love_plot_{group}.pdf\")\n",
    "        fig.savefig(plot_path, dpi=300)\n",
    "        plt.close()\n",
    "        print(f\"✅ Saved love plot: {plot_path}\")\n",
    "        print(f\"📏 Max weighted SMD for {group}: {np.max(smd_w):.3f}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error in {group}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "633fd988-7a59-468d-895b-e92529e96f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e954a5bc-a137-481e-a44f-92ed766c8abf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== Creating Heatmap for SUBCAT_Antipsychotica_atypisch ==========\n",
      "✅ Heatmap saved: outputs\\SUBCAT_Antipsychotica_atypisch\\heatmap_smd_SUBCAT_Antipsychotica_atypisch.png\n",
      "\n",
      "========== Creating Heatmap for SUBCAT_TCA ==========\n",
      "✅ Heatmap saved: outputs\\SUBCAT_TCA\\heatmap_smd_SUBCAT_TCA.png\n",
      "\n",
      "========== Creating Heatmap for SUBCAT_SSRI ==========\n",
      "✅ Heatmap saved: outputs\\SUBCAT_SSRI\\heatmap_smd_SUBCAT_SSRI.png\n",
      "\n",
      "========== Creating Heatmap for SUBCAT_SNRI ==========\n",
      "✅ Heatmap saved: outputs\\SUBCAT_SNRI\\heatmap_smd_SUBCAT_SNRI.png\n",
      "\n",
      "========== Creating Heatmap for SUBCAT_Tetracyclische_antidepressiva ==========\n",
      "✅ Heatmap saved: outputs\\SUBCAT_Tetracyclische_antidepressiva\\heatmap_smd_SUBCAT_Tetracyclische_antidepressiva.png\n",
      "\n",
      "========== Creating Heatmap for SUBCAT_Antidepressiva_overige ==========\n",
      "✅ Heatmap saved: outputs\\SUBCAT_Antidepressiva_overige\\heatmap_smd_SUBCAT_Antidepressiva_overige.png\n",
      "\n",
      "========== Creating Heatmap for SUBCAT_Systemische_antihistaminica ==========\n",
      "✅ Heatmap saved: outputs\\SUBCAT_Systemische_antihistaminica\\heatmap_smd_SUBCAT_Systemische_antihistaminica.png\n",
      "\n",
      "========== Creating Heatmap for SUBCAT_anxiolytica_Benzodiazepine ==========\n",
      "✅ Heatmap saved: outputs\\SUBCAT_anxiolytica_Benzodiazepine\\heatmap_smd_SUBCAT_anxiolytica_Benzodiazepine.png\n",
      "\n",
      "========== Creating Heatmap for SUBCAT_hypnotica_Benzodiazepine ==========\n",
      "✅ Heatmap saved: outputs\\SUBCAT_hypnotica_Benzodiazepine\\heatmap_smd_SUBCAT_hypnotica_Benzodiazepine.png\n",
      "\n",
      "========== Creating Heatmap for SUBCAT_Amfetaminen ==========\n",
      "✅ Heatmap saved: outputs\\SUBCAT_Amfetaminen\\heatmap_smd_SUBCAT_Amfetaminen.png\n",
      "\n",
      "========== Creating Heatmap for SUBCAT_Systemische_betablokkers ==========\n",
      "✅ Heatmap saved: outputs\\SUBCAT_Systemische_betablokkers\\heatmap_smd_SUBCAT_Systemische_betablokkers.png\n",
      "\n",
      "========== Creating Heatmap for SUBCAT_Paracetamol_mono ==========\n",
      "✅ Heatmap saved: outputs\\SUBCAT_Paracetamol_mono\\heatmap_smd_SUBCAT_Paracetamol_mono.png\n",
      "\n",
      "========== Creating Heatmap for SUBCAT_Anti_epileptica_stemmingsstabilisatoren ==========\n",
      "✅ Heatmap saved: outputs\\SUBCAT_Anti_epileptica_stemmingsstabilisatoren\\heatmap_smd_SUBCAT_Anti_epileptica_stemmingsstabilisatoren.png\n",
      "\n",
      "========== Creating Heatmap for SUBCAT_Opioden ==========\n",
      "✅ Heatmap saved: outputs\\SUBCAT_Opioden\\heatmap_smd_SUBCAT_Opioden.png\n",
      "\n",
      "========== Creating Heatmap for SUBCAT_Z_drugs ==========\n",
      "✅ Heatmap saved: outputs\\SUBCAT_Z_drugs\\heatmap_smd_SUBCAT_Z_drugs.png\n",
      "\n",
      "========== Creating Heatmap for SUBCAT_NSAIDs ==========\n",
      "✅ Heatmap saved: outputs\\SUBCAT_NSAIDs\\heatmap_smd_SUBCAT_NSAIDs.png\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "#-----------------------------\n",
    "# Generate heatmaps\n",
    "# -------------------------------\n",
    "for treatment_var in medication_groups:\n",
    "    print(f\"\\n========== Creating Heatmap for {treatment_var} ==========\")\n",
    "\n",
    "    try:\n",
    "        output_folder = os.path.join('outputs', treatment_var)\n",
    "        balance_path = os.path.join(output_folder, f'covariate_balance_table_{treatment_var}.xlsx')\n",
    "\n",
    "        if not os.path.exists(balance_path):\n",
    "            print(f\"❌ Balance file not found: {balance_path}\")\n",
    "            continue\n",
    "\n",
    "        balance_df = pd.read_excel(balance_path)\n",
    "\n",
    "        # ✅ Use finalized covariates + 'Propensity Score'\n",
    "        covariates = final_covariates_map[treatment_var] + ['Propensity Score']\n",
    "        balance_df = balance_df[balance_df['Covariate'].isin(covariates)]\n",
    "\n",
    "        # ✅ Check for CAPS5score_baseline\n",
    "        highlight_caps = 'CAPS5score_baseline' in balance_df['Covariate'].values\n",
    "\n",
    "        # ✅ Format for heatmap\n",
    "        heatmap_df = balance_df[['Covariate', 'SMD_Unweighted', 'SMD_Weighted']].copy()\n",
    "        heatmap_df.columns = ['Covariate', 'Unweighted', 'Weighted']\n",
    "        heatmap_df = heatmap_df.set_index('Covariate')\n",
    "        heatmap_df = heatmap_df.sort_values(by='Unweighted', ascending=False)\n",
    "\n",
    "        # ✅ Plot\n",
    "        plt.figure(figsize=(12, max(10, len(heatmap_df) * 0.35)))\n",
    "        ax = sns.heatmap(\n",
    "            heatmap_df,\n",
    "            cmap=\"coolwarm\",\n",
    "            annot=True,\n",
    "            fmt=\".2f\",\n",
    "            linewidths=0.6,\n",
    "            linecolor='gray',\n",
    "            cbar_kws={\"label\": \"Standardized Mean Difference\"}\n",
    "        )\n",
    "\n",
    "        plt.title(f\"Covariate Balance Heatmap (Rubin IPTW)\\n{treatment_var}\", fontsize=15, weight='bold')\n",
    "        plt.xlabel(\"Condition\")\n",
    "        plt.ylabel(\"Covariate\")\n",
    "\n",
    "        # ✅ Bold CAPS5score_baseline if present\n",
    "        if highlight_caps:\n",
    "            ylabels = [label.get_text() for label in ax.get_yticklabels()]\n",
    "            ax.set_yticklabels([\n",
    "                f\"{label} ←\" if label == 'CAPS5score_baseline' else label for label in ylabels\n",
    "            ])\n",
    "\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # ✅ Save image\n",
    "        save_path = os.path.join(output_folder, f'heatmap_smd_{treatment_var}.png')\n",
    "        plt.savefig(save_path, dpi=300)\n",
    "        plt.close()\n",
    "\n",
    "        print(f\"✅ Heatmap saved: {save_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error processing {treatment_var}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03725ea3-1967-48cf-a8ae-acabff5ac054",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "feef0543-716d-47ca-a887-00fcb78c5521",
   "metadata": {},
   "source": [
    "### SubSubCat Analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "384b36ab-32ce-40e7-a2ad-4ea810f87841",
   "metadata": {},
   "outputs": [],
   "source": [
    "covariates_SubSubCat_Oxazepam = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline',\n",
    "    'CAT_Antidepressiva', 'CAT_Antipsychotica',\n",
    "    'DIAGNOSIS_CHILDHOOD_TRAUMA', 'DIAGNOSIS_CPTSD', 'DIAGNOSIS_SEXUAL_TRAUMA',\n",
    "    'DIAGNOSIS_SUICIDALITY', 'age',\n",
    "    'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"Bipolar_and_Mood_disorder\", \"ethnicity_Dutch\", \"ethnicity_other\"\n",
    "]\n",
    "\n",
    "covariates_SubSubCat_Diazepam = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline',\n",
    "    'CAT_Antidepressiva', 'CAT_Antipsychotica',\n",
    "    'DIAGNOSIS_CHILDHOOD_TRAUMA', 'DIAGNOSIS_CPTSD', 'DIAGNOSIS_SEXUAL_TRAUMA',\n",
    "    'DIAGNOSIS_SUICIDALITY', 'age',\n",
    "    'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"Bipolar_and_Mood_disorder\", \"ethnicity_Dutch\", \"ethnicity_other\"\n",
    "]\n",
    "\n",
    "\n",
    "covariates_SubSubCat_Paracetamol = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline',\n",
    "    'CAT_Antidepressiva',\n",
    "    'CAT_Antipsychotica',\n",
    "    'CAT_Benzodiazepine',\n",
    "    'DIAGNOSIS_CHILDHOOD_TRAUMA', 'DIAGNOSIS_CPTSD', 'DIAGNOSIS_SEXUAL_TRAUMA',\n",
    "    'DIAGNOSIS_SUICIDALITY', 'age',\n",
    "    'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"Bipolar_and_Mood_disorder\", \"ethnicity_Dutch\", \"ethnicity_other\"\n",
    "]\n",
    "\n",
    "covariates_SubSubCat_Lorazepam = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline',\n",
    "    'CAT_Antidepressiva', 'CAT_Antipsychotica',\n",
    "    'DIAGNOSIS_CHILDHOOD_TRAUMA', 'DIAGNOSIS_CPTSD', 'DIAGNOSIS_SEXUAL_TRAUMA',\n",
    "    'DIAGNOSIS_SUICIDALITY', 'age',\n",
    "    'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"Bipolar_and_Mood_disorder\", \"ethnicity_Dutch\", \"ethnicity_other\"\n",
    "]\n",
    "\n",
    "covariates_SubSubCat_Mirtazapine = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline',\n",
    "    'CAT_Antipsychotica',\n",
    "    'CAT_Benzodiazepine',\n",
    "    'DIAGNOSIS_CHILDHOOD_TRAUMA', 'DIAGNOSIS_CPTSD', 'DIAGNOSIS_SEXUAL_TRAUMA',\n",
    "    'DIAGNOSIS_SUICIDALITY', 'age',\n",
    "    'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"Bipolar_and_Mood_disorder\", \"ethnicity_Dutch\", \"ethnicity_other\"\n",
    "]\n",
    "\n",
    "covariates_SubSubCat_Escitalopram = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline',\n",
    "    'CAT_Antipsychotica',\n",
    "    'CAT_Benzodiazepine',\n",
    "    'DIAGNOSIS_CHILDHOOD_TRAUMA', 'DIAGNOSIS_CPTSD', 'DIAGNOSIS_SEXUAL_TRAUMA',\n",
    "    'DIAGNOSIS_SUICIDALITY', 'age',\n",
    "    'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"Bipolar_and_Mood_disorder\", \"ethnicity_Dutch\", \"ethnicity_other\"\n",
    "]\n",
    "\n",
    "\n",
    "covariates_SubSubCat_Sertraline = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline',\n",
    "    'CAT_Antipsychotica',\n",
    "    'CAT_Benzodiazepine',\n",
    "    'DIAGNOSIS_CHILDHOOD_TRAUMA', 'DIAGNOSIS_CPTSD', 'DIAGNOSIS_SEXUAL_TRAUMA',\n",
    "    'DIAGNOSIS_SUICIDALITY', 'age',\n",
    "    'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"Bipolar_and_Mood_disorder\", \"ethnicity_Dutch\", \"ethnicity_other\"\n",
    "]\n",
    "\n",
    "covariates_SubSubCat_Temazepam = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline',\n",
    "    'CAT_Antidepressiva', 'CAT_Antipsychotica',\n",
    "    'DIAGNOSIS_CHILDHOOD_TRAUMA', 'DIAGNOSIS_CPTSD', 'DIAGNOSIS_SEXUAL_TRAUMA',\n",
    "    'DIAGNOSIS_SUICIDALITY', 'age',\n",
    "    'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"Bipolar_and_Mood_disorder\", \"ethnicity_Dutch\", \"ethnicity_other\"\n",
    "]\n",
    "\n",
    "covariates_SubSubCat_Citalopram = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline',\n",
    "    'CAT_Antipsychotica',\n",
    "    'CAT_Benzodiazepine',\n",
    "    'DIAGNOSIS_CHILDHOOD_TRAUMA', 'DIAGNOSIS_CPTSD', 'DIAGNOSIS_SEXUAL_TRAUMA',\n",
    "    'DIAGNOSIS_SUICIDALITY', 'age',\n",
    "    'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"Bipolar_and_Mood_disorder\", \"ethnicity_Dutch\", \"ethnicity_other\"\n",
    "]\n",
    "\n",
    "covariates_SubSubCat_Quetiapine = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline',\n",
    "    'CAT_Antidepressiva',\n",
    "    'CAT_Benzodiazepine',\n",
    "    'DIAGNOSIS_CHILDHOOD_TRAUMA', 'DIAGNOSIS_CPTSD', 'DIAGNOSIS_SEXUAL_TRAUMA',\n",
    "    'DIAGNOSIS_SUICIDALITY', 'age',\n",
    "    'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"Bipolar_and_Mood_disorder\", \"ethnicity_Dutch\", \"ethnicity_other\"\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "covariates_SubSubCat_Amitriptyline = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline',\n",
    "    'CAT_Antipsychotica',\n",
    "    'CAT_Benzodiazepine',\n",
    "    'DIAGNOSIS_CHILDHOOD_TRAUMA', 'DIAGNOSIS_CPTSD', 'DIAGNOSIS_SEXUAL_TRAUMA',\n",
    "    'DIAGNOSIS_SUICIDALITY', 'age',\n",
    "    'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"Bipolar_and_Mood_disorder\", \"ethnicity_Dutch\", \"ethnicity_other\"\n",
    "]\n",
    "\n",
    "\n",
    "covariates_SubSubCat_Venlafaxine = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline',\n",
    "    'CAT_Antipsychotica',\n",
    "    'CAT_Benzodiazepine',\n",
    "    'DIAGNOSIS_CHILDHOOD_TRAUMA', 'DIAGNOSIS_CPTSD', 'DIAGNOSIS_SEXUAL_TRAUMA',\n",
    "    'DIAGNOSIS_SUICIDALITY', 'age',\n",
    "    'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"Bipolar_and_Mood_disorder\", \"ethnicity_Dutch\", \"ethnicity_other\"\n",
    "]\n",
    "\n",
    "covariates_SubSubCat_Fluoxetine = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline',\n",
    "    'CAT_Antipsychotica',\n",
    "    'CAT_Benzodiazepine',\n",
    "    'DIAGNOSIS_CHILDHOOD_TRAUMA', 'DIAGNOSIS_CPTSD', 'DIAGNOSIS_SEXUAL_TRAUMA',\n",
    "    'DIAGNOSIS_SUICIDALITY', 'age',\n",
    "    'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"Bipolar_and_Mood_disorder\", \"ethnicity_Dutch\", \"ethnicity_other\"\n",
    "]\n",
    "\n",
    "covariates_SubSubCat_Topiramaat = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline',\n",
    "    'CAT_Antidepressiva',\n",
    "    'CAT_Antipsychotica',\n",
    "    'CAT_Benzodiazepine',\n",
    "    'DIAGNOSIS_CHILDHOOD_TRAUMA', 'DIAGNOSIS_CPTSD', 'DIAGNOSIS_SEXUAL_TRAUMA',\n",
    "    'DIAGNOSIS_SUICIDALITY', 'age',\n",
    "    'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"Bipolar_and_Mood_disorder\", \"ethnicity_Dutch\", \"ethnicity_other\"\n",
    "]\n",
    "\n",
    "covariates_SubSubCat_Tramadol = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline',\n",
    "    'CAT_Antidepressiva', 'CAT_Antipsychotica', 'CAT_Benzodiazepine',\n",
    "    'DIAGNOSIS_CHILDHOOD_TRAUMA', 'DIAGNOSIS_CPTSD', 'DIAGNOSIS_SEXUAL_TRAUMA',\n",
    "    'DIAGNOSIS_SUICIDALITY', 'age',\n",
    "    'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"Bipolar_and_Mood_disorder\", \"ethnicity_Dutch\", \"ethnicity_other\"\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "covariates_SubSubCat_Zopiclon = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline',\n",
    "    'CAT_Antidepressiva',\n",
    "    'CAT_Antipsychotica',\n",
    "    'CAT_Benzodiazepine',\n",
    "    'DIAGNOSIS_CHILDHOOD_TRAUMA', 'DIAGNOSIS_CPTSD', 'DIAGNOSIS_SEXUAL_TRAUMA',\n",
    "    'DIAGNOSIS_SUICIDALITY', 'age',\n",
    "    'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"Bipolar_and_Mood_disorder\", \"ethnicity_Dutch\", \"ethnicity_other\"\n",
    "]\n",
    "\n",
    "covariates_SubSubCat_Loprazolam = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline',\n",
    "    'CAT_Antidepressiva', 'CAT_Antipsychotica',\n",
    "    'DIAGNOSIS_CHILDHOOD_TRAUMA', 'DIAGNOSIS_CPTSD', 'DIAGNOSIS_SEXUAL_TRAUMA',\n",
    "    'DIAGNOSIS_SUICIDALITY', 'age',\n",
    "    'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"Bipolar_and_Mood_disorder\", \"ethnicity_Dutch\", \"ethnicity_other\"\n",
    "]\n",
    "\n",
    "covariates_SubSubCat_Alprazolam = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline',\n",
    "    'CAT_Antidepressiva', 'CAT_Antipsychotica',\n",
    "    'DIAGNOSIS_CHILDHOOD_TRAUMA', 'DIAGNOSIS_CPTSD', 'DIAGNOSIS_SEXUAL_TRAUMA',\n",
    "    'DIAGNOSIS_SUICIDALITY', 'age',\n",
    "    'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"Bipolar_and_Mood_disorder\", \"ethnicity_Dutch\", \"ethnicity_other\"\n",
    "]\n",
    "\n",
    "covariates_SubSubCat_promethazine = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline',\n",
    "    'CAT_Antidepressiva', 'CAT_Antipsychotica',\n",
    "    'CAT_Benzodiazepine',\n",
    "    'DIAGNOSIS_CHILDHOOD_TRAUMA', 'DIAGNOSIS_CPTSD', 'DIAGNOSIS_SEXUAL_TRAUMA',\n",
    "    'DIAGNOSIS_SUICIDALITY', 'age',\n",
    "    'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"Bipolar_and_Mood_disorder\", \"ethnicity_Dutch\", \"ethnicity_other\"\n",
    "]\n",
    "\n",
    "covariates_SubSubCat_Paroxetine = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline',\n",
    "    'CAT_Antipsychotica',\n",
    "    'CAT_Benzodiazepine',\n",
    "    'DIAGNOSIS_CHILDHOOD_TRAUMA', 'DIAGNOSIS_CPTSD', 'DIAGNOSIS_SEXUAL_TRAUMA',\n",
    "    'DIAGNOSIS_SUICIDALITY', 'age',\n",
    "    'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"Bipolar_and_Mood_disorder\", \"ethnicity_Dutch\", \"ethnicity_other\"\n",
    "]\n",
    "\n",
    "\n",
    "covariates_SubSubCat_Bupropion = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline',\n",
    "    'CAT_Antipsychotica',\n",
    "    'CAT_Benzodiazepine',\n",
    "    'DIAGNOSIS_CHILDHOOD_TRAUMA', 'DIAGNOSIS_CPTSD', 'DIAGNOSIS_SEXUAL_TRAUMA',\n",
    "    'DIAGNOSIS_SUICIDALITY', 'age',\n",
    "    'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"Bipolar_and_Mood_disorder\", \"ethnicity_Dutch\", \"ethnicity_other\"\n",
    "]\n",
    "\n",
    "covariates_SubSubCat_Methylfenidaat = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline',\n",
    "    'CAT_Antidepressiva',\n",
    "    'CAT_Antipsychotica',\n",
    "    'CAT_Benzodiazepine',\n",
    "    'DIAGNOSIS_CHILDHOOD_TRAUMA', 'DIAGNOSIS_CPTSD',\n",
    "    'DIAGNOSIS_SEXUAL_TRAUMA', 'DIAGNOSIS_SUICIDALITY',\n",
    "    'age',\n",
    "    'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"Bipolar_and_Mood_disorder\", \"ethnicity_Dutch\", \"ethnicity_other\"\n",
    "]\n",
    "\n",
    "covariates_SubSubCat_Olanzapine = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline',\n",
    "    'CAT_Antidepressiva',\n",
    "    'CAT_Benzodiazepine',\n",
    "    'DIAGNOSIS_CHILDHOOD_TRAUMA', 'DIAGNOSIS_CPTSD', 'DIAGNOSIS_SEXUAL_TRAUMA',\n",
    "    'DIAGNOSIS_SUICIDALITY', 'age',\n",
    "    'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"Bipolar_and_Mood_disorder\", \"ethnicity_Dutch\", \"ethnicity_other\"\n",
    "]\n",
    "\n",
    "\n",
    "covariates_SubSubCat_Zolpidem = [\n",
    "    'treatmentdurationdays', 'CAPS5score_baseline',\n",
    "    'CAT_Antidepressiva', 'CAT_Antipsychotica',\n",
    "    'CAT_Benzodiazepine',\n",
    "    'DIAGNOSIS_CHILDHOOD_TRAUMA', 'DIAGNOSIS_CPTSD', 'DIAGNOSIS_SEXUAL_TRAUMA',\n",
    "    'DIAGNOSIS_SUICIDALITY', 'age',\n",
    "    'DIAGNOSIS_ANXIETY_OCD',\n",
    "    'DIAGNOSIS_PSYCHOTIC',\n",
    "    'DIAGNOSIS_EATING_DISORDER',\n",
    "    'DIAGNOSIS_SUBSTANCE_DISORDER', 'SDV_SEXE_1', 'SDV_SEXE_2', 'SDV_SEXE_3', \"EB_TRAUMA_FOCUSED_THERAPY\",\n",
    "    \"Bipolar_and_Mood_disorder\", \"ethnicity_Dutch\", \"ethnicity_other\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7be2d6fd-69da-43e9-b686-89d57f72036d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Groups found: ['SubSubCat_Oxazepam', 'SubSubCat_Diazepam', 'SubSubCat_Paracetamol', 'SubSubCat_Lorazepam', 'SubSubCat_Mirtazapine', 'SubSubCat_Escitalopram', 'SubSubCat_Sertraline', 'SubSubCat_Temazepam', 'SubSubCat_Citalopram', 'SubSubCat_Quetiapine', 'SubSubCat_Amitriptyline', 'SubSubCat_Venlafaxine', 'SubSubCat_Fluoxetine', 'SubSubCat_Topiramaat', 'SubSubCat_Tramadol', 'SubSubCat_Zopiclon', 'SubSubCat_Loprazolam', 'SubSubCat_Alprazolam', 'SubSubCat_promethazine', 'SubSubCat_Paroxetine', 'SubSubCat_Bupropion', 'SubSubCat_Methylfenidaat', 'SubSubCat_Olanzapine', 'SubSubCat_Zolpidem']\n",
      "['SubSubCat_Oxazepam', 'SubSubCat_Diazepam', 'SubSubCat_Paracetamol', 'SubSubCat_Lorazepam', 'SubSubCat_Mirtazapine', 'SubSubCat_Escitalopram', 'SubSubCat_Sertraline', 'SubSubCat_Temazepam', 'SubSubCat_Citalopram', 'SubSubCat_Quetiapine', 'SubSubCat_Amitriptyline', 'SubSubCat_Venlafaxine', 'SubSubCat_Fluoxetine', 'SubSubCat_Topiramaat', 'SubSubCat_Tramadol', 'SubSubCat_Zopiclon', 'SubSubCat_Loprazolam', 'SubSubCat_Alprazolam', 'SubSubCat_promethazine', 'SubSubCat_Paroxetine', 'SubSubCat_Bupropion', 'SubSubCat_Methylfenidaat', 'SubSubCat_Olanzapine', 'SubSubCat_Zolpidem']\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# This finds all variables that start with covariates_SUbSubCAT_ or covariates_SubSubcat_\n",
    "final_covariates_map = defaultdict(list)\n",
    "final_covariates_map.update({\n",
    "    var.replace(\"covariates_\", \"\"): val\n",
    "    for var, val in globals().items()\n",
    "    if var.lower().startswith(\"covariates_subsubcat_\") and isinstance(val, list)\n",
    "})\n",
    "\n",
    "# Show detected group names\n",
    "print(\"Groups found:\", list(final_covariates_map.keys()))\n",
    "medication_groups = list(final_covariates_map.keys())\n",
    "print(medication_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8407653a-734f-44e5-bc41-aeea94992c95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Starting analysis for all SUBSUBCAT groups\n",
      "\n",
      " Processing SUBSUBCAT_Oxazepam...\n",
      "  → Using imputation 1\n",
      "  → Using imputation 2\n",
      "  → Using imputation 3\n",
      "  → Using imputation 4\n",
      "  → Using imputation 5\n",
      " Done: SUBSUBCAT_Oxazepam\n",
      "\n",
      " Processing SUBSUBCAT_Diazepam...\n",
      "  → Using imputation 1\n",
      "  → Using imputation 2\n",
      "  → Using imputation 3\n",
      "  → Using imputation 4\n",
      "  → Using imputation 5\n",
      " Done: SUBSUBCAT_Diazepam\n",
      "\n",
      " Processing SUBSUBCAT_Paracetamol...\n",
      "  → Using imputation 1\n",
      "  → Using imputation 2\n",
      "  → Using imputation 3\n",
      "  → Using imputation 4\n",
      "  → Using imputation 5\n",
      " Done: SUBSUBCAT_Paracetamol\n",
      "\n",
      " Processing SUBSUBCAT_Lorazepam...\n",
      "  → Using imputation 1\n",
      "  → Using imputation 2\n",
      "  → Using imputation 3\n",
      "  → Using imputation 4\n",
      "  → Using imputation 5\n",
      " Done: SUBSUBCAT_Lorazepam\n",
      "\n",
      " Processing SUBSUBCAT_Mirtazapine...\n",
      "  → Using imputation 1\n",
      "  → Using imputation 2\n",
      "  → Using imputation 3\n",
      "  → Using imputation 4\n",
      "  → Using imputation 5\n",
      " Done: SUBSUBCAT_Mirtazapine\n",
      "\n",
      " Processing SUBSUBCAT_Escitalopram...\n",
      "  → Using imputation 1\n",
      "  → Using imputation 2\n",
      "  → Using imputation 3\n",
      "  → Using imputation 4\n",
      "  → Using imputation 5\n",
      " Done: SUBSUBCAT_Escitalopram\n",
      "\n",
      " Processing SUBSUBCAT_Sertraline...\n",
      "  → Using imputation 1\n",
      "  → Using imputation 2\n",
      "  → Using imputation 3\n",
      "  → Using imputation 4\n",
      "  → Using imputation 5\n",
      " Done: SUBSUBCAT_Sertraline\n",
      "\n",
      " Processing SUBSUBCAT_Temazepam...\n",
      "  → Using imputation 1\n",
      "  → Using imputation 2\n",
      "  → Using imputation 3\n",
      "  → Using imputation 4\n",
      "  → Using imputation 5\n",
      " Done: SUBSUBCAT_Temazepam\n",
      "\n",
      " Processing SUBSUBCAT_Citalopram...\n",
      "  → Using imputation 1\n",
      "  → Using imputation 2\n",
      "  → Using imputation 3\n",
      "  → Using imputation 4\n",
      "  → Using imputation 5\n",
      " Done: SUBSUBCAT_Citalopram\n",
      "\n",
      " Processing SUBSUBCAT_Quetiapine...\n",
      "  → Using imputation 1\n",
      "  → Using imputation 2\n",
      "  → Using imputation 3\n",
      "  → Using imputation 4\n",
      "  → Using imputation 5\n",
      " Done: SUBSUBCAT_Quetiapine\n",
      "\n",
      " Processing SUBSUBCAT_Amitriptyline...\n",
      "  → Using imputation 1\n",
      "  → Using imputation 2\n",
      "  → Using imputation 3\n",
      "  → Using imputation 4\n",
      "  → Using imputation 5\n",
      " Done: SUBSUBCAT_Amitriptyline\n",
      "\n",
      " Processing SUBSUBCAT_Venlafaxine...\n",
      "  → Using imputation 1\n",
      "  → Using imputation 2\n",
      "  → Using imputation 3\n",
      "  → Using imputation 4\n",
      "  → Using imputation 5\n",
      " Done: SUBSUBCAT_Venlafaxine\n",
      "\n",
      " Processing SUBSUBCAT_Fluoxetine...\n",
      "  → Using imputation 1\n",
      "  → Using imputation 2\n",
      "  → Using imputation 3\n",
      "  → Using imputation 4\n",
      "  → Using imputation 5\n",
      " Done: SUBSUBCAT_Fluoxetine\n",
      "\n",
      " Processing SUBSUBCAT_Topiramaat...\n",
      "  → Using imputation 1\n",
      "  → Using imputation 2\n",
      "  → Using imputation 3\n",
      "  → Using imputation 4\n",
      "  → Using imputation 5\n",
      " Done: SUBSUBCAT_Topiramaat\n",
      "\n",
      " Processing SUBSUBCAT_Tramadol...\n",
      "  → Using imputation 1\n",
      "  → Using imputation 2\n",
      "  → Using imputation 3\n",
      "  → Using imputation 4\n",
      "  → Using imputation 5\n",
      " Done: SUBSUBCAT_Tramadol\n",
      "\n",
      " Processing SUBSUBCAT_Zopiclon...\n",
      "  → Using imputation 1\n",
      "  → Using imputation 2\n",
      "  → Using imputation 3\n",
      "  → Using imputation 4\n",
      "  → Using imputation 5\n",
      " Done: SUBSUBCAT_Zopiclon\n",
      "\n",
      " Processing SUBSUBCAT_Loprazolam...\n",
      "  → Using imputation 1\n",
      "  → Using imputation 2\n",
      "  → Using imputation 3\n",
      "  → Using imputation 4\n",
      "  → Using imputation 5\n",
      " Done: SUBSUBCAT_Loprazolam\n",
      "\n",
      " Processing SUBSUBCAT_Alprazolam...\n",
      "  → Using imputation 1\n",
      "  → Using imputation 2\n",
      "  → Using imputation 3\n",
      "  → Using imputation 4\n",
      "  → Using imputation 5\n",
      " Done: SUBSUBCAT_Alprazolam\n",
      "\n",
      " Processing SUBSUBCAT_Promethazine...\n",
      "  → Using imputation 1\n",
      "  → Using imputation 2\n",
      "  → Using imputation 3\n",
      "  → Using imputation 4\n",
      "  → Using imputation 5\n",
      " Done: SUBSUBCAT_Promethazine\n",
      "\n",
      " Processing SUBSUBCAT_Paroxetine...\n",
      "  → Using imputation 1\n",
      "  → Using imputation 2\n",
      "  → Using imputation 3\n",
      "  → Using imputation 4\n",
      "  → Using imputation 5\n",
      " Done: SUBSUBCAT_Paroxetine\n",
      "\n",
      " Processing SUBSUBCAT_Bupropion...\n",
      "  → Using imputation 1\n",
      "  → Using imputation 2\n",
      "  → Using imputation 3\n",
      "  → Using imputation 4\n",
      "  → Using imputation 5\n",
      " Done: SUBSUBCAT_Bupropion\n",
      "\n",
      " Processing SUBSUBCAT_Methylfenidaat...\n",
      "  → Using imputation 1\n",
      "  → Using imputation 2\n",
      "  → Using imputation 3\n",
      "  → Using imputation 4\n",
      "  → Using imputation 5\n",
      " Done: SUBSUBCAT_Methylfenidaat\n",
      "\n",
      " Processing SUBSUBCAT_Olanzapine...\n",
      "  → Using imputation 1\n",
      "  → Using imputation 2\n",
      "  → Using imputation 3\n",
      "  → Using imputation 4\n",
      "  → Using imputation 5\n",
      " Done: SUBSUBCAT_Olanzapine\n",
      "\n",
      " Processing SUBSUBCAT_Zolpidem...\n",
      "  → Using imputation 1\n",
      "  → Using imputation 2\n",
      "  → Using imputation 3\n",
      "  → Using imputation 4\n",
      "  → Using imputation 5\n",
      " Done: SUBSUBCAT_Zolpidem\n",
      "\n",
      " All SUBSUBCAT group analyses complete.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "def run_all_SUBSUBCAT_group_models(imputed_dfs):\n",
    "    \"\"\"\n",
    "    Runs downstream analysis for each SUBSUBCAT medisubsubcation group using imputed datasets.\n",
    "\n",
    "    Parameters:\n",
    "    - imputed_dfs: list of 5 imputed DataFrames (from df_imputed_final_imp1.pkl ... imp5.pkl)\n",
    "    \n",
    "    Notes:\n",
    "    - Covariate lists must be defined as global variables: covariates_subsubcat_<group>\n",
    "    - Outputs are saved in: outputs/SUBSUBCAT_<GROUP>/\n",
    "    \"\"\"\n",
    "\n",
    "    print(\" Starting analysis for all SUBSUBCAT groups\")\n",
    "\n",
    "    for var_name in globals():\n",
    "        if var_name.lower().startswith(\"covariates_subsubcat_\") and isinstance(globals()[var_name], list):\n",
    "            group_name = var_name.replace(\"covariates_\", \"\")\n",
    "            group_name = group_name.replace(\"_\", \" \").title().replace(\" \", \"_\")  # e.g., subsubcat_z_drugs → Subsubcat_Z_Drugs\n",
    "            group_name = group_name.replace(\"Subsubcat_\", \"SUBSUBCAT_\")  # force prefix to uppercase\n",
    "\n",
    "            covariates = globals()[var_name]\n",
    "            output_dir = f\"outputs/{group_name}\"\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "            print(f\"\\n Processing {group_name}...\")\n",
    "\n",
    "            for k, df_imp in enumerate(imputed_dfs):\n",
    "                print(f\"  → Using imputation {k+1}\")\n",
    "\n",
    "                # Define X and Y\n",
    "                X = df_imp[covariates]\n",
    "                Y = df_imp[\"caps5_change_baseline\"]\n",
    "\n",
    "                # === Save X and Y as placeholder (replace with modeling later)\n",
    "                X.to_csv(f\"{output_dir}/X_imp{k+1}.csv\", index=False)\n",
    "                Y.to_frame(name=\"Y\").to_csv(f\"{output_dir}/Y_imp{k+1}.csv\", index=False)\n",
    "\n",
    "            print(f\" Done: {group_name}\")\n",
    "\n",
    "    print(\"\\n All SUBSUBCAT group analyses complete.\")\n",
    "\n",
    "# ========= STEP 4: Execute ========= #\n",
    "run_all_SUBSUBCAT_group_models(imputed_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a3f192e5-b4a6-42ed-98ee-0183e20212b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " SubSubCat_Oxazepam\n",
      "  Imp 1: Treated = 291, Control = 5834, Missing = 0\n",
      "  Imp 2: Treated = 291, Control = 5834, Missing = 0\n",
      "  Imp 3: Treated = 291, Control = 5834, Missing = 0\n",
      "  Imp 4: Treated = 291, Control = 5834, Missing = 0\n",
      "  Imp 5: Treated = 291, Control = 5834, Missing = 0\n",
      "\n",
      " SubSubCat_Diazepam\n",
      "  Imp 1: Treated = 55, Control = 6070, Missing = 0\n",
      "  Imp 2: Treated = 55, Control = 6070, Missing = 0\n",
      "  Imp 3: Treated = 55, Control = 6070, Missing = 0\n",
      "  Imp 4: Treated = 55, Control = 6070, Missing = 0\n",
      "  Imp 5: Treated = 55, Control = 6070, Missing = 0\n",
      "\n",
      " SubSubCat_Paracetamol\n",
      "  Imp 1: Treated = 67, Control = 6058, Missing = 0\n",
      "  Imp 2: Treated = 67, Control = 6058, Missing = 0\n",
      "  Imp 3: Treated = 67, Control = 6058, Missing = 0\n",
      "  Imp 4: Treated = 67, Control = 6058, Missing = 0\n",
      "  Imp 5: Treated = 67, Control = 6058, Missing = 0\n",
      "\n",
      " SubSubCat_Lorazepam\n",
      "  Imp 1: Treated = 95, Control = 6030, Missing = 0\n",
      "  Imp 2: Treated = 95, Control = 6030, Missing = 0\n",
      "  Imp 3: Treated = 95, Control = 6030, Missing = 0\n",
      "  Imp 4: Treated = 95, Control = 6030, Missing = 0\n",
      "  Imp 5: Treated = 95, Control = 6030, Missing = 0\n",
      "\n",
      " SubSubCat_Mirtazapine\n",
      "  Imp 1: Treated = 115, Control = 6010, Missing = 0\n",
      "  Imp 2: Treated = 115, Control = 6010, Missing = 0\n",
      "  Imp 3: Treated = 115, Control = 6010, Missing = 0\n",
      "  Imp 4: Treated = 115, Control = 6010, Missing = 0\n",
      "  Imp 5: Treated = 115, Control = 6010, Missing = 0\n",
      "\n",
      " SubSubCat_Escitalopram\n",
      "  Imp 1: Treated = 109, Control = 6016, Missing = 0\n",
      "  Imp 2: Treated = 109, Control = 6016, Missing = 0\n",
      "  Imp 3: Treated = 109, Control = 6016, Missing = 0\n",
      "  Imp 4: Treated = 109, Control = 6016, Missing = 0\n",
      "  Imp 5: Treated = 109, Control = 6016, Missing = 0\n",
      "\n",
      " SubSubCat_Sertraline\n",
      "  Imp 1: Treated = 141, Control = 5984, Missing = 0\n",
      "  Imp 2: Treated = 141, Control = 5984, Missing = 0\n",
      "  Imp 3: Treated = 141, Control = 5984, Missing = 0\n",
      "  Imp 4: Treated = 141, Control = 5984, Missing = 0\n",
      "  Imp 5: Treated = 141, Control = 5984, Missing = 0\n",
      "\n",
      " SubSubCat_Temazepam\n",
      "  Imp 1: Treated = 123, Control = 6002, Missing = 0\n",
      "  Imp 2: Treated = 123, Control = 6002, Missing = 0\n",
      "  Imp 3: Treated = 123, Control = 6002, Missing = 0\n",
      "  Imp 4: Treated = 123, Control = 6002, Missing = 0\n",
      "  Imp 5: Treated = 123, Control = 6002, Missing = 0\n",
      "\n",
      " SubSubCat_Citalopram\n",
      "  Imp 1: Treated = 174, Control = 5951, Missing = 0\n",
      "  Imp 2: Treated = 174, Control = 5951, Missing = 0\n",
      "  Imp 3: Treated = 174, Control = 5951, Missing = 0\n",
      "  Imp 4: Treated = 174, Control = 5951, Missing = 0\n",
      "  Imp 5: Treated = 174, Control = 5951, Missing = 0\n",
      "\n",
      " SubSubCat_Quetiapine\n",
      "  Imp 1: Treated = 280, Control = 5845, Missing = 0\n",
      "  Imp 2: Treated = 280, Control = 5845, Missing = 0\n",
      "  Imp 3: Treated = 280, Control = 5845, Missing = 0\n",
      "  Imp 4: Treated = 280, Control = 5845, Missing = 0\n",
      "  Imp 5: Treated = 280, Control = 5845, Missing = 0\n",
      "\n",
      " SubSubCat_Amitriptyline\n",
      "  Imp 1: Treated = 70, Control = 6055, Missing = 0\n",
      "  Imp 2: Treated = 70, Control = 6055, Missing = 0\n",
      "  Imp 3: Treated = 70, Control = 6055, Missing = 0\n",
      "  Imp 4: Treated = 70, Control = 6055, Missing = 0\n",
      "  Imp 5: Treated = 70, Control = 6055, Missing = 0\n",
      "\n",
      " SubSubCat_Venlafaxine\n",
      "  Imp 1: Treated = 75, Control = 6050, Missing = 0\n",
      "  Imp 2: Treated = 75, Control = 6050, Missing = 0\n",
      "  Imp 3: Treated = 75, Control = 6050, Missing = 0\n",
      "  Imp 4: Treated = 75, Control = 6050, Missing = 0\n",
      "  Imp 5: Treated = 75, Control = 6050, Missing = 0\n",
      "\n",
      " SubSubCat_Fluoxetine\n",
      "  Imp 1: Treated = 91, Control = 6034, Missing = 0\n",
      "  Imp 2: Treated = 91, Control = 6034, Missing = 0\n",
      "  Imp 3: Treated = 91, Control = 6034, Missing = 0\n",
      "  Imp 4: Treated = 91, Control = 6034, Missing = 0\n",
      "  Imp 5: Treated = 91, Control = 6034, Missing = 0\n",
      "\n",
      " SubSubCat_Topiramaat\n",
      "  Imp 1: Treated = 65, Control = 6060, Missing = 0\n",
      "  Imp 2: Treated = 65, Control = 6060, Missing = 0\n",
      "  Imp 3: Treated = 65, Control = 6060, Missing = 0\n",
      "  Imp 4: Treated = 65, Control = 6060, Missing = 0\n",
      "  Imp 5: Treated = 65, Control = 6060, Missing = 0\n",
      "\n",
      " SubSubCat_Tramadol\n",
      "  Imp 1: Treated = 39, Control = 6086, Missing = 0\n",
      "  Imp 2: Treated = 39, Control = 6086, Missing = 0\n",
      "  Imp 3: Treated = 39, Control = 6086, Missing = 0\n",
      "  Imp 4: Treated = 39, Control = 6086, Missing = 0\n",
      "  Imp 5: Treated = 39, Control = 6086, Missing = 0\n",
      "\n",
      " SubSubCat_Zopiclon\n",
      "  Imp 1: Treated = 49, Control = 6076, Missing = 0\n",
      "  Imp 2: Treated = 49, Control = 6076, Missing = 0\n",
      "  Imp 3: Treated = 49, Control = 6076, Missing = 0\n",
      "  Imp 4: Treated = 49, Control = 6076, Missing = 0\n",
      "  Imp 5: Treated = 49, Control = 6076, Missing = 0\n",
      "\n",
      " SubSubCat_Loprazolam\n",
      "  Imp 1: Treated = 37, Control = 6088, Missing = 0\n",
      "  Imp 2: Treated = 37, Control = 6088, Missing = 0\n",
      "  Imp 3: Treated = 37, Control = 6088, Missing = 0\n",
      "  Imp 4: Treated = 37, Control = 6088, Missing = 0\n",
      "  Imp 5: Treated = 37, Control = 6088, Missing = 0\n",
      "\n",
      " SubSubCat_Alprazolam\n",
      "  Imp 1: Treated = 31, Control = 6094, Missing = 0\n",
      "  Imp 2: Treated = 31, Control = 6094, Missing = 0\n",
      "  Imp 3: Treated = 31, Control = 6094, Missing = 0\n",
      "  Imp 4: Treated = 31, Control = 6094, Missing = 0\n",
      "  Imp 5: Treated = 31, Control = 6094, Missing = 0\n",
      "\n",
      " SubSubCat_promethazine\n",
      "  Imp 1: Treated = 32, Control = 6093, Missing = 0\n",
      "  Imp 2: Treated = 32, Control = 6093, Missing = 0\n",
      "  Imp 3: Treated = 32, Control = 6093, Missing = 0\n",
      "  Imp 4: Treated = 32, Control = 6093, Missing = 0\n",
      "  Imp 5: Treated = 32, Control = 6093, Missing = 0\n",
      "\n",
      " SubSubCat_Paroxetine\n",
      "  Imp 1: Treated = 34, Control = 6091, Missing = 0\n",
      "  Imp 2: Treated = 34, Control = 6091, Missing = 0\n",
      "  Imp 3: Treated = 34, Control = 6091, Missing = 0\n",
      "  Imp 4: Treated = 34, Control = 6091, Missing = 0\n",
      "  Imp 5: Treated = 34, Control = 6091, Missing = 0\n",
      "\n",
      " SubSubCat_Bupropion\n",
      "  Imp 1: Treated = 44, Control = 6081, Missing = 0\n",
      "  Imp 2: Treated = 44, Control = 6081, Missing = 0\n",
      "  Imp 3: Treated = 44, Control = 6081, Missing = 0\n",
      "  Imp 4: Treated = 44, Control = 6081, Missing = 0\n",
      "  Imp 5: Treated = 44, Control = 6081, Missing = 0\n",
      "\n",
      " SubSubCat_Methylfenidaat\n",
      "  Imp 1: Treated = 59, Control = 6066, Missing = 0\n",
      "  Imp 2: Treated = 59, Control = 6066, Missing = 0\n",
      "  Imp 3: Treated = 59, Control = 6066, Missing = 0\n",
      "  Imp 4: Treated = 59, Control = 6066, Missing = 0\n",
      "  Imp 5: Treated = 59, Control = 6066, Missing = 0\n",
      "\n",
      " SubSubCat_Olanzapine\n",
      "  Imp 1: Treated = 46, Control = 6079, Missing = 0\n",
      "  Imp 2: Treated = 46, Control = 6079, Missing = 0\n",
      "  Imp 3: Treated = 46, Control = 6079, Missing = 0\n",
      "  Imp 4: Treated = 46, Control = 6079, Missing = 0\n",
      "  Imp 5: Treated = 46, Control = 6079, Missing = 0\n",
      "\n",
      " SubSubCat_Zolpidem\n",
      "  Imp 1: Treated = 44, Control = 6081, Missing = 0\n",
      "  Imp 2: Treated = 44, Control = 6081, Missing = 0\n",
      "  Imp 3: Treated = 44, Control = 6081, Missing = 0\n",
      "  Imp 4: Treated = 44, Control = 6081, Missing = 0\n",
      "  Imp 5: Treated = 44, Control = 6081, Missing = 0\n"
     ]
    }
   ],
   "source": [
    "for treatment_var in medication_groups:\n",
    "    print(f\"\\n {treatment_var}\")\n",
    "    \n",
    "    for i, df in enumerate(imputed_dfs):\n",
    "        if treatment_var not in df.columns:\n",
    "            print(f\"  Imp {i+1}:  Not found in columns.\")\n",
    "            continue\n",
    "\n",
    "        treated = (df[treatment_var] == 1).sum()\n",
    "        control = (df[treatment_var] == 0).sum()\n",
    "        missing = df[treatment_var].isna().sum()\n",
    "\n",
    "        print(f\"  Imp {i+1}: Treated = {treated}, Control = {control}, Missing = {missing}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1735200e-f051-4982-b69a-62345955e88e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Processing VIF for SubSubCat_Oxazepam\n",
      " ✅ Saved: outputs\\SubSubCat_Oxazepam/pooled_vif.csv\n",
      "\n",
      "🔍 Processing VIF for SubSubCat_Diazepam\n",
      " ✅ Saved: outputs\\SubSubCat_Diazepam/pooled_vif.csv\n",
      "\n",
      "🔍 Processing VIF for SubSubCat_Paracetamol\n",
      " ✅ Saved: outputs\\SubSubCat_Paracetamol/pooled_vif.csv\n",
      "\n",
      "🔍 Processing VIF for SubSubCat_Lorazepam\n",
      " ✅ Saved: outputs\\SubSubCat_Lorazepam/pooled_vif.csv\n",
      "\n",
      "🔍 Processing VIF for SubSubCat_Mirtazapine\n",
      " ✅ Saved: outputs\\SubSubCat_Mirtazapine/pooled_vif.csv\n",
      "\n",
      "🔍 Processing VIF for SubSubCat_Escitalopram\n",
      " ✅ Saved: outputs\\SubSubCat_Escitalopram/pooled_vif.csv\n",
      "\n",
      "🔍 Processing VIF for SubSubCat_Sertraline\n",
      " ✅ Saved: outputs\\SubSubCat_Sertraline/pooled_vif.csv\n",
      "\n",
      "🔍 Processing VIF for SubSubCat_Temazepam\n",
      " ✅ Saved: outputs\\SubSubCat_Temazepam/pooled_vif.csv\n",
      "\n",
      "🔍 Processing VIF for SubSubCat_Citalopram\n",
      " ✅ Saved: outputs\\SubSubCat_Citalopram/pooled_vif.csv\n",
      "\n",
      "🔍 Processing VIF for SubSubCat_Quetiapine\n",
      " ✅ Saved: outputs\\SubSubCat_Quetiapine/pooled_vif.csv\n",
      "\n",
      "🔍 Processing VIF for SubSubCat_Amitriptyline\n",
      " ✅ Saved: outputs\\SubSubCat_Amitriptyline/pooled_vif.csv\n",
      "\n",
      "🔍 Processing VIF for SubSubCat_Venlafaxine\n",
      " ✅ Saved: outputs\\SubSubCat_Venlafaxine/pooled_vif.csv\n",
      "\n",
      "🔍 Processing VIF for SubSubCat_Fluoxetine\n",
      " ✅ Saved: outputs\\SubSubCat_Fluoxetine/pooled_vif.csv\n",
      "\n",
      "🔍 Processing VIF for SubSubCat_Topiramaat\n",
      " ✅ Saved: outputs\\SubSubCat_Topiramaat/pooled_vif.csv\n",
      "\n",
      "🔍 Processing VIF for SubSubCat_Tramadol\n",
      " ✅ Saved: outputs\\SubSubCat_Tramadol/pooled_vif.csv\n",
      "\n",
      "🔍 Processing VIF for SubSubCat_Zopiclon\n",
      " ✅ Saved: outputs\\SubSubCat_Zopiclon/pooled_vif.csv\n",
      "\n",
      "🔍 Processing VIF for SubSubCat_Loprazolam\n",
      " ✅ Saved: outputs\\SubSubCat_Loprazolam/pooled_vif.csv\n",
      "\n",
      "🔍 Processing VIF for SubSubCat_Alprazolam\n",
      " ✅ Saved: outputs\\SubSubCat_Alprazolam/pooled_vif.csv\n",
      "\n",
      "🔍 Processing VIF for SubSubCat_promethazine\n",
      " ✅ Saved: outputs\\SubSubCat_promethazine/pooled_vif.csv\n",
      "\n",
      "🔍 Processing VIF for SubSubCat_Paroxetine\n",
      " ✅ Saved: outputs\\SubSubCat_Paroxetine/pooled_vif.csv\n",
      "\n",
      "🔍 Processing VIF for SubSubCat_Bupropion\n",
      " ✅ Saved: outputs\\SubSubCat_Bupropion/pooled_vif.csv\n",
      "\n",
      "🔍 Processing VIF for SubSubCat_Methylfenidaat\n",
      " ✅ Saved: outputs\\SubSubCat_Methylfenidaat/pooled_vif.csv\n",
      "\n",
      "🔍 Processing VIF for SubSubCat_Olanzapine\n",
      " ✅ Saved: outputs\\SubSubCat_Olanzapine/pooled_vif.csv\n",
      "\n",
      "🔍 Processing VIF for SubSubCat_Zolpidem\n",
      " ✅ Saved: outputs\\SubSubCat_Zolpidem/pooled_vif.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from collections import defaultdict\n",
    "\n",
    "# ✅ VIF computation function\n",
    "def compute_vif(X):\n",
    "    X = sm.add_constant(X, has_constant='add')\n",
    "    vif_df = pd.DataFrame()\n",
    "    vif_df[\"variable\"] = X.columns\n",
    "    vif_df[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "    return vif_df\n",
    "\n",
    "# ✅ Process each group\n",
    "for group in medication_groups:\n",
    "    print(f\"\\n🔍 Processing VIF for {group}\")\n",
    "\n",
    "    if group not in final_covariates_map:\n",
    "        print(f\" ⚠️ No covariates found for {group}. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    covariates = final_covariates_map[group]\n",
    "    vif_list = []\n",
    "\n",
    "    for i, df_imp in enumerate(imputed_dfs):\n",
    "        try:\n",
    "            X = df_imp[covariates].copy()\n",
    "            vif_df = compute_vif(X)\n",
    "            vif_df[\"imputation\"] = i + 1\n",
    "            vif_list.append(vif_df)\n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ Failed on imputation {i+1} for {group}: {e}\")\n",
    "\n",
    "    if vif_list:\n",
    "        all_vif = pd.concat(vif_list)\n",
    "        pooled_vif = all_vif.groupby(\"variable\")[\"VIF\"].mean().reset_index()\n",
    "        pooled_vif = pooled_vif.sort_values(by=\"VIF\", ascending=False)\n",
    "\n",
    "        output_folder = os.path.join(\"outputs\", group)\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "        pooled_vif.to_csv(os.path.join(output_folder, \"pooled_vif.csv\"), index=False)\n",
    "\n",
    "        print(f\" ✅ Saved: {output_folder}/pooled_vif.csv\")\n",
    "    else:\n",
    "        print(f\" ⚠️ Skipped {group}: No valid imputations.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "528186ed-26e4-4988-83b8-14883f9d0c3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Running PS estimation for SubSubCat_Oxazepam\n",
      "   Imp 1: AUC = 0.777, ROC saved.\n",
      "   Imp 2: AUC = 0.768, ROC saved.\n",
      "   Imp 3: AUC = 0.780, ROC saved.\n",
      "   Imp 4: AUC = 0.778, ROC saved.\n",
      "   Imp 5: AUC = 0.768, ROC saved.\n",
      " Composite PS + AUC saved for SubSubCat_Oxazepam\n",
      " Running PS estimation for SubSubCat_Diazepam\n",
      "   Imp 1: AUC = 0.715, ROC saved.\n",
      "   Imp 2: AUC = 0.720, ROC saved.\n",
      "   Imp 3: AUC = 0.730, ROC saved.\n",
      "   Imp 4: AUC = 0.716, ROC saved.\n",
      "   Imp 5: AUC = 0.732, ROC saved.\n",
      " Composite PS + AUC saved for SubSubCat_Diazepam\n",
      " Running PS estimation for SubSubCat_Paracetamol\n",
      "   Imp 1: AUC = 0.684, ROC saved.\n",
      "   Imp 2: AUC = 0.755, ROC saved.\n",
      "   Imp 3: AUC = 0.674, ROC saved.\n",
      "   Imp 4: AUC = 0.707, ROC saved.\n",
      "   Imp 5: AUC = 0.691, ROC saved.\n",
      " Composite PS + AUC saved for SubSubCat_Paracetamol\n",
      " Running PS estimation for SubSubCat_Lorazepam\n",
      "   Imp 1: AUC = 0.779, ROC saved.\n",
      "   Imp 2: AUC = 0.776, ROC saved.\n",
      "   Imp 3: AUC = 0.793, ROC saved.\n",
      "   Imp 4: AUC = 0.782, ROC saved.\n",
      "   Imp 5: AUC = 0.802, ROC saved.\n",
      " Composite PS + AUC saved for SubSubCat_Lorazepam\n",
      " Running PS estimation for SubSubCat_Mirtazapine\n",
      "   Imp 1: AUC = 0.682, ROC saved.\n",
      "   Imp 2: AUC = 0.691, ROC saved.\n",
      "   Imp 3: AUC = 0.695, ROC saved.\n",
      "   Imp 4: AUC = 0.691, ROC saved.\n",
      "   Imp 5: AUC = 0.688, ROC saved.\n",
      " Composite PS + AUC saved for SubSubCat_Mirtazapine\n",
      " Running PS estimation for SubSubCat_Escitalopram\n",
      "   Imp 1: AUC = 0.577, ROC saved.\n",
      "   Imp 2: AUC = 0.608, ROC saved.\n",
      "   Imp 3: AUC = 0.591, ROC saved.\n",
      "   Imp 4: AUC = 0.613, ROC saved.\n",
      "   Imp 5: AUC = 0.602, ROC saved.\n",
      " Composite PS + AUC saved for SubSubCat_Escitalopram\n",
      " Running PS estimation for SubSubCat_Sertraline\n",
      "   Imp 1: AUC = 0.790, ROC saved.\n",
      "   Imp 2: AUC = 0.794, ROC saved.\n",
      "   Imp 3: AUC = 0.791, ROC saved.\n",
      "   Imp 4: AUC = 0.799, ROC saved.\n",
      "   Imp 5: AUC = 0.798, ROC saved.\n",
      " Composite PS + AUC saved for SubSubCat_Sertraline\n",
      " Running PS estimation for SubSubCat_Temazepam\n",
      "   Imp 1: AUC = 0.637, ROC saved.\n",
      "   Imp 2: AUC = 0.637, ROC saved.\n",
      "   Imp 3: AUC = 0.665, ROC saved.\n",
      "   Imp 4: AUC = 0.653, ROC saved.\n",
      "   Imp 5: AUC = 0.647, ROC saved.\n",
      " Composite PS + AUC saved for SubSubCat_Temazepam\n",
      " Running PS estimation for SubSubCat_Citalopram\n",
      "   Imp 1: AUC = 0.737, ROC saved.\n",
      "   Imp 2: AUC = 0.729, ROC saved.\n",
      "   Imp 3: AUC = 0.731, ROC saved.\n",
      "   Imp 4: AUC = 0.713, ROC saved.\n",
      "   Imp 5: AUC = 0.731, ROC saved.\n",
      " Composite PS + AUC saved for SubSubCat_Citalopram\n",
      " Running PS estimation for SubSubCat_Quetiapine\n",
      "   Imp 1: AUC = 0.822, ROC saved.\n",
      "   Imp 2: AUC = 0.824, ROC saved.\n",
      "   Imp 3: AUC = 0.823, ROC saved.\n",
      "   Imp 4: AUC = 0.822, ROC saved.\n",
      "   Imp 5: AUC = 0.818, ROC saved.\n",
      " Composite PS + AUC saved for SubSubCat_Quetiapine\n",
      " Running PS estimation for SubSubCat_Amitriptyline\n",
      "   Imp 1: AUC = 0.642, ROC saved.\n",
      "   Imp 2: AUC = 0.611, ROC saved.\n",
      "   Imp 3: AUC = 0.631, ROC saved.\n",
      "   Imp 4: AUC = 0.654, ROC saved.\n",
      "   Imp 5: AUC = 0.636, ROC saved.\n",
      " Composite PS + AUC saved for SubSubCat_Amitriptyline\n",
      " Running PS estimation for SubSubCat_Venlafaxine\n",
      "   Imp 1: AUC = 0.834, ROC saved.\n",
      "   Imp 2: AUC = 0.827, ROC saved.\n",
      "   Imp 3: AUC = 0.810, ROC saved.\n",
      "   Imp 4: AUC = 0.831, ROC saved.\n",
      "   Imp 5: AUC = 0.833, ROC saved.\n",
      " Composite PS + AUC saved for SubSubCat_Venlafaxine\n",
      " Running PS estimation for SubSubCat_Fluoxetine\n",
      "   Imp 1: AUC = 0.812, ROC saved.\n",
      "   Imp 2: AUC = 0.810, ROC saved.\n",
      "   Imp 3: AUC = 0.808, ROC saved.\n",
      "   Imp 4: AUC = 0.807, ROC saved.\n",
      "   Imp 5: AUC = 0.811, ROC saved.\n",
      " Composite PS + AUC saved for SubSubCat_Fluoxetine\n",
      " Running PS estimation for SubSubCat_Topiramaat\n",
      "   Imp 1: AUC = 0.861, ROC saved.\n",
      "   Imp 2: AUC = 0.867, ROC saved.\n",
      "   Imp 3: AUC = 0.870, ROC saved.\n",
      "   Imp 4: AUC = 0.859, ROC saved.\n",
      "   Imp 5: AUC = 0.857, ROC saved.\n",
      " Composite PS + AUC saved for SubSubCat_Topiramaat\n",
      " Running PS estimation for SubSubCat_Tramadol\n",
      "   Imp 1: AUC = 0.756, ROC saved.\n",
      "   Imp 2: AUC = 0.764, ROC saved.\n",
      "   Imp 3: AUC = 0.728, ROC saved.\n",
      "   Imp 4: AUC = 0.765, ROC saved.\n",
      "   Imp 5: AUC = 0.740, ROC saved.\n",
      " Composite PS + AUC saved for SubSubCat_Tramadol\n",
      " Running PS estimation for SubSubCat_Zopiclon\n",
      "   Imp 1: AUC = 0.778, ROC saved.\n",
      "   Imp 2: AUC = 0.792, ROC saved.\n",
      "   Imp 3: AUC = 0.803, ROC saved.\n",
      "   Imp 4: AUC = 0.795, ROC saved.\n",
      "   Imp 5: AUC = 0.798, ROC saved.\n",
      " Composite PS + AUC saved for SubSubCat_Zopiclon\n",
      " Running PS estimation for SubSubCat_Loprazolam\n",
      "   Imp 1: AUC = 0.677, ROC saved.\n",
      "   Imp 2: AUC = 0.665, ROC saved.\n",
      "   Imp 3: AUC = 0.672, ROC saved.\n",
      "   Imp 4: AUC = 0.668, ROC saved.\n",
      "   Imp 5: AUC = 0.665, ROC saved.\n",
      " Composite PS + AUC saved for SubSubCat_Loprazolam\n",
      " Running PS estimation for SubSubCat_Alprazolam\n",
      "   Imp 1: AUC = 0.627, ROC saved.\n",
      "   Imp 2: AUC = 0.603, ROC saved.\n",
      "   Imp 3: AUC = 0.648, ROC saved.\n",
      "   Imp 4: AUC = 0.624, ROC saved.\n",
      "   Imp 5: AUC = 0.619, ROC saved.\n",
      " Composite PS + AUC saved for SubSubCat_Alprazolam\n",
      " Running PS estimation for SubSubCat_promethazine\n",
      "   Imp 1: AUC = 0.841, ROC saved.\n",
      "   Imp 2: AUC = 0.870, ROC saved.\n",
      "   Imp 3: AUC = 0.861, ROC saved.\n",
      "   Imp 4: AUC = 0.867, ROC saved.\n",
      "   Imp 5: AUC = 0.859, ROC saved.\n",
      " Composite PS + AUC saved for SubSubCat_promethazine\n",
      " Running PS estimation for SubSubCat_Paroxetine\n",
      "   Imp 1: AUC = 0.741, ROC saved.\n",
      "   Imp 2: AUC = 0.740, ROC saved.\n",
      "   Imp 3: AUC = 0.724, ROC saved.\n",
      "   Imp 4: AUC = 0.734, ROC saved.\n",
      "   Imp 5: AUC = 0.741, ROC saved.\n",
      " Composite PS + AUC saved for SubSubCat_Paroxetine\n",
      " Running PS estimation for SubSubCat_Bupropion\n",
      "   Imp 1: AUC = 0.609, ROC saved.\n",
      "   Imp 2: AUC = 0.631, ROC saved.\n",
      "   Imp 3: AUC = 0.631, ROC saved.\n",
      "   Imp 4: AUC = 0.638, ROC saved.\n",
      "   Imp 5: AUC = 0.633, ROC saved.\n",
      " Composite PS + AUC saved for SubSubCat_Bupropion\n",
      " Running PS estimation for SubSubCat_Methylfenidaat\n",
      "   Imp 1: AUC = 0.778, ROC saved.\n",
      "   Imp 2: AUC = 0.792, ROC saved.\n",
      "   Imp 3: AUC = 0.814, ROC saved.\n",
      "   Imp 4: AUC = 0.808, ROC saved.\n",
      "   Imp 5: AUC = 0.766, ROC saved.\n",
      " Composite PS + AUC saved for SubSubCat_Methylfenidaat\n",
      " Running PS estimation for SubSubCat_Olanzapine\n",
      "   Imp 1: AUC = 0.774, ROC saved.\n",
      "   Imp 2: AUC = 0.793, ROC saved.\n",
      "   Imp 3: AUC = 0.767, ROC saved.\n",
      "   Imp 4: AUC = 0.777, ROC saved.\n",
      "   Imp 5: AUC = 0.787, ROC saved.\n",
      " Composite PS + AUC saved for SubSubCat_Olanzapine\n",
      " Running PS estimation for SubSubCat_Zolpidem\n",
      "   Imp 1: AUC = 0.767, ROC saved.\n",
      "   Imp 2: AUC = 0.779, ROC saved.\n",
      "   Imp 3: AUC = 0.796, ROC saved.\n",
      "   Imp 4: AUC = 0.761, ROC saved.\n",
      "   Imp 5: AUC = 0.786, ROC saved.\n",
      " Composite PS + AUC saved for SubSubCat_Zolpidem\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ---------- PS Estimation Function ----------\n",
    "def run_logistic_ps_modeling(imputed_dfs, medication_groups, final_covariates_map):\n",
    "    for group in medication_groups:\n",
    "        print(f\" Running PS estimation for {group}\")\n",
    "\n",
    "        if group not in final_covariates_map:\n",
    "            print(f\" No covariates found for {group}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        output_folder = os.path.join(\"outputs\", group)\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "        covariates = final_covariates_map[group]\n",
    "        ps_matrix = pd.DataFrame()\n",
    "        auc_list = []\n",
    "\n",
    "        for i, df_imp in enumerate(imputed_dfs):\n",
    "            if group not in df_imp.columns:\n",
    "                print(f\" {group} not found in imputed dataset {i+1}. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            X = df_imp[covariates].copy()\n",
    "            T = df_imp[group]\n",
    "\n",
    "            # Drop missing treatment rows\n",
    "            valid_idx = T.dropna().index\n",
    "            X = X.loc[valid_idx]\n",
    "            T = T.loc[valid_idx]\n",
    "\n",
    "            # Train-test split for ROC\n",
    "            X_train, X_test, T_train, T_test = train_test_split(\n",
    "                X, T, stratify=T, test_size=0.3, random_state=42\n",
    "            )\n",
    "\n",
    "            try:\n",
    "                model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "                model.fit(X_train, T_train)\n",
    "\n",
    "                ps_scores = model.predict_proba(X)[:, 1]\n",
    "                ps_matrix[f\"ps_imp{i+1}\"] = pd.Series(ps_scores, index=valid_idx)\n",
    "\n",
    "                # ROC & AUC\n",
    "                auc = roc_auc_score(T_test, model.predict_proba(X_test)[:, 1])\n",
    "                auc_list.append(auc)\n",
    "\n",
    "                fpr, tpr, _ = roc_curve(T_test, model.predict_proba(X_test)[:, 1])\n",
    "                plt.figure()\n",
    "                plt.plot(fpr, tpr, label=f\"AUC = {auc:.3f}\")\n",
    "                plt.plot([0, 1], [0, 1], 'k--')\n",
    "                plt.xlabel(\"False Positive Rate\")\n",
    "                plt.ylabel(\"True Positive Rate\")\n",
    "                plt.title(f\"ROC Curve - {group} (Imp {i+1})\")\n",
    "                plt.legend()\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(os.path.join(output_folder, f\"roc_curve_imp{i+1}.png\"))\n",
    "                plt.close()\n",
    "                print(f\"   Imp {i+1}: AUC = {auc:.3f}, ROC saved.\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"   Error in {group} (imp {i+1}): {e}\")\n",
    "\n",
    "        # Save AUCs and Composite PS\n",
    "        if not ps_matrix.empty:\n",
    "            # Fill NaN rows (from dropped subjects in some imputations) with mean\n",
    "            ps_matrix[\"composite_ps\"] = ps_matrix.mean(axis=1)\n",
    "            ps_matrix.to_excel(os.path.join(output_folder, \"propensity_scores.xlsx\"))\n",
    "\n",
    "            auc_df = pd.DataFrame({\n",
    "                \"imputation\": [f\"imp{i+1}\" for i in range(len(auc_list))],\n",
    "                \"AUC\": auc_list\n",
    "            })\n",
    "            auc_df.loc[len(auc_df.index)] = [\"mean\", np.mean(auc_list) if auc_list else np.nan]\n",
    "            auc_df.to_excel(os.path.join(output_folder, \"auc_scores.xlsx\"), index=False)\n",
    "\n",
    "            print(f\" Composite PS + AUC saved for {group}\")\n",
    "        else:\n",
    "            print(f\" No valid PS scores generated for {group}\")\n",
    "\n",
    "# ---------- Run ----------\n",
    "run_logistic_ps_modeling(imputed_dfs, medication_groups, final_covariates_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b29752b0-b189-4d0c-92bf-17cbcf7a96d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Computing feature importance for SubSubCat_Oxazepam\n",
      " Saved feature importance plot and CSV for SubSubCat_Oxazepam\n",
      "\n",
      " Computing feature importance for SubSubCat_Diazepam\n",
      " Saved feature importance plot and CSV for SubSubCat_Diazepam\n",
      "\n",
      " Computing feature importance for SubSubCat_Paracetamol\n",
      " Saved feature importance plot and CSV for SubSubCat_Paracetamol\n",
      "\n",
      " Computing feature importance for SubSubCat_Lorazepam\n",
      " Saved feature importance plot and CSV for SubSubCat_Lorazepam\n",
      "\n",
      " Computing feature importance for SubSubCat_Mirtazapine\n",
      " Saved feature importance plot and CSV for SubSubCat_Mirtazapine\n",
      "\n",
      " Computing feature importance for SubSubCat_Escitalopram\n",
      " Saved feature importance plot and CSV for SubSubCat_Escitalopram\n",
      "\n",
      " Computing feature importance for SubSubCat_Sertraline\n",
      " Saved feature importance plot and CSV for SubSubCat_Sertraline\n",
      "\n",
      " Computing feature importance for SubSubCat_Temazepam\n",
      " Saved feature importance plot and CSV for SubSubCat_Temazepam\n",
      "\n",
      " Computing feature importance for SubSubCat_Citalopram\n",
      " Saved feature importance plot and CSV for SubSubCat_Citalopram\n",
      "\n",
      " Computing feature importance for SubSubCat_Quetiapine\n",
      " Saved feature importance plot and CSV for SubSubCat_Quetiapine\n",
      "\n",
      " Computing feature importance for SubSubCat_Amitriptyline\n",
      " Saved feature importance plot and CSV for SubSubCat_Amitriptyline\n",
      "\n",
      " Computing feature importance for SubSubCat_Venlafaxine\n",
      " Saved feature importance plot and CSV for SubSubCat_Venlafaxine\n",
      "\n",
      " Computing feature importance for SubSubCat_Fluoxetine\n",
      " Saved feature importance plot and CSV for SubSubCat_Fluoxetine\n",
      "\n",
      " Computing feature importance for SubSubCat_Topiramaat\n",
      " Saved feature importance plot and CSV for SubSubCat_Topiramaat\n",
      "\n",
      " Computing feature importance for SubSubCat_Tramadol\n",
      " Saved feature importance plot and CSV for SubSubCat_Tramadol\n",
      "\n",
      " Computing feature importance for SubSubCat_Zopiclon\n",
      " Saved feature importance plot and CSV for SubSubCat_Zopiclon\n",
      "\n",
      " Computing feature importance for SubSubCat_Loprazolam\n",
      " Saved feature importance plot and CSV for SubSubCat_Loprazolam\n",
      "\n",
      " Computing feature importance for SubSubCat_Alprazolam\n",
      " Saved feature importance plot and CSV for SubSubCat_Alprazolam\n",
      "\n",
      " Computing feature importance for SubSubCat_promethazine\n",
      " Saved feature importance plot and CSV for SubSubCat_promethazine\n",
      "\n",
      " Computing feature importance for SubSubCat_Paroxetine\n",
      " Saved feature importance plot and CSV for SubSubCat_Paroxetine\n",
      "\n",
      " Computing feature importance for SubSubCat_Bupropion\n",
      " Saved feature importance plot and CSV for SubSubCat_Bupropion\n",
      "\n",
      " Computing feature importance for SubSubCat_Methylfenidaat\n",
      " Saved feature importance plot and CSV for SubSubCat_Methylfenidaat\n",
      "\n",
      " Computing feature importance for SubSubCat_Olanzapine\n",
      " Saved feature importance plot and CSV for SubSubCat_Olanzapine\n",
      "\n",
      " Computing feature importance for SubSubCat_Zolpidem\n",
      " Saved feature importance plot and CSV for SubSubCat_Zolpidem\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def compute_feature_importance(imputed_dfs, medication_groups, final_covariates_map):\n",
    "    for group in medication_groups:\n",
    "        print(f\"\\n Computing feature importance for {group}\")\n",
    "\n",
    "        if group not in final_covariates_map:\n",
    "            print(f\" No covariates found for {group}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        covariates = final_covariates_map[group]\n",
    "        output_folder = os.path.join(\"outputs\", group)\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "        importance_df_list = []\n",
    "\n",
    "        for i, df_imp in enumerate(imputed_dfs):\n",
    "            if group not in df_imp.columns:\n",
    "                print(f\" {group} not in imputed dataset {i+1}. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            X = df_imp[covariates].copy()\n",
    "            T = df_imp[group]\n",
    "\n",
    "            # Drop NaNs in treatment\n",
    "            valid_idx = T.dropna().index\n",
    "            X = X.loc[valid_idx]\n",
    "            T = T.loc[valid_idx]\n",
    "\n",
    "            try:\n",
    "                model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "                model.fit(X, T)\n",
    "\n",
    "                # Get feature importance (absolute coefficients)\n",
    "                importances = np.abs(model.coef_[0])\n",
    "                importance_dict = dict(zip(X.columns, importances))\n",
    "                df_feat = pd.DataFrame.from_dict(importance_dict, orient='index', columns=[f\"imp{i+1}\"])\n",
    "                df_feat.index.name = 'feature'\n",
    "                importance_df_list.append(df_feat)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"   Error during modeling: {e}\")\n",
    "\n",
    "        if importance_df_list:\n",
    "            # Combine and average\n",
    "            all_feat = pd.concat(importance_df_list, axis=1).fillna(0)\n",
    "            all_feat[\"mean_importance\"] = all_feat.mean(axis=1)\n",
    "\n",
    "            # Filter top 30 non-zero\n",
    "            non_zero = all_feat[all_feat[\"mean_importance\"] > 0]\n",
    "            top30 = non_zero.sort_values(by=\"mean_importance\", ascending=False).head(30)\n",
    "\n",
    "            # Save to CSV\n",
    "            top30.to_csv(os.path.join(output_folder, \"feature_importance.csv\"))\n",
    "\n",
    "            # Plot\n",
    "            plt.figure(figsize=(10, 8))\n",
    "            plt.barh(top30.index[::-1], top30[\"mean_importance\"][::-1])  # plot top → bottom\n",
    "            plt.xlabel(\"Mean Gain Importance\")\n",
    "            plt.title(f\"Top 30 Feature Importance - {group}\")\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(output_folder, \"feature_importance_top30.png\"))\n",
    "            plt.close()\n",
    "\n",
    "            print(f\" Saved feature importance plot and CSV for {group}\")\n",
    "        else:\n",
    "            print(f\" No valid models for {group}\")\n",
    "\n",
    "#  Run\n",
    "compute_feature_importance(imputed_dfs, medication_groups, final_covariates_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "aec84d68-8957-4d30-b509-c6fe2224a9f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Processing IPTW + trimming + clipping for SubSubCat_Oxazepam\n",
      "✅ Saved IPTW weights for SubSubCat_Oxazepam\n",
      "    ℹ️ Retained 1533/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SubSubCat_Oxazepam/trimmed_data_imp1.*\n",
      "    ℹ️ Retained 1535/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SubSubCat_Oxazepam/trimmed_data_imp2.*\n",
      "    ℹ️ Retained 1543/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SubSubCat_Oxazepam/trimmed_data_imp3.*\n",
      "    ℹ️ Retained 1523/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SubSubCat_Oxazepam/trimmed_data_imp4.*\n",
      "    ℹ️ Retained 1516/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SubSubCat_Oxazepam/trimmed_data_imp5.*\n",
      "\n",
      "🔍 Processing IPTW + trimming + clipping for SubSubCat_Diazepam\n",
      "✅ Saved IPTW weights for SubSubCat_Diazepam\n",
      "    ℹ️ Retained 133/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SubSubCat_Diazepam/trimmed_data_imp1.*\n",
      "    ℹ️ Retained 120/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SubSubCat_Diazepam/trimmed_data_imp2.*\n",
      "    ℹ️ Retained 116/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SubSubCat_Diazepam/trimmed_data_imp3.*\n",
      "    ℹ️ Retained 115/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SubSubCat_Diazepam/trimmed_data_imp4.*\n",
      "    ℹ️ Retained 101/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SubSubCat_Diazepam/trimmed_data_imp5.*\n",
      "\n",
      "🔍 Processing IPTW + trimming + clipping for SubSubCat_Paracetamol\n",
      "✅ Saved IPTW weights for SubSubCat_Paracetamol\n",
      "    ℹ️ Retained 155/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SubSubCat_Paracetamol/trimmed_data_imp1.*\n",
      "    ℹ️ Retained 160/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SubSubCat_Paracetamol/trimmed_data_imp2.*\n",
      "    ℹ️ Retained 156/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SubSubCat_Paracetamol/trimmed_data_imp3.*\n",
      "    ℹ️ Retained 152/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SubSubCat_Paracetamol/trimmed_data_imp4.*\n",
      "    ℹ️ Retained 150/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SubSubCat_Paracetamol/trimmed_data_imp5.*\n",
      "\n",
      "🔍 Processing IPTW + trimming + clipping for SubSubCat_Lorazepam\n",
      "✅ Saved IPTW weights for SubSubCat_Lorazepam\n",
      "    ℹ️ Retained 366/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SubSubCat_Lorazepam/trimmed_data_imp1.*\n",
      "    ℹ️ Retained 361/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SubSubCat_Lorazepam/trimmed_data_imp2.*\n",
      "    ℹ️ Retained 370/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SubSubCat_Lorazepam/trimmed_data_imp3.*\n",
      "    ℹ️ Retained 372/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SubSubCat_Lorazepam/trimmed_data_imp4.*\n",
      "    ℹ️ Retained 347/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SubSubCat_Lorazepam/trimmed_data_imp5.*\n",
      "\n",
      "🔍 Processing IPTW + trimming + clipping for SubSubCat_Mirtazapine\n",
      "✅ Saved IPTW weights for SubSubCat_Mirtazapine\n",
      "    ℹ️ Retained 419/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SubSubCat_Mirtazapine/trimmed_data_imp1.*\n",
      "    ℹ️ Retained 405/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SubSubCat_Mirtazapine/trimmed_data_imp2.*\n",
      "    ℹ️ Retained 418/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SubSubCat_Mirtazapine/trimmed_data_imp3.*\n",
      "    ℹ️ Retained 423/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SubSubCat_Mirtazapine/trimmed_data_imp4.*\n",
      "    ℹ️ Retained 437/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SubSubCat_Mirtazapine/trimmed_data_imp5.*\n",
      "\n",
      "🔍 Processing IPTW + trimming + clipping for SubSubCat_Escitalopram\n",
      "✅ Saved IPTW weights for SubSubCat_Escitalopram\n",
      "    ℹ️ Retained 257/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SubSubCat_Escitalopram/trimmed_data_imp1.*\n",
      "    ℹ️ Retained 270/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SubSubCat_Escitalopram/trimmed_data_imp2.*\n",
      "    ℹ️ Retained 267/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SubSubCat_Escitalopram/trimmed_data_imp3.*\n",
      "    ℹ️ Retained 249/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SubSubCat_Escitalopram/trimmed_data_imp4.*\n",
      "    ℹ️ Retained 236/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SubSubCat_Escitalopram/trimmed_data_imp5.*\n",
      "\n",
      "🔍 Processing IPTW + trimming + clipping for SubSubCat_Sertraline\n",
      "✅ Saved IPTW weights for SubSubCat_Sertraline\n",
      "    ℹ️ Retained 586/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SubSubCat_Sertraline/trimmed_data_imp1.*\n",
      "    ℹ️ Retained 576/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SubSubCat_Sertraline/trimmed_data_imp2.*\n",
      "    ℹ️ Retained 583/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SubSubCat_Sertraline/trimmed_data_imp3.*\n",
      "    ℹ️ Retained 572/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SubSubCat_Sertraline/trimmed_data_imp4.*\n",
      "    ℹ️ Retained 592/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SubSubCat_Sertraline/trimmed_data_imp5.*\n",
      "\n",
      "🔍 Processing IPTW + trimming + clipping for SubSubCat_Temazepam\n",
      "✅ Saved IPTW weights for SubSubCat_Temazepam\n",
      "    ℹ️ Retained 515/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SubSubCat_Temazepam/trimmed_data_imp1.*\n",
      "    ℹ️ Retained 526/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SubSubCat_Temazepam/trimmed_data_imp2.*\n",
      "    ℹ️ Retained 514/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SubSubCat_Temazepam/trimmed_data_imp3.*\n",
      "    ℹ️ Retained 525/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SubSubCat_Temazepam/trimmed_data_imp4.*\n",
      "    ℹ️ Retained 524/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SubSubCat_Temazepam/trimmed_data_imp5.*\n",
      "\n",
      "🔍 Processing IPTW + trimming + clipping for SubSubCat_Citalopram\n",
      "✅ Saved IPTW weights for SubSubCat_Citalopram\n",
      "    ℹ️ Retained 925/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SubSubCat_Citalopram/trimmed_data_imp1.*\n",
      "    ℹ️ Retained 961/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SubSubCat_Citalopram/trimmed_data_imp2.*\n",
      "    ℹ️ Retained 917/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SubSubCat_Citalopram/trimmed_data_imp3.*\n",
      "    ℹ️ Retained 922/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SubSubCat_Citalopram/trimmed_data_imp4.*\n",
      "    ℹ️ Retained 888/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SubSubCat_Citalopram/trimmed_data_imp5.*\n",
      "\n",
      "🔍 Processing IPTW + trimming + clipping for SubSubCat_Quetiapine\n",
      "✅ Saved IPTW weights for SubSubCat_Quetiapine\n",
      "    ℹ️ Retained 1315/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SubSubCat_Quetiapine/trimmed_data_imp1.*\n",
      "    ℹ️ Retained 1305/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SubSubCat_Quetiapine/trimmed_data_imp2.*\n",
      "    ℹ️ Retained 1313/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SubSubCat_Quetiapine/trimmed_data_imp3.*\n",
      "    ℹ️ Retained 1331/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SubSubCat_Quetiapine/trimmed_data_imp4.*\n",
      "    ℹ️ Retained 1322/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SubSubCat_Quetiapine/trimmed_data_imp5.*\n",
      "\n",
      "🔍 Processing IPTW + trimming + clipping for SubSubCat_Amitriptyline\n",
      "✅ Saved IPTW weights for SubSubCat_Amitriptyline\n",
      "    ℹ️ Retained 99/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SubSubCat_Amitriptyline/trimmed_data_imp1.*\n",
      "    ℹ️ Retained 119/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SubSubCat_Amitriptyline/trimmed_data_imp2.*\n",
      "    ℹ️ Retained 97/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SubSubCat_Amitriptyline/trimmed_data_imp3.*\n",
      "    ℹ️ Retained 101/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SubSubCat_Amitriptyline/trimmed_data_imp4.*\n",
      "    ℹ️ Retained 107/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SubSubCat_Amitriptyline/trimmed_data_imp5.*\n",
      "\n",
      "🔍 Processing IPTW + trimming + clipping for SubSubCat_Venlafaxine\n",
      "✅ Saved IPTW weights for SubSubCat_Venlafaxine\n",
      "    ℹ️ Retained 186/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SubSubCat_Venlafaxine/trimmed_data_imp1.*\n",
      "    ℹ️ Retained 193/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SubSubCat_Venlafaxine/trimmed_data_imp2.*\n",
      "    ℹ️ Retained 211/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SubSubCat_Venlafaxine/trimmed_data_imp3.*\n",
      "    ℹ️ Retained 198/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SubSubCat_Venlafaxine/trimmed_data_imp4.*\n",
      "    ℹ️ Retained 195/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SubSubCat_Venlafaxine/trimmed_data_imp5.*\n",
      "\n",
      "🔍 Processing IPTW + trimming + clipping for SubSubCat_Fluoxetine\n",
      "✅ Saved IPTW weights for SubSubCat_Fluoxetine\n",
      "    ℹ️ Retained 326/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SubSubCat_Fluoxetine/trimmed_data_imp1.*\n",
      "    ℹ️ Retained 336/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SubSubCat_Fluoxetine/trimmed_data_imp2.*\n",
      "    ℹ️ Retained 310/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SubSubCat_Fluoxetine/trimmed_data_imp3.*\n",
      "    ℹ️ Retained 327/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SubSubCat_Fluoxetine/trimmed_data_imp4.*\n",
      "    ℹ️ Retained 315/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SubSubCat_Fluoxetine/trimmed_data_imp5.*\n",
      "\n",
      "🔍 Processing IPTW + trimming + clipping for SubSubCat_Topiramaat\n",
      "✅ Saved IPTW weights for SubSubCat_Topiramaat\n",
      "    ℹ️ Retained 239/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SubSubCat_Topiramaat/trimmed_data_imp1.*\n",
      "    ℹ️ Retained 240/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SubSubCat_Topiramaat/trimmed_data_imp2.*\n",
      "    ℹ️ Retained 253/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SubSubCat_Topiramaat/trimmed_data_imp3.*\n",
      "    ℹ️ Retained 246/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SubSubCat_Topiramaat/trimmed_data_imp4.*\n",
      "    ℹ️ Retained 238/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SubSubCat_Topiramaat/trimmed_data_imp5.*\n",
      "\n",
      "🔍 Processing IPTW + trimming + clipping for SubSubCat_Tramadol\n",
      "✅ Saved IPTW weights for SubSubCat_Tramadol\n",
      "    ℹ️ Retained 76/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SubSubCat_Tramadol/trimmed_data_imp1.*\n",
      "    ℹ️ Retained 85/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SubSubCat_Tramadol/trimmed_data_imp2.*\n",
      "    ℹ️ Retained 69/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SubSubCat_Tramadol/trimmed_data_imp3.*\n",
      "    ℹ️ Retained 80/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SubSubCat_Tramadol/trimmed_data_imp4.*\n",
      "    ℹ️ Retained 81/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SubSubCat_Tramadol/trimmed_data_imp5.*\n",
      "\n",
      "🔍 Processing IPTW + trimming + clipping for SubSubCat_Zopiclon\n",
      "✅ Saved IPTW weights for SubSubCat_Zopiclon\n",
      "    ℹ️ Retained 145/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SubSubCat_Zopiclon/trimmed_data_imp1.*\n",
      "    ℹ️ Retained 153/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SubSubCat_Zopiclon/trimmed_data_imp2.*\n",
      "    ℹ️ Retained 157/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SubSubCat_Zopiclon/trimmed_data_imp3.*\n",
      "    ℹ️ Retained 153/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SubSubCat_Zopiclon/trimmed_data_imp4.*\n",
      "    ℹ️ Retained 158/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SubSubCat_Zopiclon/trimmed_data_imp5.*\n",
      "\n",
      "🔍 Processing IPTW + trimming + clipping for SubSubCat_Loprazolam\n",
      "✅ Saved IPTW weights for SubSubCat_Loprazolam\n",
      "    ℹ️ Retained 93/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SubSubCat_Loprazolam/trimmed_data_imp1.*\n",
      "    ℹ️ Retained 96/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SubSubCat_Loprazolam/trimmed_data_imp2.*\n",
      "    ℹ️ Retained 96/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SubSubCat_Loprazolam/trimmed_data_imp3.*\n",
      "    ℹ️ Retained 94/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SubSubCat_Loprazolam/trimmed_data_imp4.*\n",
      "    ℹ️ Retained 96/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SubSubCat_Loprazolam/trimmed_data_imp5.*\n",
      "\n",
      "🔍 Processing IPTW + trimming + clipping for SubSubCat_Alprazolam\n",
      "✅ Saved IPTW weights for SubSubCat_Alprazolam\n",
      "    ℹ️ Retained 62/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SubSubCat_Alprazolam/trimmed_data_imp1.*\n",
      "    ℹ️ Retained 55/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SubSubCat_Alprazolam/trimmed_data_imp2.*\n",
      "    ℹ️ Retained 63/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SubSubCat_Alprazolam/trimmed_data_imp3.*\n",
      "    ℹ️ Retained 53/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SubSubCat_Alprazolam/trimmed_data_imp4.*\n",
      "    ℹ️ Retained 58/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SubSubCat_Alprazolam/trimmed_data_imp5.*\n",
      "\n",
      "🔍 Processing IPTW + trimming + clipping for SubSubCat_promethazine\n",
      "✅ Saved IPTW weights for SubSubCat_promethazine\n",
      "    ℹ️ Retained 111/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SubSubCat_promethazine/trimmed_data_imp1.*\n",
      "    ℹ️ Retained 114/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SubSubCat_promethazine/trimmed_data_imp2.*\n",
      "    ℹ️ Retained 115/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SubSubCat_promethazine/trimmed_data_imp3.*\n",
      "    ℹ️ Retained 115/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SubSubCat_promethazine/trimmed_data_imp4.*\n",
      "    ℹ️ Retained 111/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SubSubCat_promethazine/trimmed_data_imp5.*\n",
      "\n",
      "🔍 Processing IPTW + trimming + clipping for SubSubCat_Paroxetine\n",
      "✅ Saved IPTW weights for SubSubCat_Paroxetine\n",
      "    ℹ️ Retained 59/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SubSubCat_Paroxetine/trimmed_data_imp1.*\n",
      "    ℹ️ Retained 64/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SubSubCat_Paroxetine/trimmed_data_imp2.*\n",
      "    ℹ️ Retained 52/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SubSubCat_Paroxetine/trimmed_data_imp3.*\n",
      "    ℹ️ Retained 49/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SubSubCat_Paroxetine/trimmed_data_imp4.*\n",
      "    ℹ️ Retained 58/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SubSubCat_Paroxetine/trimmed_data_imp5.*\n",
      "\n",
      "🔍 Processing IPTW + trimming + clipping for SubSubCat_Bupropion\n",
      "✅ Saved IPTW weights for SubSubCat_Bupropion\n",
      "    ℹ️ Retained 55/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SubSubCat_Bupropion/trimmed_data_imp1.*\n",
      "    ℹ️ Retained 61/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SubSubCat_Bupropion/trimmed_data_imp2.*\n",
      "    ℹ️ Retained 48/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SubSubCat_Bupropion/trimmed_data_imp3.*\n",
      "    ℹ️ Retained 79/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SubSubCat_Bupropion/trimmed_data_imp4.*\n",
      "    ℹ️ Retained 67/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SubSubCat_Bupropion/trimmed_data_imp5.*\n",
      "\n",
      "🔍 Processing IPTW + trimming + clipping for SubSubCat_Methylfenidaat\n",
      "✅ Saved IPTW weights for SubSubCat_Methylfenidaat\n",
      "    ℹ️ Retained 90/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SubSubCat_Methylfenidaat/trimmed_data_imp1.*\n",
      "    ℹ️ Retained 87/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SubSubCat_Methylfenidaat/trimmed_data_imp2.*\n",
      "    ℹ️ Retained 77/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SubSubCat_Methylfenidaat/trimmed_data_imp3.*\n",
      "    ℹ️ Retained 80/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SubSubCat_Methylfenidaat/trimmed_data_imp4.*\n",
      "    ℹ️ Retained 98/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SubSubCat_Methylfenidaat/trimmed_data_imp5.*\n",
      "\n",
      "🔍 Processing IPTW + trimming + clipping for SubSubCat_Olanzapine\n",
      "✅ Saved IPTW weights for SubSubCat_Olanzapine\n",
      "    ℹ️ Retained 175/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SubSubCat_Olanzapine/trimmed_data_imp1.*\n",
      "    ℹ️ Retained 172/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SubSubCat_Olanzapine/trimmed_data_imp2.*\n",
      "    ℹ️ Retained 171/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SubSubCat_Olanzapine/trimmed_data_imp3.*\n",
      "    ℹ️ Retained 164/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SubSubCat_Olanzapine/trimmed_data_imp4.*\n",
      "    ℹ️ Retained 172/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SubSubCat_Olanzapine/trimmed_data_imp5.*\n",
      "\n",
      "🔍 Processing IPTW + trimming + clipping for SubSubCat_Zolpidem\n",
      "✅ Saved IPTW weights for SubSubCat_Zolpidem\n",
      "    ℹ️ Retained 169/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SubSubCat_Zolpidem/trimmed_data_imp1.*\n",
      "    ℹ️ Retained 170/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SubSubCat_Zolpidem/trimmed_data_imp2.*\n",
      "    ℹ️ Retained 168/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SubSubCat_Zolpidem/trimmed_data_imp3.*\n",
      "    ℹ️ Retained 172/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SubSubCat_Zolpidem/trimmed_data_imp4.*\n",
      "    ℹ️ Retained 170/6125 rows after IPTW NaN drop.\n",
      "  💾 Saved trimmed dataset: outputs\\SubSubCat_Zolpidem/trimmed_data_imp5.*\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def compute_trimmed_clipped_iptw(ps_df, treatment, lower=0.05, upper=0.95, clip_max=10):\n",
    "    weights = []\n",
    "    keep_mask = (ps_df > lower) & (ps_df < upper)\n",
    "\n",
    "    for i in range(ps_df.shape[1]):\n",
    "        ps = ps_df.iloc[:, i].clip(lower=1e-6, upper=1 - 1e-6)  # avoid div by zero\n",
    "        mask = keep_mask.iloc[:, i]\n",
    "        w = pd.Series(np.nan, index=ps.index)\n",
    "\n",
    "        w[mask & (treatment == 1)] = 1 / ps[mask & (treatment == 1)]\n",
    "        w[mask & (treatment == 0)] = 1 / (1 - ps[mask & (treatment == 0)])\n",
    "        w = w.clip(upper=clip_max)\n",
    "        weights.append(w)\n",
    "\n",
    "    return pd.concat(weights, axis=1)\n",
    "\n",
    "\n",
    "def apply_rubins_rule_to_iptw(iptw_matrix):\n",
    "    \"\"\"\n",
    "    Given an IPTW matrix (n rows × M imputations), return Rubin’s rule pooled mean, SD, SE.\n",
    "    \"\"\"\n",
    "    M = iptw_matrix.shape[1]\n",
    "    q_bar = iptw_matrix.mean(axis=1)\n",
    "    u_bar = iptw_matrix.var(axis=1, ddof=1)\n",
    "    B = iptw_matrix.apply(lambda x: x.mean(), axis=1).var(ddof=1)\n",
    "    total_var = u_bar + (1 + 1/M) * B\n",
    "    total_se = np.sqrt(total_var)\n",
    "    return q_bar, u_bar.pow(0.5), total_se\n",
    "\n",
    "\n",
    "def run_trim_clip_save_all(imputed_dfs, medication_groups, final_covariates_map):\n",
    "    for group in medication_groups:\n",
    "        print(f\"\\n🔍 Processing IPTW + trimming + clipping for {group}\")\n",
    "        output_folder = os.path.join(\"outputs\", group)\n",
    "        ps_path = os.path.join(output_folder, \"propensity_scores.xlsx\")\n",
    "\n",
    "        if not os.path.exists(ps_path):\n",
    "            print(f\"⚠️ Missing PS file: {ps_path}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            ps_all = pd.read_excel(ps_path, index_col=0)\n",
    "            ps_cols = [col for col in ps_all.columns if col.startswith(\"ps_imp\")]\n",
    "            composite_index = ps_all.index\n",
    "\n",
    "            # Get treatment from one imputed dataset\n",
    "            T_full = None\n",
    "            for df in imputed_dfs:\n",
    "                if group in df.columns:\n",
    "                    T_full = df.loc[composite_index, group]\n",
    "                    break\n",
    "\n",
    "            if T_full is None:\n",
    "                print(f\"❌ Treatment column {group} not found in any imputed dataset.\")\n",
    "                continue\n",
    "\n",
    "            # Compute IPTW matrix (shape: n × M)\n",
    "            iptw_matrix = compute_trimmed_clipped_iptw(ps_all[ps_cols], T_full)\n",
    "            iptw_matrix.columns = [f\"iptw_imp{i+1}\" for i in range(iptw_matrix.shape[1])]\n",
    "\n",
    "            # Apply Rubin’s Rule for mean, SD, SE\n",
    "            iptw_matrix[\"iptw_mean\"], iptw_matrix[\"iptw_sd\"], iptw_matrix[\"iptw_se\"] = apply_rubins_rule_to_iptw(\n",
    "                iptw_matrix[[f\"iptw_imp{i+1}\" for i in range(iptw_matrix.shape[1])]]\n",
    "            )\n",
    "\n",
    "            # Save IPTW matrix separately\n",
    "            iptw_matrix.to_excel(os.path.join(output_folder, \"iptw_weights.xlsx\"))\n",
    "            print(f\"✅ Saved IPTW weights for {group}\")\n",
    "\n",
    "            # Save trimmed & clipped imputed datasets with IPTW\n",
    "            for i in range(5):\n",
    "                df = imputed_dfs[i].copy()\n",
    "                if group not in df.columns:\n",
    "                    continue\n",
    "\n",
    "                trimmed_idx = iptw_matrix.index.intersection(df.index)\n",
    "                needed_cols = final_covariates_map[group] + [group, \"caps5_change_baseline\"]\n",
    "\n",
    "                # Select only necessary columns\n",
    "                df_trimmed = df.loc[trimmed_idx, needed_cols].copy()\n",
    "                df_trimmed[\"iptw\"] = iptw_matrix[f\"iptw_imp{i+1}\"].loc[trimmed_idx]\n",
    "\n",
    "                # ✅ DROP rows with missing IPTW values\n",
    "                before = len(df_trimmed)\n",
    "                df_trimmed = df_trimmed.dropna(subset=[\"iptw\"])\n",
    "                after = len(df_trimmed)\n",
    "                print(f\"    ℹ️ Retained {after}/{before} rows after IPTW NaN drop.\")\n",
    "\n",
    "                # Save to .pkl\n",
    "                df_trimmed.to_pickle(os.path.join(output_folder, f\"trimmed_data_imp{i+1}.pkl\"))\n",
    "                print(f\"  💾 Saved trimmed dataset: {output_folder}/trimmed_data_imp{i+1}.*\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error in {group}: {e}\")\n",
    "\n",
    "\n",
    "run_trim_clip_save_all(imputed_dfs, medication_groups, final_covariates_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "0fc51d56-a7dc-4735-b9a7-9f1645bd811f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Plotting PS overlap for SubSubCat_Oxazepam\n",
      "✅ Saved unweighted and weighted PS plots for SubSubCat_Oxazepam\n",
      "\n",
      "📊 Plotting PS overlap for SubSubCat_Diazepam\n",
      "✅ Saved unweighted and weighted PS plots for SubSubCat_Diazepam\n",
      "\n",
      "📊 Plotting PS overlap for SubSubCat_Paracetamol\n",
      "✅ Saved unweighted and weighted PS plots for SubSubCat_Paracetamol\n",
      "\n",
      "📊 Plotting PS overlap for SubSubCat_Lorazepam\n",
      "✅ Saved unweighted and weighted PS plots for SubSubCat_Lorazepam\n",
      "\n",
      "📊 Plotting PS overlap for SubSubCat_Mirtazapine\n",
      "✅ Saved unweighted and weighted PS plots for SubSubCat_Mirtazapine\n",
      "\n",
      "📊 Plotting PS overlap for SubSubCat_Escitalopram\n",
      "✅ Saved unweighted and weighted PS plots for SubSubCat_Escitalopram\n",
      "\n",
      "📊 Plotting PS overlap for SubSubCat_Sertraline\n",
      "✅ Saved unweighted and weighted PS plots for SubSubCat_Sertraline\n",
      "\n",
      "📊 Plotting PS overlap for SubSubCat_Temazepam\n",
      "✅ Saved unweighted and weighted PS plots for SubSubCat_Temazepam\n",
      "\n",
      "📊 Plotting PS overlap for SubSubCat_Citalopram\n",
      "✅ Saved unweighted and weighted PS plots for SubSubCat_Citalopram\n",
      "\n",
      "📊 Plotting PS overlap for SubSubCat_Quetiapine\n",
      "✅ Saved unweighted and weighted PS plots for SubSubCat_Quetiapine\n",
      "\n",
      "📊 Plotting PS overlap for SubSubCat_Amitriptyline\n",
      "✅ Saved unweighted and weighted PS plots for SubSubCat_Amitriptyline\n",
      "\n",
      "📊 Plotting PS overlap for SubSubCat_Venlafaxine\n",
      "✅ Saved unweighted and weighted PS plots for SubSubCat_Venlafaxine\n",
      "\n",
      "📊 Plotting PS overlap for SubSubCat_Fluoxetine\n",
      "✅ Saved unweighted and weighted PS plots for SubSubCat_Fluoxetine\n",
      "\n",
      "📊 Plotting PS overlap for SubSubCat_Topiramaat\n",
      "✅ Saved unweighted and weighted PS plots for SubSubCat_Topiramaat\n",
      "\n",
      "📊 Plotting PS overlap for SubSubCat_Tramadol\n",
      "✅ Saved unweighted and weighted PS plots for SubSubCat_Tramadol\n",
      "\n",
      "📊 Plotting PS overlap for SubSubCat_Zopiclon\n",
      "✅ Saved unweighted and weighted PS plots for SubSubCat_Zopiclon\n",
      "\n",
      "📊 Plotting PS overlap for SubSubCat_Loprazolam\n",
      "✅ Saved unweighted and weighted PS plots for SubSubCat_Loprazolam\n",
      "\n",
      "📊 Plotting PS overlap for SubSubCat_Alprazolam\n",
      "✅ Saved unweighted and weighted PS plots for SubSubCat_Alprazolam\n",
      "\n",
      "📊 Plotting PS overlap for SubSubCat_promethazine\n",
      "✅ Saved unweighted and weighted PS plots for SubSubCat_promethazine\n",
      "\n",
      "📊 Plotting PS overlap for SubSubCat_Paroxetine\n",
      "✅ Saved unweighted and weighted PS plots for SubSubCat_Paroxetine\n",
      "\n",
      "📊 Plotting PS overlap for SubSubCat_Bupropion\n",
      "✅ Saved unweighted and weighted PS plots for SubSubCat_Bupropion\n",
      "\n",
      "📊 Plotting PS overlap for SubSubCat_Methylfenidaat\n",
      "✅ Saved unweighted and weighted PS plots for SubSubCat_Methylfenidaat\n",
      "\n",
      "📊 Plotting PS overlap for SubSubCat_Olanzapine\n",
      "✅ Saved unweighted and weighted PS plots for SubSubCat_Olanzapine\n",
      "\n",
      "📊 Plotting PS overlap for SubSubCat_Zolpidem\n",
      "✅ Saved unweighted and weighted PS plots for SubSubCat_Zolpidem\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_ps_overlap_all_groups(medication_groups):\n",
    "    for group in medication_groups:\n",
    "        print(f\"\\n📊 Plotting PS overlap for {group}\")\n",
    "\n",
    "        folder = os.path.join(\"outputs\", group)\n",
    "        ps_file = os.path.join(folder, \"propensity_scores.xlsx\")\n",
    "        iptw_file = os.path.join(folder, \"iptw_weights.xlsx\")\n",
    "        trimmed_file = os.path.join(folder, \"trimmed_data_imp1.pkl\")\n",
    "\n",
    "        if not all(os.path.exists(f) for f in [ps_file, iptw_file, trimmed_file]):\n",
    "            print(f\"⚠️ Missing required files for {group}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            ps_df = pd.read_excel(ps_file, index_col=0)\n",
    "            iptw_df = pd.read_excel(iptw_file, index_col=0)\n",
    "            trimmed_df = pd.read_pickle(trimmed_file)\n",
    "\n",
    "            # Extract\n",
    "            ps = ps_df[\"composite_ps\"].reindex(trimmed_df.index)\n",
    "            w = iptw_df[\"iptw_mean\"].reindex(trimmed_df.index)\n",
    "            T = trimmed_df[group]\n",
    "\n",
    "            # Masks to remove NaNs\n",
    "            treated_mask = (T == 1) & ps.notna() & w.notna()\n",
    "            control_mask = (T == 0) & ps.notna() & w.notna()\n",
    "\n",
    "            treated = ps[treated_mask]\n",
    "            treated_w = w[treated_mask]\n",
    "\n",
    "            control = ps[control_mask]\n",
    "            control_w = w[control_mask]\n",
    "\n",
    "            # === Unweighted Plot ===\n",
    "            plt.figure(figsize=(8, 5))\n",
    "            plt.hist([treated, control], bins=25, label=[\"Treated\", \"Control\"], alpha=0.6)\n",
    "            plt.title(f\"Unweighted PS Overlap - {group}\")\n",
    "            plt.xlabel(\"Composite Propensity Score\")\n",
    "            plt.ylabel(\"Count\")\n",
    "            plt.legend()\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(folder, \"ps_overlap_unweighted.png\"))\n",
    "            plt.close()\n",
    "\n",
    "            # === Weighted Plot ===\n",
    "            plt.figure(figsize=(8, 5))\n",
    "            plt.hist([treated, control], bins=25, weights=[treated_w, control_w], label=[\"Treated\", \"Control\"], alpha=0.6)\n",
    "            plt.title(f\"Weighted PS Overlap - {group}\")\n",
    "            plt.xlabel(\"Composite Propensity Score\")\n",
    "            plt.ylabel(\"Weighted Count\")\n",
    "            plt.legend()\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(folder, \"ps_overlap_weighted.png\"))\n",
    "            plt.close()\n",
    "\n",
    "            print(f\"✅ Saved unweighted and weighted PS plots for {group}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error processing {group}: {e}\")\n",
    "\n",
    "# 🔁 Run\n",
    "plot_ps_overlap_all_groups(medication_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "9b3cfb7d-89a3-4646-a30c-59dfd41f5c51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ✅ Saved: outputs\\SubSubCat_Oxazepam\\four_panel_overlap_SubSubCat_Oxazepam.png\n",
      " ✅ Saved: outputs\\SubSubCat_Diazepam\\four_panel_overlap_SubSubCat_Diazepam.png\n",
      " ✅ Saved: outputs\\SubSubCat_Paracetamol\\four_panel_overlap_SubSubCat_Paracetamol.png\n",
      " ✅ Saved: outputs\\SubSubCat_Lorazepam\\four_panel_overlap_SubSubCat_Lorazepam.png\n",
      " ✅ Saved: outputs\\SubSubCat_Mirtazapine\\four_panel_overlap_SubSubCat_Mirtazapine.png\n",
      " ✅ Saved: outputs\\SubSubCat_Escitalopram\\four_panel_overlap_SubSubCat_Escitalopram.png\n",
      " ✅ Saved: outputs\\SubSubCat_Sertraline\\four_panel_overlap_SubSubCat_Sertraline.png\n",
      " ✅ Saved: outputs\\SubSubCat_Temazepam\\four_panel_overlap_SubSubCat_Temazepam.png\n",
      " ✅ Saved: outputs\\SubSubCat_Citalopram\\four_panel_overlap_SubSubCat_Citalopram.png\n",
      " ✅ Saved: outputs\\SubSubCat_Quetiapine\\four_panel_overlap_SubSubCat_Quetiapine.png\n",
      " ✅ Saved: outputs\\SubSubCat_Amitriptyline\\four_panel_overlap_SubSubCat_Amitriptyline.png\n",
      " ✅ Saved: outputs\\SubSubCat_Venlafaxine\\four_panel_overlap_SubSubCat_Venlafaxine.png\n",
      " ✅ Saved: outputs\\SubSubCat_Fluoxetine\\four_panel_overlap_SubSubCat_Fluoxetine.png\n",
      " ✅ Saved: outputs\\SubSubCat_Topiramaat\\four_panel_overlap_SubSubCat_Topiramaat.png\n",
      " ✅ Saved: outputs\\SubSubCat_Tramadol\\four_panel_overlap_SubSubCat_Tramadol.png\n",
      " ✅ Saved: outputs\\SubSubCat_Zopiclon\\four_panel_overlap_SubSubCat_Zopiclon.png\n",
      " ✅ Saved: outputs\\SubSubCat_Loprazolam\\four_panel_overlap_SubSubCat_Loprazolam.png\n",
      " ✅ Saved: outputs\\SubSubCat_Alprazolam\\four_panel_overlap_SubSubCat_Alprazolam.png\n",
      " ✅ Saved: outputs\\SubSubCat_promethazine\\four_panel_overlap_SubSubCat_promethazine.png\n",
      " ✅ Saved: outputs\\SubSubCat_Paroxetine\\four_panel_overlap_SubSubCat_Paroxetine.png\n",
      " ✅ Saved: outputs\\SubSubCat_Bupropion\\four_panel_overlap_SubSubCat_Bupropion.png\n",
      " ✅ Saved: outputs\\SubSubCat_Methylfenidaat\\four_panel_overlap_SubSubCat_Methylfenidaat.png\n",
      " ✅ Saved: outputs\\SubSubCat_Olanzapine\\four_panel_overlap_SubSubCat_Olanzapine.png\n",
      " ✅ Saved: outputs\\SubSubCat_Zolpidem\\four_panel_overlap_SubSubCat_Zolpidem.png\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set up base output folder\n",
    "output_base = \"outputs\"\n",
    "ps_file = \"propensity_scores.xlsx\"\n",
    "iptw_file = \"iptw_weights.xlsx\"\n",
    "trimmed_data_file = \"trimmed_data_imp1.pkl\"\n",
    "\n",
    "# Collect all treatment group folders\n",
    "groups = [g for g in medication_groups if os.path.isdir(os.path.join(output_base, g))]\n",
    "\n",
    "# Generate 4-panel overlap plots\n",
    "for group in groups:\n",
    "    group_path = os.path.join(output_base, group)\n",
    "    try:\n",
    "        # Load trimmed treatment info\n",
    "        trimmed_df = pd.read_pickle(os.path.join(group_path, trimmed_data_file))\n",
    "        index = trimmed_df.index\n",
    "\n",
    "        # Fix: case-insensitive match for treatment variable\n",
    "        possible_cols = [col for col in trimmed_df.columns if col.upper() == group.upper()]\n",
    "        if not possible_cols:\n",
    "            print(f\" Treatment variable {group} not found in {group}, skipping.\")\n",
    "            continue\n",
    "        treatment_var = possible_cols[0]\n",
    "        T = trimmed_df[treatment_var]\n",
    "\n",
    "        # Load composite PS (aligned to trimmed_df index)\n",
    "        ps_df = pd.read_excel(os.path.join(group_path, ps_file), index_col=0)\n",
    "        if 'composite_ps' not in ps_df.columns:\n",
    "            print(f\" Composite column missing in {ps_file}, skipping {group}.\")\n",
    "            continue\n",
    "        ps = ps_df.loc[index, 'composite_ps']\n",
    "\n",
    "        # Load IPTW weights (aligned to trimmed_df index)\n",
    "        weights_df = pd.read_excel(os.path.join(group_path, iptw_file), index_col=0)\n",
    "        if 'iptw_mean' not in weights_df.columns:\n",
    "            print(f\" IPTW weight column missing in {iptw_file}, skipping {group}.\")\n",
    "            continue\n",
    "        weights = weights_df.loc[index, 'iptw_mean']\n",
    "\n",
    "        # Prepare 4 datasets\n",
    "        raw_treated = ps[T == 1]\n",
    "        raw_control = ps[T == 0]\n",
    "        weighted_treated = (ps[T == 1], weights[T == 1])\n",
    "        weighted_control = (ps[T == 0], weights[T == 0])\n",
    "\n",
    "        # Create plot\n",
    "        fig, axs = plt.subplots(2, 2, figsize=(12, 8))\n",
    "        fig.suptitle(f\"Propensity Score Distribution - {group}\", fontsize=14)\n",
    "\n",
    "        axs[0, 0].hist(raw_treated, bins=20, alpha=0.7, color='blue')\n",
    "        axs[0, 0].set_title(\"Raw Treated\")\n",
    "\n",
    "        axs[0, 1].hist(raw_control, bins=20, alpha=0.7, color='green')\n",
    "        axs[0, 1].set_title(\"Raw Control\")\n",
    "\n",
    "        axs[1, 0].hist(weighted_treated[0], bins=20, weights=weighted_treated[1], alpha=0.7, color='blue')\n",
    "        axs[1, 0].set_title(\"Weighted Treated\")\n",
    "\n",
    "        axs[1, 1].hist(weighted_control[0], bins=20, weights=weighted_control[1], alpha=0.7, color='green')\n",
    "        axs[1, 1].set_title(\"Weighted Control\")\n",
    "\n",
    "        for ax in axs.flat:\n",
    "            ax.set_xlim(0, 1)\n",
    "            ax.set_xlabel(\"Propensity Score\")\n",
    "            ax.set_ylabel(\"Count\")\n",
    "\n",
    "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "\n",
    "        # Save figure\n",
    "        plot_path = os.path.join(group_path, f\"four_panel_overlap_{group}.png\")\n",
    "        plt.savefig(plot_path)\n",
    "        plt.close()\n",
    "        print(f\" ✅ Saved: {plot_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\" ❌ Error in {group}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "859bca6c-231a-474e-a286-a502ad718a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ATT calculation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f6a37efb-712b-4a73-aaa2-b62ca84f289c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weighted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "4214a077-9e99-4d3e-b9c6-12cf0486f5d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🚀 Running OLS for SubSubCat_Oxazepam\n",
      "✅ SubSubCat_Oxazepam | Seed 1: ATT = 0.0868, SE = 1.4032, p = 0.95366\n",
      "✅ SubSubCat_Oxazepam | Seed 2: ATT = 0.1236, SE = 1.5250, p = 0.93928\n",
      "✅ SubSubCat_Oxazepam | Seed 3: ATT = 0.6143, SE = 1.5027, p = 0.70363\n",
      "✅ SubSubCat_Oxazepam | Seed 4: ATT = 0.1286, SE = 1.5918, p = 0.93947\n",
      "✅ SubSubCat_Oxazepam | Seed 5: ATT = -0.3906, SE = 1.3402, p = 0.78518\n",
      "✅ SubSubCat_Oxazepam | Seed 6: ATT = 0.6141, SE = 1.4396, p = 0.69165\n",
      "✅ SubSubCat_Oxazepam | Seed 7: ATT = -0.0904, SE = 1.8898, p = 0.96414\n",
      "✅ SubSubCat_Oxazepam | Seed 8: ATT = 0.4791, SE = 1.4898, p = 0.76388\n",
      "✅ SubSubCat_Oxazepam | Seed 9: ATT = 0.7935, SE = 1.3179, p = 0.57959\n",
      "✅ SubSubCat_Oxazepam | Seed 10: ATT = 0.9925, SE = 1.3674, p = 0.50811\n",
      "📊 Diagnostic plots saved for SubSubCat_Oxazepam\n",
      "🏆 Best result for SubSubCat_Oxazepam → Seed 9 | SE = 1.3179\n",
      "\n",
      "🚀 Running OLS for SubSubCat_Diazepam\n",
      "✅ SubSubCat_Diazepam | Seed 1: ATT = -10.2416, SE = 5.4303, p = 0.13236\n",
      "✅ SubSubCat_Diazepam | Seed 2: ATT = -9.5676, SE = 6.1477, p = 0.19463\n",
      "✅ SubSubCat_Diazepam | Seed 3: ATT = -11.6093, SE = 8.0929, p = 0.22474\n",
      "✅ SubSubCat_Diazepam | Seed 4: ATT = -8.0255, SE = 5.7138, p = 0.23284\n",
      "✅ SubSubCat_Diazepam | Seed 5: ATT = -10.5111, SE = 9.0540, p = 0.31023\n",
      "✅ SubSubCat_Diazepam | Seed 6: ATT = -10.1660, SE = 4.5121, p = 0.08735\n",
      "✅ SubSubCat_Diazepam | Seed 7: ATT = -7.6370, SE = 4.7282, p = 0.18157\n",
      "✅ SubSubCat_Diazepam | Seed 8: ATT = -10.5216, SE = 4.7440, p = 0.09083\n",
      "✅ SubSubCat_Diazepam | Seed 9: ATT = -11.5062, SE = 6.8539, p = 0.16850\n",
      "✅ SubSubCat_Diazepam | Seed 10: ATT = -7.5257, SE = 6.4594, p = 0.30873\n",
      "📊 Diagnostic plots saved for SubSubCat_Diazepam\n",
      "🏆 Best result for SubSubCat_Diazepam → Seed 6 | SE = 4.5121\n",
      "\n",
      "🚀 Running OLS for SubSubCat_Paracetamol\n",
      "✅ SubSubCat_Paracetamol | Seed 1: ATT = 4.4903, SE = 4.6872, p = 0.39231\n",
      "✅ SubSubCat_Paracetamol | Seed 2: ATT = -0.9957, SE = 7.7449, p = 0.90391\n",
      "✅ SubSubCat_Paracetamol | Seed 3: ATT = 3.9787, SE = 5.1468, p = 0.48264\n",
      "✅ SubSubCat_Paracetamol | Seed 4: ATT = 4.4983, SE = 6.1638, p = 0.50595\n",
      "✅ SubSubCat_Paracetamol | Seed 5: ATT = 2.6284, SE = 7.1968, p = 0.73344\n",
      "✅ SubSubCat_Paracetamol | Seed 6: ATT = 5.8825, SE = 7.8154, p = 0.49352\n",
      "✅ SubSubCat_Paracetamol | Seed 7: ATT = 2.2011, SE = 8.5281, p = 0.80906\n",
      "✅ SubSubCat_Paracetamol | Seed 8: ATT = 4.7185, SE = 9.1619, p = 0.63370\n",
      "✅ SubSubCat_Paracetamol | Seed 9: ATT = 1.7678, SE = 6.9554, p = 0.81190\n",
      "✅ SubSubCat_Paracetamol | Seed 10: ATT = 6.3058, SE = 7.8038, p = 0.46438\n",
      "📊 Diagnostic plots saved for SubSubCat_Paracetamol\n",
      "🏆 Best result for SubSubCat_Paracetamol → Seed 1 | SE = 4.6872\n",
      "\n",
      "🚀 Running OLS for SubSubCat_Lorazepam\n",
      "✅ SubSubCat_Lorazepam | Seed 1: ATT = 3.6133, SE = 4.3155, p = 0.44954\n",
      "✅ SubSubCat_Lorazepam | Seed 2: ATT = 2.5652, SE = 4.3406, p = 0.58632\n",
      "✅ SubSubCat_Lorazepam | Seed 3: ATT = 3.5151, SE = 2.5484, p = 0.23989\n",
      "✅ SubSubCat_Lorazepam | Seed 4: ATT = 3.7233, SE = 2.6570, p = 0.23374\n",
      "✅ SubSubCat_Lorazepam | Seed 5: ATT = 2.9098, SE = 3.9234, p = 0.49949\n",
      "✅ SubSubCat_Lorazepam | Seed 6: ATT = 2.9363, SE = 3.0598, p = 0.39158\n",
      "✅ SubSubCat_Lorazepam | Seed 7: ATT = 3.9579, SE = 3.6589, p = 0.34023\n",
      "✅ SubSubCat_Lorazepam | Seed 8: ATT = 2.1386, SE = 3.1106, p = 0.52954\n",
      "✅ SubSubCat_Lorazepam | Seed 9: ATT = 4.2986, SE = 3.1851, p = 0.24847\n",
      "✅ SubSubCat_Lorazepam | Seed 10: ATT = 2.5330, SE = 3.2649, p = 0.48116\n",
      "📊 Diagnostic plots saved for SubSubCat_Lorazepam\n",
      "🏆 Best result for SubSubCat_Lorazepam → Seed 3 | SE = 2.5484\n",
      "\n",
      "🚀 Running OLS for SubSubCat_Mirtazapine\n",
      "✅ SubSubCat_Mirtazapine | Seed 1: ATT = 5.8818, SE = 3.5875, p = 0.17645\n",
      "✅ SubSubCat_Mirtazapine | Seed 2: ATT = 2.8027, SE = 3.7845, p = 0.50006\n",
      "✅ SubSubCat_Mirtazapine | Seed 3: ATT = 4.9566, SE = 3.0288, p = 0.17707\n",
      "✅ SubSubCat_Mirtazapine | Seed 4: ATT = 5.5095, SE = 3.9049, p = 0.23110\n",
      "✅ SubSubCat_Mirtazapine | Seed 5: ATT = 5.6271, SE = 3.2214, p = 0.15560\n",
      "✅ SubSubCat_Mirtazapine | Seed 6: ATT = 5.3987, SE = 3.8655, p = 0.23503\n",
      "✅ SubSubCat_Mirtazapine | Seed 7: ATT = 4.5643, SE = 4.5728, p = 0.37470\n",
      "✅ SubSubCat_Mirtazapine | Seed 8: ATT = 6.8108, SE = 3.9821, p = 0.16238\n",
      "✅ SubSubCat_Mirtazapine | Seed 9: ATT = 5.5092, SE = 4.7224, p = 0.30818\n",
      "✅ SubSubCat_Mirtazapine | Seed 10: ATT = 5.2162, SE = 3.5548, p = 0.21618\n",
      "📊 Diagnostic plots saved for SubSubCat_Mirtazapine\n",
      "🏆 Best result for SubSubCat_Mirtazapine → Seed 3 | SE = 3.0288\n",
      "\n",
      "🚀 Running OLS for SubSubCat_Escitalopram\n",
      "✅ SubSubCat_Escitalopram | Seed 1: ATT = 4.9398, SE = 7.0043, p = 0.51955\n",
      "✅ SubSubCat_Escitalopram | Seed 2: ATT = -3.2439, SE = 4.5343, p = 0.51390\n",
      "✅ SubSubCat_Escitalopram | Seed 3: ATT = 0.7460, SE = 6.5563, p = 0.91489\n",
      "✅ SubSubCat_Escitalopram | Seed 4: ATT = -0.2077, SE = 6.1398, p = 0.97463\n",
      "✅ SubSubCat_Escitalopram | Seed 5: ATT = -2.9416, SE = 4.1936, p = 0.52169\n",
      "✅ SubSubCat_Escitalopram | Seed 6: ATT = -0.9203, SE = 8.0872, p = 0.91488\n",
      "✅ SubSubCat_Escitalopram | Seed 7: ATT = 1.2807, SE = 6.3782, p = 0.85066\n",
      "✅ SubSubCat_Escitalopram | Seed 8: ATT = -0.0252, SE = 5.0278, p = 0.99623\n",
      "✅ SubSubCat_Escitalopram | Seed 9: ATT = 6.1057, SE = 6.6115, p = 0.40801\n",
      "✅ SubSubCat_Escitalopram | Seed 10: ATT = 0.1900, SE = 5.8287, p = 0.97555\n",
      "📊 Diagnostic plots saved for SubSubCat_Escitalopram\n",
      "🏆 Best result for SubSubCat_Escitalopram → Seed 5 | SE = 4.1936\n",
      "\n",
      "🚀 Running OLS for SubSubCat_Sertraline\n",
      "✅ SubSubCat_Sertraline | Seed 1: ATT = -0.3724, SE = 2.7649, p = 0.89936\n",
      "✅ SubSubCat_Sertraline | Seed 2: ATT = 0.8475, SE = 2.5747, p = 0.75856\n",
      "✅ SubSubCat_Sertraline | Seed 3: ATT = 0.5193, SE = 2.2404, p = 0.82809\n",
      "✅ SubSubCat_Sertraline | Seed 4: ATT = -0.1291, SE = 2.4638, p = 0.96071\n",
      "✅ SubSubCat_Sertraline | Seed 5: ATT = 0.7097, SE = 2.3514, p = 0.77784\n",
      "✅ SubSubCat_Sertraline | Seed 6: ATT = -1.7467, SE = 2.9381, p = 0.58417\n",
      "✅ SubSubCat_Sertraline | Seed 7: ATT = -0.3940, SE = 2.4322, p = 0.87916\n",
      "✅ SubSubCat_Sertraline | Seed 8: ATT = 0.0812, SE = 2.6162, p = 0.97673\n",
      "✅ SubSubCat_Sertraline | Seed 9: ATT = 0.3020, SE = 2.1330, p = 0.89425\n",
      "✅ SubSubCat_Sertraline | Seed 10: ATT = -1.1393, SE = 2.4282, p = 0.66337\n",
      "📊 Diagnostic plots saved for SubSubCat_Sertraline\n",
      "🏆 Best result for SubSubCat_Sertraline → Seed 9 | SE = 2.1330\n",
      "\n",
      "🚀 Running OLS for SubSubCat_Temazepam\n",
      "✅ SubSubCat_Temazepam | Seed 1: ATT = 4.0280, SE = 3.7710, p = 0.34563\n",
      "✅ SubSubCat_Temazepam | Seed 2: ATT = 2.8743, SE = 3.0974, p = 0.40595\n",
      "✅ SubSubCat_Temazepam | Seed 3: ATT = 1.8349, SE = 3.1745, p = 0.59425\n",
      "✅ SubSubCat_Temazepam | Seed 4: ATT = 1.3885, SE = 4.6421, p = 0.77976\n",
      "✅ SubSubCat_Temazepam | Seed 5: ATT = 2.3340, SE = 3.0486, p = 0.48660\n",
      "✅ SubSubCat_Temazepam | Seed 6: ATT = 2.7255, SE = 3.1651, p = 0.43774\n",
      "✅ SubSubCat_Temazepam | Seed 7: ATT = 2.0273, SE = 3.3834, p = 0.58133\n",
      "✅ SubSubCat_Temazepam | Seed 8: ATT = 2.4584, SE = 2.9221, p = 0.44752\n",
      "✅ SubSubCat_Temazepam | Seed 9: ATT = 4.0978, SE = 2.5728, p = 0.18643\n",
      "✅ SubSubCat_Temazepam | Seed 10: ATT = 1.6389, SE = 2.8824, p = 0.60004\n",
      "📊 Diagnostic plots saved for SubSubCat_Temazepam\n",
      "🏆 Best result for SubSubCat_Temazepam → Seed 9 | SE = 2.5728\n",
      "\n",
      "🚀 Running OLS for SubSubCat_Citalopram\n",
      "✅ SubSubCat_Citalopram | Seed 1: ATT = -3.7741, SE = 1.7906, p = 0.10275\n",
      "✅ SubSubCat_Citalopram | Seed 2: ATT = -2.8072, SE = 2.6150, p = 0.34350\n",
      "✅ SubSubCat_Citalopram | Seed 3: ATT = -2.8576, SE = 2.1083, p = 0.24678\n",
      "✅ SubSubCat_Citalopram | Seed 4: ATT = -4.9812, SE = 2.1126, p = 0.07785\n",
      "✅ SubSubCat_Citalopram | Seed 5: ATT = -2.6657, SE = 1.6250, p = 0.17627\n",
      "✅ SubSubCat_Citalopram | Seed 6: ATT = -4.2183, SE = 2.2242, p = 0.13076\n",
      "✅ SubSubCat_Citalopram | Seed 7: ATT = -2.6308, SE = 2.0638, p = 0.27142\n",
      "✅ SubSubCat_Citalopram | Seed 8: ATT = -4.3415, SE = 2.0432, p = 0.10078\n",
      "✅ SubSubCat_Citalopram | Seed 9: ATT = -2.8708, SE = 1.6588, p = 0.15855\n",
      "✅ SubSubCat_Citalopram | Seed 10: ATT = -3.8253, SE = 3.0838, p = 0.28260\n",
      "📊 Diagnostic plots saved for SubSubCat_Citalopram\n",
      "🏆 Best result for SubSubCat_Citalopram → Seed 5 | SE = 1.6250\n",
      "\n",
      "🚀 Running OLS for SubSubCat_Quetiapine\n",
      "✅ SubSubCat_Quetiapine | Seed 1: ATT = 2.7391, SE = 1.7206, p = 0.18661\n",
      "✅ SubSubCat_Quetiapine | Seed 2: ATT = 1.4940, SE = 2.7152, p = 0.61144\n",
      "✅ SubSubCat_Quetiapine | Seed 3: ATT = 1.8702, SE = 1.7169, p = 0.33726\n",
      "✅ SubSubCat_Quetiapine | Seed 4: ATT = 1.5640, SE = 1.8808, p = 0.45241\n",
      "✅ SubSubCat_Quetiapine | Seed 5: ATT = 2.7305, SE = 1.2495, p = 0.09418\n",
      "✅ SubSubCat_Quetiapine | Seed 6: ATT = 2.2842, SE = 2.1304, p = 0.34402\n",
      "✅ SubSubCat_Quetiapine | Seed 7: ATT = 2.2531, SE = 1.6772, p = 0.25029\n",
      "✅ SubSubCat_Quetiapine | Seed 8: ATT = 2.3103, SE = 2.6144, p = 0.42678\n",
      "✅ SubSubCat_Quetiapine | Seed 9: ATT = 1.7733, SE = 1.6141, p = 0.33362\n",
      "✅ SubSubCat_Quetiapine | Seed 10: ATT = 2.2710, SE = 1.6943, p = 0.25119\n",
      "📊 Diagnostic plots saved for SubSubCat_Quetiapine\n",
      "🏆 Best result for SubSubCat_Quetiapine → Seed 5 | SE = 1.2495\n",
      "\n",
      "🚀 Running OLS for SubSubCat_Amitriptyline\n",
      "✅ SubSubCat_Amitriptyline | Seed 1: ATT = 11.6949, SE = 15.0165, p = 0.47960\n",
      "✅ SubSubCat_Amitriptyline | Seed 2: ATT = 9.2040, SE = 11.6615, p = 0.47411\n",
      "✅ SubSubCat_Amitriptyline | Seed 3: ATT = 10.5759, SE = 11.0452, p = 0.39253\n",
      "✅ SubSubCat_Amitriptyline | Seed 4: ATT = 12.0149, SE = 7.0078, p = 0.16159\n",
      "✅ SubSubCat_Amitriptyline | Seed 5: ATT = 15.3216, SE = 10.3990, p = 0.21465\n",
      "✅ SubSubCat_Amitriptyline | Seed 6: ATT = 12.8035, SE = 9.2768, p = 0.23965\n",
      "✅ SubSubCat_Amitriptyline | Seed 7: ATT = 11.6956, SE = 5.8566, p = 0.11652\n",
      "✅ SubSubCat_Amitriptyline | Seed 8: ATT = 13.0774, SE = 7.0480, p = 0.13712\n",
      "✅ SubSubCat_Amitriptyline | Seed 9: ATT = 6.3154, SE = 10.4487, p = 0.57817\n",
      "✅ SubSubCat_Amitriptyline | Seed 10: ATT = 8.3589, SE = 8.2754, p = 0.36959\n",
      "📊 Diagnostic plots saved for SubSubCat_Amitriptyline\n",
      "🏆 Best result for SubSubCat_Amitriptyline → Seed 7 | SE = 5.8566\n",
      "\n",
      "🚀 Running OLS for SubSubCat_Venlafaxine\n",
      "✅ SubSubCat_Venlafaxine | Seed 1: ATT = 9.7179, SE = 4.5127, p = 0.09760\n",
      "✅ SubSubCat_Venlafaxine | Seed 2: ATT = 8.9590, SE = 6.4340, p = 0.23620\n",
      "✅ SubSubCat_Venlafaxine | Seed 3: ATT = 14.4608, SE = 6.7873, p = 0.10014\n",
      "✅ SubSubCat_Venlafaxine | Seed 4: ATT = 15.8281, SE = 6.4929, p = 0.07138\n",
      "✅ SubSubCat_Venlafaxine | Seed 5: ATT = 12.4375, SE = 8.7485, p = 0.22818\n",
      "✅ SubSubCat_Venlafaxine | Seed 6: ATT = 12.4714, SE = 11.1813, p = 0.32718\n",
      "✅ SubSubCat_Venlafaxine | Seed 7: ATT = 9.6272, SE = 5.2658, p = 0.14151\n",
      "✅ SubSubCat_Venlafaxine | Seed 8: ATT = 10.5146, SE = 5.2234, p = 0.11441\n",
      "✅ SubSubCat_Venlafaxine | Seed 9: ATT = 13.9983, SE = 4.2369, p = 0.02982\n",
      "✅ SubSubCat_Venlafaxine | Seed 10: ATT = 11.1265, SE = 5.8273, p = 0.12884\n",
      "📊 Diagnostic plots saved for SubSubCat_Venlafaxine\n",
      "🏆 Best result for SubSubCat_Venlafaxine → Seed 9 | SE = 4.2369\n",
      "\n",
      "🚀 Running OLS for SubSubCat_Fluoxetine\n",
      "✅ SubSubCat_Fluoxetine | Seed 1: ATT = 4.8612, SE = 3.3305, p = 0.21817\n",
      "✅ SubSubCat_Fluoxetine | Seed 2: ATT = 2.9149, SE = 4.1720, p = 0.52324\n",
      "✅ SubSubCat_Fluoxetine | Seed 3: ATT = 5.9073, SE = 5.6949, p = 0.35819\n",
      "✅ SubSubCat_Fluoxetine | Seed 4: ATT = 5.1609, SE = 5.3972, p = 0.39312\n",
      "✅ SubSubCat_Fluoxetine | Seed 5: ATT = 7.3794, SE = 4.7432, p = 0.19474\n",
      "✅ SubSubCat_Fluoxetine | Seed 6: ATT = 7.1510, SE = 3.7346, p = 0.12804\n",
      "✅ SubSubCat_Fluoxetine | Seed 7: ATT = 6.3545, SE = 3.0967, p = 0.10944\n",
      "✅ SubSubCat_Fluoxetine | Seed 8: ATT = 5.3966, SE = 3.9219, p = 0.24083\n",
      "✅ SubSubCat_Fluoxetine | Seed 9: ATT = 4.9524, SE = 3.1009, p = 0.18549\n",
      "✅ SubSubCat_Fluoxetine | Seed 10: ATT = 4.3145, SE = 5.1804, p = 0.45177\n",
      "📊 Diagnostic plots saved for SubSubCat_Fluoxetine\n",
      "🏆 Best result for SubSubCat_Fluoxetine → Seed 7 | SE = 3.0967\n",
      "\n",
      "🚀 Running OLS for SubSubCat_Topiramaat\n",
      "✅ SubSubCat_Topiramaat | Seed 1: ATT = 5.8148, SE = 4.7508, p = 0.28813\n",
      "✅ SubSubCat_Topiramaat | Seed 2: ATT = 7.7328, SE = 5.1703, p = 0.20908\n",
      "✅ SubSubCat_Topiramaat | Seed 3: ATT = 3.3524, SE = 5.1592, p = 0.55126\n",
      "✅ SubSubCat_Topiramaat | Seed 4: ATT = 5.6000, SE = 3.4663, p = 0.18149\n",
      "✅ SubSubCat_Topiramaat | Seed 5: ATT = 4.0542, SE = 4.3032, p = 0.39947\n",
      "✅ SubSubCat_Topiramaat | Seed 6: ATT = 5.9427, SE = 5.1052, p = 0.30910\n",
      "✅ SubSubCat_Topiramaat | Seed 7: ATT = 5.2062, SE = 3.9825, p = 0.26120\n",
      "✅ SubSubCat_Topiramaat | Seed 8: ATT = 6.6713, SE = 4.8802, p = 0.24341\n",
      "✅ SubSubCat_Topiramaat | Seed 9: ATT = 4.7124, SE = 3.3321, p = 0.23019\n",
      "✅ SubSubCat_Topiramaat | Seed 10: ATT = 2.2992, SE = 4.1076, p = 0.60552\n",
      "📊 Diagnostic plots saved for SubSubCat_Topiramaat\n",
      "🏆 Best result for SubSubCat_Topiramaat → Seed 9 | SE = 3.3321\n",
      "\n",
      "🚀 Running OLS for SubSubCat_Tramadol\n",
      "✅ SubSubCat_Tramadol | Seed 1: ATT = -10.8997, SE = 11.7426, p = 0.40583\n",
      "✅ SubSubCat_Tramadol | Seed 2: ATT = -10.5065, SE = 14.7324, p = 0.51515\n",
      "✅ SubSubCat_Tramadol | Seed 3: ATT = -8.5190, SE = 18.6101, p = 0.67088\n",
      "✅ SubSubCat_Tramadol | Seed 4: ATT = -4.0371, SE = 10.8461, p = 0.72862\n",
      "✅ SubSubCat_Tramadol | Seed 5: ATT = -4.4215, SE = 13.2632, p = 0.75560\n",
      "✅ SubSubCat_Tramadol | Seed 6: ATT = -8.2650, SE = 18.0525, p = 0.67084\n",
      "✅ SubSubCat_Tramadol | Seed 7: ATT = -7.9063, SE = 14.1709, p = 0.60664\n",
      "✅ SubSubCat_Tramadol | Seed 8: ATT = 4.2078, SE = 18.3033, p = 0.82945\n",
      "✅ SubSubCat_Tramadol | Seed 9: ATT = 0.2219, SE = 24.7486, p = 0.99328\n",
      "✅ SubSubCat_Tramadol | Seed 10: ATT = -15.2421, SE = 10.4541, p = 0.21858\n",
      "📊 Diagnostic plots saved for SubSubCat_Tramadol\n",
      "🏆 Best result for SubSubCat_Tramadol → Seed 10 | SE = 10.4541\n",
      "\n",
      "🚀 Running OLS for SubSubCat_Zopiclon\n",
      "✅ SubSubCat_Zopiclon | Seed 1: ATT = 4.4136, SE = 5.5049, p = 0.46762\n",
      "✅ SubSubCat_Zopiclon | Seed 2: ATT = 2.0401, SE = 5.2940, p = 0.71959\n",
      "✅ SubSubCat_Zopiclon | Seed 3: ATT = 2.3962, SE = 4.0830, p = 0.58881\n",
      "✅ SubSubCat_Zopiclon | Seed 4: ATT = 4.3446, SE = 4.0702, p = 0.34592\n",
      "✅ SubSubCat_Zopiclon | Seed 5: ATT = 1.2429, SE = 5.0173, p = 0.81655\n",
      "✅ SubSubCat_Zopiclon | Seed 6: ATT = 0.3752, SE = 3.9294, p = 0.92852\n",
      "✅ SubSubCat_Zopiclon | Seed 7: ATT = -0.2635, SE = 6.9980, p = 0.97176\n",
      "✅ SubSubCat_Zopiclon | Seed 8: ATT = 1.3481, SE = 5.5330, p = 0.81949\n",
      "✅ SubSubCat_Zopiclon | Seed 9: ATT = 3.1996, SE = 5.2088, p = 0.57225\n",
      "✅ SubSubCat_Zopiclon | Seed 10: ATT = 4.0144, SE = 4.1720, p = 0.39043\n",
      "📊 Diagnostic plots saved for SubSubCat_Zopiclon\n",
      "🏆 Best result for SubSubCat_Zopiclon → Seed 6 | SE = 3.9294\n",
      "\n",
      "🚀 Running OLS for SubSubCat_Loprazolam\n",
      "✅ SubSubCat_Loprazolam | Seed 1: ATT = -0.4541, SE = 6.2841, p = 0.94587\n",
      "✅ SubSubCat_Loprazolam | Seed 2: ATT = 5.4924, SE = 14.6638, p = 0.72700\n",
      "✅ SubSubCat_Loprazolam | Seed 3: ATT = 3.6145, SE = 9.8499, p = 0.73224\n",
      "✅ SubSubCat_Loprazolam | Seed 4: ATT = 1.3146, SE = 11.8684, p = 0.91714\n",
      "✅ SubSubCat_Loprazolam | Seed 5: ATT = -1.3142, SE = 6.3188, p = 0.84540\n",
      "✅ SubSubCat_Loprazolam | Seed 6: ATT = 1.4223, SE = 8.1847, p = 0.87048\n",
      "✅ SubSubCat_Loprazolam | Seed 7: ATT = 3.2004, SE = 10.2726, p = 0.77095\n",
      "✅ SubSubCat_Loprazolam | Seed 8: ATT = -3.2472, SE = 7.2370, p = 0.67689\n",
      "✅ SubSubCat_Loprazolam | Seed 9: ATT = 0.5106, SE = 13.2778, p = 0.97117\n",
      "✅ SubSubCat_Loprazolam | Seed 10: ATT = -3.8615, SE = 8.5243, p = 0.67403\n",
      "📊 Diagnostic plots saved for SubSubCat_Loprazolam\n",
      "🏆 Best result for SubSubCat_Loprazolam → Seed 1 | SE = 6.2841\n",
      "\n",
      "🚀 Running OLS for SubSubCat_Alprazolam\n",
      "✅ SubSubCat_Alprazolam | Seed 1: ATT = 4.6754, SE = 6.8607, p = 0.53298\n",
      "✅ SubSubCat_Alprazolam | Seed 2: ATT = 0.3375, SE = 8.8547, p = 0.97142\n",
      "✅ SubSubCat_Alprazolam | Seed 3: ATT = 9.8108, SE = 17.1648, p = 0.59820\n",
      "✅ SubSubCat_Alprazolam | Seed 4: ATT = 7.6978, SE = 17.2668, p = 0.67880\n",
      "✅ SubSubCat_Alprazolam | Seed 5: ATT = -8.7679, SE = 9.5920, p = 0.41239\n",
      "✅ SubSubCat_Alprazolam | Seed 6: ATT = -0.1378, SE = 12.1489, p = 0.99149\n",
      "✅ SubSubCat_Alprazolam | Seed 7: ATT = 5.5120, SE = 11.5292, p = 0.65754\n",
      "✅ SubSubCat_Alprazolam | Seed 8: ATT = -7.9591, SE = 26.9029, p = 0.78207\n",
      "✅ SubSubCat_Alprazolam | Seed 9: ATT = 1.3409, SE = 11.2133, p = 0.91058\n",
      "✅ SubSubCat_Alprazolam | Seed 10: ATT = -5.9071, SE = 19.1786, p = 0.77345\n",
      "📊 Diagnostic plots saved for SubSubCat_Alprazolam\n",
      "🏆 Best result for SubSubCat_Alprazolam → Seed 1 | SE = 6.8607\n",
      "\n",
      "🚀 Running OLS for SubSubCat_promethazine\n",
      "✅ SubSubCat_promethazine | Seed 1: ATT = 5.2103, SE = 7.4186, p = 0.52120\n",
      "✅ SubSubCat_promethazine | Seed 2: ATT = 5.2676, SE = 4.5120, p = 0.30787\n",
      "✅ SubSubCat_promethazine | Seed 3: ATT = 6.5125, SE = 5.1394, p = 0.27384\n",
      "✅ SubSubCat_promethazine | Seed 4: ATT = 3.7806, SE = 7.5565, p = 0.64313\n",
      "✅ SubSubCat_promethazine | Seed 5: ATT = 4.2269, SE = 4.0733, p = 0.35802\n",
      "✅ SubSubCat_promethazine | Seed 6: ATT = 7.5675, SE = 5.2199, p = 0.22073\n",
      "✅ SubSubCat_promethazine | Seed 7: ATT = 7.3380, SE = 4.8350, p = 0.20370\n",
      "✅ SubSubCat_promethazine | Seed 8: ATT = 9.0451, SE = 3.3199, p = 0.05274\n",
      "✅ SubSubCat_promethazine | Seed 9: ATT = 6.5052, SE = 11.9615, p = 0.61544\n",
      "✅ SubSubCat_promethazine | Seed 10: ATT = 10.4697, SE = 4.8926, p = 0.09910\n",
      "📊 Diagnostic plots saved for SubSubCat_promethazine\n",
      "🏆 Best result for SubSubCat_promethazine → Seed 8 | SE = 3.3199\n",
      "\n",
      "🚀 Running OLS for SubSubCat_Paroxetine\n",
      "✅ SubSubCat_Paroxetine | Seed 1: ATT = 2.8755, SE = 12.9257, p = 0.83485\n",
      "✅ SubSubCat_Paroxetine | Seed 2: ATT = 7.1164, SE = 10.5797, p = 0.53803\n",
      "✅ SubSubCat_Paroxetine | Seed 3: ATT = 8.3862, SE = 25.1144, p = 0.75521\n",
      "✅ SubSubCat_Paroxetine | Seed 4: ATT = 5.5196, SE = 17.2026, p = 0.76438\n",
      "✅ SubSubCat_Paroxetine | Seed 5: ATT = 1.7522, SE = 12.7935, p = 0.89768\n",
      "✅ SubSubCat_Paroxetine | Seed 6: ATT = 0.4683, SE = 22.6222, p = 0.98447\n",
      "✅ SubSubCat_Paroxetine | Seed 7: ATT = -2.2803, SE = 13.4986, p = 0.87405\n",
      "✅ SubSubCat_Paroxetine | Seed 8: ATT = 26.3641, SE = 34.8889, p = 0.49191\n",
      "✅ SubSubCat_Paroxetine | Seed 9: ATT = 4.0060, SE = 22.1250, p = 0.86512\n",
      "✅ SubSubCat_Paroxetine | Seed 10: ATT = 14.2296, SE = 17.0430, p = 0.45073\n",
      "📊 Diagnostic plots saved for SubSubCat_Paroxetine\n",
      "🏆 Best result for SubSubCat_Paroxetine → Seed 2 | SE = 10.5797\n",
      "\n",
      "🚀 Running OLS for SubSubCat_Bupropion\n",
      "✅ SubSubCat_Bupropion | Seed 1: ATT = -0.1338, SE = 12.0984, p = 0.99170\n",
      "✅ SubSubCat_Bupropion | Seed 2: ATT = -3.1793, SE = 12.9514, p = 0.81817\n",
      "✅ SubSubCat_Bupropion | Seed 3: ATT = 2.7576, SE = 15.4165, p = 0.86673\n",
      "✅ SubSubCat_Bupropion | Seed 4: ATT = 1.5669, SE = 22.3024, p = 0.94736\n",
      "✅ SubSubCat_Bupropion | Seed 5: ATT = 5.3884, SE = 15.0886, p = 0.73905\n",
      "✅ SubSubCat_Bupropion | Seed 6: ATT = -0.5599, SE = 18.6171, p = 0.97745\n",
      "✅ SubSubCat_Bupropion | Seed 7: ATT = -1.5032, SE = 13.7728, p = 0.91835\n",
      "✅ SubSubCat_Bupropion | Seed 8: ATT = -8.6064, SE = 23.2027, p = 0.72951\n",
      "✅ SubSubCat_Bupropion | Seed 9: ATT = 1.0168, SE = 12.1783, p = 0.93747\n",
      "✅ SubSubCat_Bupropion | Seed 10: ATT = -2.8677, SE = 7.6807, p = 0.72782\n",
      "📊 Diagnostic plots saved for SubSubCat_Bupropion\n",
      "🏆 Best result for SubSubCat_Bupropion → Seed 10 | SE = 7.6807\n",
      "\n",
      "🚀 Running OLS for SubSubCat_Methylfenidaat\n",
      "✅ SubSubCat_Methylfenidaat | Seed 1: ATT = -1.2163, SE = 5.9146, p = 0.84711\n",
      "✅ SubSubCat_Methylfenidaat | Seed 2: ATT = 9.7640, SE = 9.7124, p = 0.37163\n",
      "✅ SubSubCat_Methylfenidaat | Seed 3: ATT = 3.0977, SE = 10.3225, p = 0.77905\n",
      "✅ SubSubCat_Methylfenidaat | Seed 4: ATT = 13.6255, SE = 8.6795, p = 0.19154\n",
      "✅ SubSubCat_Methylfenidaat | Seed 5: ATT = 0.8445, SE = 7.7696, p = 0.91868\n",
      "✅ SubSubCat_Methylfenidaat | Seed 6: ATT = 4.3171, SE = 6.5157, p = 0.54383\n",
      "✅ SubSubCat_Methylfenidaat | Seed 7: ATT = 4.6344, SE = 9.8585, p = 0.66277\n",
      "✅ SubSubCat_Methylfenidaat | Seed 8: ATT = 6.9614, SE = 9.1536, p = 0.48931\n",
      "✅ SubSubCat_Methylfenidaat | Seed 9: ATT = 7.8377, SE = 25.1923, p = 0.77125\n",
      "✅ SubSubCat_Methylfenidaat | Seed 10: ATT = 4.3515, SE = 10.3092, p = 0.69465\n",
      "📊 Diagnostic plots saved for SubSubCat_Methylfenidaat\n",
      "🏆 Best result for SubSubCat_Methylfenidaat → Seed 1 | SE = 5.9146\n",
      "\n",
      "🚀 Running OLS for SubSubCat_Olanzapine\n",
      "✅ SubSubCat_Olanzapine | Seed 1: ATT = -3.3746, SE = 4.1249, p = 0.45924\n",
      "✅ SubSubCat_Olanzapine | Seed 2: ATT = -1.0050, SE = 4.9708, p = 0.84964\n",
      "✅ SubSubCat_Olanzapine | Seed 3: ATT = -0.8321, SE = 6.7019, p = 0.90718\n",
      "✅ SubSubCat_Olanzapine | Seed 4: ATT = 0.8848, SE = 5.7249, p = 0.88466\n",
      "✅ SubSubCat_Olanzapine | Seed 5: ATT = -2.0731, SE = 4.7293, p = 0.68377\n",
      "✅ SubSubCat_Olanzapine | Seed 6: ATT = -1.9642, SE = 5.2261, p = 0.72611\n",
      "✅ SubSubCat_Olanzapine | Seed 7: ATT = -1.4071, SE = 3.5409, p = 0.71138\n",
      "✅ SubSubCat_Olanzapine | Seed 8: ATT = 0.9505, SE = 3.6407, p = 0.80693\n",
      "✅ SubSubCat_Olanzapine | Seed 9: ATT = -2.2497, SE = 7.4060, p = 0.77645\n",
      "✅ SubSubCat_Olanzapine | Seed 10: ATT = -0.9533, SE = 3.3663, p = 0.79108\n",
      "📊 Diagnostic plots saved for SubSubCat_Olanzapine\n",
      "🏆 Best result for SubSubCat_Olanzapine → Seed 10 | SE = 3.3663\n",
      "\n",
      "🚀 Running OLS for SubSubCat_Zolpidem\n",
      "✅ SubSubCat_Zolpidem | Seed 1: ATT = 15.1552, SE = 4.7396, p = 0.03298\n",
      "✅ SubSubCat_Zolpidem | Seed 2: ATT = 14.9658, SE = 10.4301, p = 0.22464\n",
      "✅ SubSubCat_Zolpidem | Seed 3: ATT = 11.5857, SE = 4.0478, p = 0.04583\n",
      "✅ SubSubCat_Zolpidem | Seed 4: ATT = 9.6792, SE = 7.2743, p = 0.25411\n",
      "✅ SubSubCat_Zolpidem | Seed 5: ATT = 6.4007, SE = 9.6770, p = 0.54449\n",
      "✅ SubSubCat_Zolpidem | Seed 6: ATT = 9.6625, SE = 7.7431, p = 0.28013\n",
      "✅ SubSubCat_Zolpidem | Seed 7: ATT = 11.5191, SE = 5.6944, p = 0.11313\n",
      "✅ SubSubCat_Zolpidem | Seed 8: ATT = 9.0174, SE = 5.6791, p = 0.18752\n",
      "✅ SubSubCat_Zolpidem | Seed 9: ATT = 8.5552, SE = 3.9198, p = 0.09448\n",
      "✅ SubSubCat_Zolpidem | Seed 10: ATT = 10.1918, SE = 5.8795, p = 0.15804\n",
      "📊 Diagnostic plots saved for SubSubCat_Zolpidem\n",
      "🏆 Best result for SubSubCat_Zolpidem → Seed 9 | SE = 3.9198\n",
      "\n",
      "🎯 All summary files saved.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import KFold\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import t, probplot\n",
    "import xgboost as xgb\n",
    "\n",
    "# -----------------------------\n",
    "# Configuration\n",
    "# -----------------------------\n",
    "seeds = list(range(1, 11))\n",
    "imputations = 5\n",
    "output_folder = \"outputs\"\n",
    "\n",
    "# -----------------------------\n",
    "# Diagnostic Plotting Function\n",
    "# -----------------------------\n",
    "def create_diagnostic_plots(residuals_data, fitted_data, group_name):\n",
    "    \"\"\"Create 4 diagnostic plots for model validation\"\"\"\n",
    "    plots_dir = os.path.join(output_folder, \"plots\")\n",
    "    os.makedirs(plots_dir, exist_ok=True)\n",
    "    \n",
    "    # Flatten the collected data\n",
    "    all_residuals = np.concatenate(residuals_data)\n",
    "    all_fitted = np.concatenate(fitted_data)\n",
    "    \n",
    "    # Create figure with 2x2 subplots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    fig.suptitle(f'Diagnostic Plots - {group_name}', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Residuals vs Fitted\n",
    "    axes[0,0].scatter(all_fitted, all_residuals, alpha=0.6, s=20)\n",
    "    axes[0,0].axhline(y=0, color='red', linestyle='--', alpha=0.8)\n",
    "    axes[0,0].set_xlabel('Fitted Values')\n",
    "    axes[0,0].set_ylabel('Residuals')\n",
    "    axes[0,0].set_title('Residuals vs Fitted')\n",
    "    axes[0,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. QQ Plot\n",
    "    probplot(all_residuals, dist=\"norm\", plot=axes[0,1])\n",
    "    axes[0,1].set_title('Q-Q Plot (Normal)')\n",
    "    axes[0,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Residual Histogram\n",
    "    axes[1,0].hist(all_residuals, bins=30, density=True, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    axes[1,0].set_xlabel('Residuals')\n",
    "    axes[1,0].set_ylabel('Density')\n",
    "    axes[1,0].set_title('Residual Distribution')\n",
    "    axes[1,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Scale-Location Plot\n",
    "    sqrt_abs_residuals = np.sqrt(np.abs(all_residuals))\n",
    "    axes[1,1].scatter(all_fitted, sqrt_abs_residuals, alpha=0.6, s=20)\n",
    "    axes[1,1].set_xlabel('Fitted Values')\n",
    "    axes[1,1].set_ylabel('√|Residuals|')\n",
    "    axes[1,1].set_title('Scale-Location Plot')\n",
    "    axes[1,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save plot\n",
    "    plot_filename = os.path.join(plots_dir, f'{group_name}.png')\n",
    "    plt.savefig(plot_filename, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"📊 Diagnostic plots saved for {group_name}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Rubin's Rule\n",
    "# -----------------------------\n",
    "def rubins_pool(estimates, ses):\n",
    "    m = len(estimates)\n",
    "    q_bar = np.mean(estimates)\n",
    "    u_bar = np.mean(np.square(ses))\n",
    "    b_m = np.var(estimates, ddof=1)\n",
    "    total_var = u_bar + ((1 + 1/m) * b_m)\n",
    "    total_se = np.sqrt(total_var)\n",
    "    ci_lower = q_bar - 1.96 * total_se\n",
    "    ci_upper = q_bar + 1.96 * total_se\n",
    "    p_value = 2 * (1 - t.cdf(np.abs(q_bar / total_se), df=m-1))\n",
    "    rounded_p = round(p_value, 5)\n",
    "    formatted_p = \"< 0.00001\" if rounded_p <= 0.00001 else f\"{rounded_p:.5f}\"\n",
    "    return q_bar, total_se, ci_lower, ci_upper, formatted_p\n",
    "\n",
    "# -----------------------------\n",
    "# SMD + Variance Ratio\n",
    "# -----------------------------\n",
    "def calculate_smd_vr(X, T, weights):\n",
    "    treated = X[T == 1]\n",
    "    control = X[T == 0]\n",
    "    w_treated = weights[T == 1]\n",
    "    w_control = weights[T == 0]\n",
    "    smd, vr = [], []\n",
    "    for col in X.columns:\n",
    "        try:\n",
    "            m1, m0 = np.average(treated[col], weights=w_treated), np.average(control[col], weights=w_control)\n",
    "            s1 = np.sqrt(np.average((treated[col] - m1) ** 2, weights=w_treated))\n",
    "            s0 = np.sqrt(np.average((control[col] - m0) ** 2, weights=w_control))\n",
    "            pooled_sd = np.sqrt((s1 ** 2 + s0 ** 2) / 2)\n",
    "            smd.append((m1 - m0) / pooled_sd if pooled_sd > 0 else 0)\n",
    "            vr.append(s1**2 / s0**2 if s0**2 > 0 else 0)\n",
    "        except Exception:\n",
    "            smd.append(np.nan)\n",
    "            vr.append(np.nan)\n",
    "    return np.nanmean(smd), np.nanmean(vr)\n",
    "\n",
    "# -----------------------------\n",
    "# OLS Main Loop\n",
    "# -----------------------------\n",
    "def run_dml_with_trimmed_data(final_covariates_map):\n",
    "    att_results = []\n",
    "    balance_results = []\n",
    "\n",
    "    for group, covariates in final_covariates_map.items():\n",
    "        print(f\"\\n🚀 Running OLS for {group}\")\n",
    "        group_dir = os.path.join(output_folder, group)\n",
    "        os.makedirs(group_dir, exist_ok=True)\n",
    "\n",
    "        best_result = None\n",
    "        best_se = float(\"inf\")\n",
    "        \n",
    "        # Initialize lists to collect residuals and fitted values for diagnostic plots\n",
    "        group_residuals = []\n",
    "        group_fitted = []\n",
    "\n",
    "        for seed in seeds:\n",
    "            # Set random seed for this iteration\n",
    "            np.random.seed(seed)\n",
    "            \n",
    "            att_list, se_list, r2_list, rmse_list, smd_list, vr_list = [], [], [], [], [], []\n",
    "\n",
    "            for imp in range(1, imputations + 1):\n",
    "                file_path = os.path.join(group_dir, f\"trimmed_data_imp{imp}.pkl\")\n",
    "                if not os.path.exists(file_path):\n",
    "                    continue\n",
    "\n",
    "                df = pd.read_pickle(file_path)\n",
    "                if group not in df.columns or \"iptw\" not in df.columns:\n",
    "                    continue\n",
    "\n",
    "                # Add bootstrap sampling with seed-based randomization\n",
    "                n_samples = len(df)\n",
    "                bootstrap_idx = np.random.choice(n_samples, size=n_samples, replace=True)\n",
    "                df_bootstrap = df.iloc[bootstrap_idx].reset_index(drop=True)\n",
    "\n",
    "                X = df_bootstrap[covariates].copy()\n",
    "                T = df_bootstrap[group]\n",
    "                Y = df_bootstrap[\"caps5_change_baseline\"]\n",
    "                W = df_bootstrap[\"iptw\"]\n",
    "\n",
    "                try:\n",
    "                    # Create design matrix with treatment variable and covariates\n",
    "                    X_ols = pd.concat([T, X], axis=1)\n",
    "                    X_ols = sm.add_constant(X_ols)\n",
    "                    \n",
    "                    # Fit weighted OLS with robust standard errors\n",
    "                    ols_model = sm.WLS(Y, X_ols, weights=W).fit(cov_type='HC1')\n",
    "                    \n",
    "                    # Extract treatment effect (coefficient of treatment variable)\n",
    "                    att = ols_model.params[group]  # Treatment coefficient\n",
    "                    se = ols_model.bse[group]  # Robust standard error for treatment\n",
    "                    \n",
    "                    att_list.append(att)\n",
    "                    se_list.append(se)\n",
    "\n",
    "                    # Calculate model fit statistics\n",
    "                    Y_pred = ols_model.fittedvalues\n",
    "                    residuals = ols_model.resid\n",
    "                    rmse = mean_squared_error(Y, Y_pred, squared=False)\n",
    "                    r2 = ols_model.rsquared\n",
    "                    r2_list.append(r2)\n",
    "                    rmse_list.append(rmse)\n",
    "                    \n",
    "                    # Collect residuals and fitted values for diagnostic plots\n",
    "                    group_residuals.append(residuals.values)\n",
    "                    group_fitted.append(Y_pred.values)\n",
    "\n",
    "                    smd, vr = calculate_smd_vr(X, T, W)\n",
    "                    smd_list.append(smd)\n",
    "                    vr_list.append(vr)\n",
    "                except Exception as e:\n",
    "                    print(f\"⚠️ Error in {group}, seed {seed}, imp {imp}: {e}\")\n",
    "\n",
    "            if att_list:\n",
    "                att, se, ci_l, ci_u, p_val = rubins_pool(att_list, se_list)\n",
    "                avg_r2 = np.mean(r2_list)\n",
    "                avg_rmse = np.mean(rmse_list)\n",
    "                avg_smd = np.mean(smd_list)\n",
    "                avg_vr = np.mean(vr_list)\n",
    "\n",
    "                balance_results.append({\n",
    "                    \"group\": group, \"seed\": seed, \"smd\": avg_smd, \"vr\": avg_vr\n",
    "                })\n",
    "\n",
    "                if se < best_se:\n",
    "                    best_se = se\n",
    "                    best_result = {\n",
    "                        \"group\": group, \"seed\": seed, \"att\": att, \"se\": se,\n",
    "                        \"ci_lower\": ci_l, \"ci_upper\": ci_u, \"p_value\": p_val,\n",
    "                        \"r2\": avg_r2, \"rmse\": avg_rmse\n",
    "                    }\n",
    "\n",
    "                print(f\"✅ {group} | Seed {seed}: ATT = {att:.4f}, SE = {se:.4f}, p = {p_val}\")\n",
    "            else:\n",
    "                print(f\"⚠️ No valid results for {group} | Seed {seed}\")\n",
    "\n",
    "        # Create diagnostic plots for this group\n",
    "        if group_residuals and group_fitted:\n",
    "            create_diagnostic_plots(group_residuals, group_fitted, group)\n",
    "\n",
    "        if best_result:\n",
    "            att_results.append(best_result)\n",
    "            print(f\"🏆 Best result for {group} → Seed {best_result['seed']} | SE = {best_result['se']:.4f}\")\n",
    "\n",
    "    # Save final output\n",
    "    pd.DataFrame(att_results).to_excel(\"ols_rubin_summary_subsubcats.xlsx\", index=False)\n",
    "    pd.DataFrame(balance_results).to_excel(\"smd_vr_summary_subsubcats.xlsx\", index=False)\n",
    "    print(\"\\n🎯 All summary files saved.\")\n",
    "\n",
    "run_dml_with_trimmed_data(final_covariates_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "33707ec9-ca5a-413e-95df-a57b83ae905d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unweighted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "52d64785-d493-4aa1-8042-f15bddc87406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🚀 Running OLS for SubSubCat_Oxazepam\n",
      "✅ SubSubCat_Oxazepam | Seed 1: ATT = -0.0399, SE = 1.3432, p = 0.97774\n",
      "✅ SubSubCat_Oxazepam | Seed 2: ATT = 0.0996, SE = 1.4561, p = 0.94877\n",
      "✅ SubSubCat_Oxazepam | Seed 3: ATT = 0.7378, SE = 1.6199, p = 0.67240\n",
      "✅ SubSubCat_Oxazepam | Seed 4: ATT = 0.1610, SE = 1.7001, p = 0.92911\n",
      "✅ SubSubCat_Oxazepam | Seed 5: ATT = -0.8644, SE = 1.4484, p = 0.58281\n",
      "✅ SubSubCat_Oxazepam | Seed 6: ATT = 0.5571, SE = 1.4591, p = 0.72202\n",
      "✅ SubSubCat_Oxazepam | Seed 7: ATT = -0.3440, SE = 1.8962, p = 0.86487\n",
      "✅ SubSubCat_Oxazepam | Seed 8: ATT = 0.5488, SE = 1.6198, p = 0.75178\n",
      "✅ SubSubCat_Oxazepam | Seed 9: ATT = 0.7892, SE = 1.2741, p = 0.56918\n",
      "✅ SubSubCat_Oxazepam | Seed 10: ATT = 0.8554, SE = 1.3495, p = 0.56062\n",
      "📊 Diagnostic plots saved for SubSubCat_Oxazepam\n",
      "🏆 Best result for SubSubCat_Oxazepam → Seed 9 | SE = 1.2741\n",
      "\n",
      "🚀 Running OLS for SubSubCat_Diazepam\n",
      "✅ SubSubCat_Diazepam | Seed 1: ATT = -10.6111, SE = 5.1358, p = 0.10771\n",
      "✅ SubSubCat_Diazepam | Seed 2: ATT = -9.0265, SE = 6.7277, p = 0.25080\n",
      "✅ SubSubCat_Diazepam | Seed 3: ATT = -11.7721, SE = 7.6375, p = 0.19809\n",
      "✅ SubSubCat_Diazepam | Seed 4: ATT = -8.4892, SE = 6.9822, p = 0.29089\n",
      "✅ SubSubCat_Diazepam | Seed 5: ATT = -10.4414, SE = 8.6208, p = 0.29249\n",
      "✅ SubSubCat_Diazepam | Seed 6: ATT = -10.2315, SE = 5.4707, p = 0.13479\n",
      "✅ SubSubCat_Diazepam | Seed 7: ATT = -8.1084, SE = 6.1066, p = 0.25495\n",
      "✅ SubSubCat_Diazepam | Seed 8: ATT = -10.3369, SE = 7.1138, p = 0.21986\n",
      "✅ SubSubCat_Diazepam | Seed 9: ATT = -12.6341, SE = 7.7001, p = 0.17619\n",
      "✅ SubSubCat_Diazepam | Seed 10: ATT = -6.6035, SE = 6.5853, p = 0.37272\n",
      "📊 Diagnostic plots saved for SubSubCat_Diazepam\n",
      "🏆 Best result for SubSubCat_Diazepam → Seed 1 | SE = 5.1358\n",
      "\n",
      "🚀 Running OLS for SubSubCat_Paracetamol\n",
      "✅ SubSubCat_Paracetamol | Seed 1: ATT = 4.9132, SE = 4.6538, p = 0.35063\n",
      "✅ SubSubCat_Paracetamol | Seed 2: ATT = 0.1514, SE = 7.6392, p = 0.98513\n",
      "✅ SubSubCat_Paracetamol | Seed 3: ATT = 6.0386, SE = 6.4911, p = 0.40488\n",
      "✅ SubSubCat_Paracetamol | Seed 4: ATT = 3.0894, SE = 6.2348, p = 0.64622\n",
      "✅ SubSubCat_Paracetamol | Seed 5: ATT = 0.9080, SE = 7.0246, p = 0.90340\n",
      "✅ SubSubCat_Paracetamol | Seed 6: ATT = 6.9312, SE = 8.4439, p = 0.45783\n",
      "✅ SubSubCat_Paracetamol | Seed 7: ATT = 2.8200, SE = 9.4151, p = 0.77946\n",
      "✅ SubSubCat_Paracetamol | Seed 8: ATT = 5.4980, SE = 9.5436, p = 0.59541\n",
      "✅ SubSubCat_Paracetamol | Seed 9: ATT = 3.1815, SE = 7.9450, p = 0.70930\n",
      "✅ SubSubCat_Paracetamol | Seed 10: ATT = 6.4709, SE = 7.8783, p = 0.45758\n",
      "📊 Diagnostic plots saved for SubSubCat_Paracetamol\n",
      "🏆 Best result for SubSubCat_Paracetamol → Seed 1 | SE = 4.6538\n",
      "\n",
      "🚀 Running OLS for SubSubCat_Lorazepam\n",
      "✅ SubSubCat_Lorazepam | Seed 1: ATT = 2.6519, SE = 4.1905, p = 0.56121\n",
      "✅ SubSubCat_Lorazepam | Seed 2: ATT = 2.1115, SE = 5.2484, p = 0.70802\n",
      "✅ SubSubCat_Lorazepam | Seed 3: ATT = 3.5078, SE = 2.8334, p = 0.28342\n",
      "✅ SubSubCat_Lorazepam | Seed 4: ATT = 3.4894, SE = 3.3174, p = 0.35221\n",
      "✅ SubSubCat_Lorazepam | Seed 5: ATT = 2.3821, SE = 3.7997, p = 0.56471\n",
      "✅ SubSubCat_Lorazepam | Seed 6: ATT = 1.8856, SE = 3.4147, p = 0.61021\n",
      "✅ SubSubCat_Lorazepam | Seed 7: ATT = 3.0028, SE = 3.6595, p = 0.45798\n",
      "✅ SubSubCat_Lorazepam | Seed 8: ATT = 1.5862, SE = 4.2243, p = 0.72636\n",
      "✅ SubSubCat_Lorazepam | Seed 9: ATT = 3.8792, SE = 2.9397, p = 0.25743\n",
      "✅ SubSubCat_Lorazepam | Seed 10: ATT = 1.3997, SE = 3.6581, p = 0.72146\n",
      "📊 Diagnostic plots saved for SubSubCat_Lorazepam\n",
      "🏆 Best result for SubSubCat_Lorazepam → Seed 3 | SE = 2.8334\n",
      "\n",
      "🚀 Running OLS for SubSubCat_Mirtazapine\n",
      "✅ SubSubCat_Mirtazapine | Seed 1: ATT = 5.5856, SE = 3.8826, p = 0.22365\n",
      "✅ SubSubCat_Mirtazapine | Seed 2: ATT = 2.8296, SE = 4.2176, p = 0.53903\n",
      "✅ SubSubCat_Mirtazapine | Seed 3: ATT = 4.3709, SE = 2.9860, p = 0.21710\n",
      "✅ SubSubCat_Mirtazapine | Seed 4: ATT = 4.7443, SE = 4.1315, p = 0.31483\n",
      "✅ SubSubCat_Mirtazapine | Seed 5: ATT = 5.7406, SE = 3.3069, p = 0.15758\n",
      "✅ SubSubCat_Mirtazapine | Seed 6: ATT = 4.7973, SE = 3.8969, p = 0.28574\n",
      "✅ SubSubCat_Mirtazapine | Seed 7: ATT = 5.0805, SE = 4.8918, p = 0.35766\n",
      "✅ SubSubCat_Mirtazapine | Seed 8: ATT = 6.1191, SE = 4.4260, p = 0.23898\n",
      "✅ SubSubCat_Mirtazapine | Seed 9: ATT = 5.8624, SE = 5.3433, p = 0.33419\n",
      "✅ SubSubCat_Mirtazapine | Seed 10: ATT = 5.3387, SE = 3.3759, p = 0.18894\n",
      "📊 Diagnostic plots saved for SubSubCat_Mirtazapine\n",
      "🏆 Best result for SubSubCat_Mirtazapine → Seed 3 | SE = 2.9860\n",
      "\n",
      "🚀 Running OLS for SubSubCat_Escitalopram\n",
      "✅ SubSubCat_Escitalopram | Seed 1: ATT = 3.9585, SE = 6.9518, p = 0.59953\n",
      "✅ SubSubCat_Escitalopram | Seed 2: ATT = -2.9784, SE = 5.9142, p = 0.64100\n",
      "✅ SubSubCat_Escitalopram | Seed 3: ATT = 1.5121, SE = 8.2426, p = 0.86337\n",
      "✅ SubSubCat_Escitalopram | Seed 4: ATT = -1.5575, SE = 7.1368, p = 0.83793\n",
      "✅ SubSubCat_Escitalopram | Seed 5: ATT = -3.6683, SE = 5.6796, p = 0.55355\n",
      "✅ SubSubCat_Escitalopram | Seed 6: ATT = -1.0161, SE = 8.1323, p = 0.90659\n",
      "✅ SubSubCat_Escitalopram | Seed 7: ATT = 0.1659, SE = 6.3189, p = 0.98031\n",
      "✅ SubSubCat_Escitalopram | Seed 8: ATT = -1.2620, SE = 5.2962, p = 0.82337\n",
      "✅ SubSubCat_Escitalopram | Seed 9: ATT = 6.3417, SE = 7.5281, p = 0.44698\n",
      "✅ SubSubCat_Escitalopram | Seed 10: ATT = 0.4827, SE = 6.4041, p = 0.94353\n",
      "📊 Diagnostic plots saved for SubSubCat_Escitalopram\n",
      "🏆 Best result for SubSubCat_Escitalopram → Seed 8 | SE = 5.2962\n",
      "\n",
      "🚀 Running OLS for SubSubCat_Sertraline\n",
      "✅ SubSubCat_Sertraline | Seed 1: ATT = -1.2365, SE = 2.8506, p = 0.68683\n",
      "✅ SubSubCat_Sertraline | Seed 2: ATT = -0.2998, SE = 2.4982, p = 0.91026\n",
      "✅ SubSubCat_Sertraline | Seed 3: ATT = -0.9527, SE = 2.3778, p = 0.70914\n",
      "✅ SubSubCat_Sertraline | Seed 4: ATT = -1.0353, SE = 2.2008, p = 0.66256\n",
      "✅ SubSubCat_Sertraline | Seed 5: ATT = -0.3579, SE = 2.5570, p = 0.89544\n",
      "✅ SubSubCat_Sertraline | Seed 6: ATT = -2.5767, SE = 2.8200, p = 0.41256\n",
      "✅ SubSubCat_Sertraline | Seed 7: ATT = -1.1960, SE = 2.3242, p = 0.63399\n",
      "✅ SubSubCat_Sertraline | Seed 8: ATT = -0.4142, SE = 2.5566, p = 0.87915\n",
      "✅ SubSubCat_Sertraline | Seed 9: ATT = -0.5362, SE = 2.0833, p = 0.80958\n",
      "✅ SubSubCat_Sertraline | Seed 10: ATT = -2.2938, SE = 2.2408, p = 0.36386\n",
      "📊 Diagnostic plots saved for SubSubCat_Sertraline\n",
      "🏆 Best result for SubSubCat_Sertraline → Seed 9 | SE = 2.0833\n",
      "\n",
      "🚀 Running OLS for SubSubCat_Temazepam\n",
      "✅ SubSubCat_Temazepam | Seed 1: ATT = 3.5118, SE = 4.3614, p = 0.46584\n",
      "✅ SubSubCat_Temazepam | Seed 2: ATT = 2.1284, SE = 3.4878, p = 0.57467\n",
      "✅ SubSubCat_Temazepam | Seed 3: ATT = 1.2785, SE = 3.7032, p = 0.74731\n",
      "✅ SubSubCat_Temazepam | Seed 4: ATT = 0.6194, SE = 5.2944, p = 0.91250\n",
      "✅ SubSubCat_Temazepam | Seed 5: ATT = 1.6633, SE = 3.5160, p = 0.66082\n",
      "✅ SubSubCat_Temazepam | Seed 6: ATT = 1.5352, SE = 3.4830, p = 0.68214\n",
      "✅ SubSubCat_Temazepam | Seed 7: ATT = 1.4579, SE = 3.8527, p = 0.72436\n",
      "✅ SubSubCat_Temazepam | Seed 8: ATT = 1.6428, SE = 3.5920, p = 0.67115\n",
      "✅ SubSubCat_Temazepam | Seed 9: ATT = 3.3976, SE = 2.9075, p = 0.30747\n",
      "✅ SubSubCat_Temazepam | Seed 10: ATT = 0.9745, SE = 3.7653, p = 0.80856\n",
      "📊 Diagnostic plots saved for SubSubCat_Temazepam\n",
      "🏆 Best result for SubSubCat_Temazepam → Seed 9 | SE = 2.9075\n",
      "\n",
      "🚀 Running OLS for SubSubCat_Citalopram\n",
      "✅ SubSubCat_Citalopram | Seed 1: ATT = -3.9443, SE = 1.9958, p = 0.11931\n",
      "✅ SubSubCat_Citalopram | Seed 2: ATT = -2.1548, SE = 2.8087, p = 0.48575\n",
      "✅ SubSubCat_Citalopram | Seed 3: ATT = -2.5281, SE = 2.1873, p = 0.31208\n",
      "✅ SubSubCat_Citalopram | Seed 4: ATT = -4.9766, SE = 2.1348, p = 0.08015\n",
      "✅ SubSubCat_Citalopram | Seed 5: ATT = -2.2963, SE = 2.0183, p = 0.31875\n",
      "✅ SubSubCat_Citalopram | Seed 6: ATT = -4.1882, SE = 2.6093, p = 0.18374\n",
      "✅ SubSubCat_Citalopram | Seed 7: ATT = -2.4010, SE = 2.2919, p = 0.35394\n",
      "✅ SubSubCat_Citalopram | Seed 8: ATT = -4.1818, SE = 1.9388, p = 0.09722\n",
      "✅ SubSubCat_Citalopram | Seed 9: ATT = -2.6035, SE = 2.1599, p = 0.29449\n",
      "✅ SubSubCat_Citalopram | Seed 10: ATT = -3.3190, SE = 3.2618, p = 0.36644\n",
      "📊 Diagnostic plots saved for SubSubCat_Citalopram\n",
      "🏆 Best result for SubSubCat_Citalopram → Seed 8 | SE = 1.9388\n",
      "\n",
      "🚀 Running OLS for SubSubCat_Quetiapine\n",
      "✅ SubSubCat_Quetiapine | Seed 1: ATT = 2.7756, SE = 1.8980, p = 0.21746\n",
      "✅ SubSubCat_Quetiapine | Seed 2: ATT = 1.5365, SE = 2.3280, p = 0.54530\n",
      "✅ SubSubCat_Quetiapine | Seed 3: ATT = 2.1041, SE = 1.6924, p = 0.28165\n",
      "✅ SubSubCat_Quetiapine | Seed 4: ATT = 1.8409, SE = 1.8105, p = 0.36673\n",
      "✅ SubSubCat_Quetiapine | Seed 5: ATT = 2.6305, SE = 1.3621, p = 0.12564\n",
      "✅ SubSubCat_Quetiapine | Seed 6: ATT = 2.4450, SE = 2.0437, p = 0.29762\n",
      "✅ SubSubCat_Quetiapine | Seed 7: ATT = 2.2416, SE = 1.5805, p = 0.22909\n",
      "✅ SubSubCat_Quetiapine | Seed 8: ATT = 2.5026, SE = 2.4941, p = 0.37244\n",
      "✅ SubSubCat_Quetiapine | Seed 9: ATT = 1.8282, SE = 1.4130, p = 0.26537\n",
      "✅ SubSubCat_Quetiapine | Seed 10: ATT = 2.4453, SE = 1.5487, p = 0.18949\n",
      "📊 Diagnostic plots saved for SubSubCat_Quetiapine\n",
      "🏆 Best result for SubSubCat_Quetiapine → Seed 5 | SE = 1.3621\n",
      "\n",
      "🚀 Running OLS for SubSubCat_Amitriptyline\n",
      "✅ SubSubCat_Amitriptyline | Seed 1: ATT = 10.6766, SE = 14.3797, p = 0.49903\n",
      "✅ SubSubCat_Amitriptyline | Seed 2: ATT = 7.6189, SE = 12.3975, p = 0.57209\n",
      "✅ SubSubCat_Amitriptyline | Seed 3: ATT = 11.5748, SE = 13.6890, p = 0.44542\n",
      "✅ SubSubCat_Amitriptyline | Seed 4: ATT = 12.8336, SE = 7.3801, p = 0.15703\n",
      "✅ SubSubCat_Amitriptyline | Seed 5: ATT = 15.8683, SE = 9.8044, p = 0.18087\n",
      "✅ SubSubCat_Amitriptyline | Seed 6: ATT = 11.7171, SE = 10.4874, p = 0.32646\n",
      "✅ SubSubCat_Amitriptyline | Seed 7: ATT = 11.7695, SE = 6.2566, p = 0.13311\n",
      "✅ SubSubCat_Amitriptyline | Seed 8: ATT = 10.4951, SE = 9.7342, p = 0.34164\n",
      "✅ SubSubCat_Amitriptyline | Seed 9: ATT = 5.9216, SE = 11.2691, p = 0.62704\n",
      "✅ SubSubCat_Amitriptyline | Seed 10: ATT = 6.9595, SE = 9.9940, p = 0.52455\n",
      "📊 Diagnostic plots saved for SubSubCat_Amitriptyline\n",
      "🏆 Best result for SubSubCat_Amitriptyline → Seed 7 | SE = 6.2566\n",
      "\n",
      "🚀 Running OLS for SubSubCat_Venlafaxine\n",
      "✅ SubSubCat_Venlafaxine | Seed 1: ATT = 9.0122, SE = 6.5133, p = 0.23867\n",
      "✅ SubSubCat_Venlafaxine | Seed 2: ATT = 9.5786, SE = 8.0149, p = 0.29806\n",
      "✅ SubSubCat_Venlafaxine | Seed 3: ATT = 16.2247, SE = 7.7319, p = 0.10384\n",
      "✅ SubSubCat_Venlafaxine | Seed 4: ATT = 17.4420, SE = 7.2353, p = 0.07350\n",
      "✅ SubSubCat_Venlafaxine | Seed 5: ATT = 14.0131, SE = 10.2278, p = 0.24252\n",
      "✅ SubSubCat_Venlafaxine | Seed 6: ATT = 12.5361, SE = 12.5459, p = 0.37424\n",
      "✅ SubSubCat_Venlafaxine | Seed 7: ATT = 8.0297, SE = 6.7569, p = 0.30042\n",
      "✅ SubSubCat_Venlafaxine | Seed 8: ATT = 9.3891, SE = 6.5958, p = 0.22769\n",
      "✅ SubSubCat_Venlafaxine | Seed 9: ATT = 13.8594, SE = 6.3047, p = 0.09283\n",
      "✅ SubSubCat_Venlafaxine | Seed 10: ATT = 11.3007, SE = 8.4878, p = 0.25386\n",
      "📊 Diagnostic plots saved for SubSubCat_Venlafaxine\n",
      "🏆 Best result for SubSubCat_Venlafaxine → Seed 9 | SE = 6.3047\n",
      "\n",
      "🚀 Running OLS for SubSubCat_Fluoxetine\n",
      "✅ SubSubCat_Fluoxetine | Seed 1: ATT = 4.4529, SE = 4.8552, p = 0.41097\n",
      "✅ SubSubCat_Fluoxetine | Seed 2: ATT = 2.4195, SE = 4.0648, p = 0.58372\n",
      "✅ SubSubCat_Fluoxetine | Seed 3: ATT = 5.3756, SE = 5.8423, p = 0.40958\n",
      "✅ SubSubCat_Fluoxetine | Seed 4: ATT = 3.8930, SE = 5.3904, p = 0.51013\n",
      "✅ SubSubCat_Fluoxetine | Seed 5: ATT = 7.1239, SE = 4.3214, p = 0.17459\n",
      "✅ SubSubCat_Fluoxetine | Seed 6: ATT = 7.4569, SE = 4.2182, p = 0.15183\n",
      "✅ SubSubCat_Fluoxetine | Seed 7: ATT = 5.5235, SE = 4.5366, p = 0.29031\n",
      "✅ SubSubCat_Fluoxetine | Seed 8: ATT = 5.2472, SE = 4.2791, p = 0.28736\n",
      "✅ SubSubCat_Fluoxetine | Seed 9: ATT = 3.1946, SE = 3.7186, p = 0.43873\n",
      "✅ SubSubCat_Fluoxetine | Seed 10: ATT = 2.8244, SE = 5.1075, p = 0.60972\n",
      "📊 Diagnostic plots saved for SubSubCat_Fluoxetine\n",
      "🏆 Best result for SubSubCat_Fluoxetine → Seed 9 | SE = 3.7186\n",
      "\n",
      "🚀 Running OLS for SubSubCat_Topiramaat\n",
      "✅ SubSubCat_Topiramaat | Seed 1: ATT = 5.3436, SE = 4.7695, p = 0.32528\n",
      "✅ SubSubCat_Topiramaat | Seed 2: ATT = 6.6959, SE = 5.7249, p = 0.30710\n",
      "✅ SubSubCat_Topiramaat | Seed 3: ATT = 1.8286, SE = 4.1765, p = 0.68412\n",
      "✅ SubSubCat_Topiramaat | Seed 4: ATT = 5.6460, SE = 4.4081, p = 0.26947\n",
      "✅ SubSubCat_Topiramaat | Seed 5: ATT = 2.8321, SE = 3.8492, p = 0.50268\n",
      "✅ SubSubCat_Topiramaat | Seed 6: ATT = 4.8652, SE = 4.3550, p = 0.32650\n",
      "✅ SubSubCat_Topiramaat | Seed 7: ATT = 5.7900, SE = 4.0167, p = 0.22290\n",
      "✅ SubSubCat_Topiramaat | Seed 8: ATT = 5.4859, SE = 5.2513, p = 0.35514\n",
      "✅ SubSubCat_Topiramaat | Seed 9: ATT = 3.2155, SE = 3.4327, p = 0.40193\n",
      "✅ SubSubCat_Topiramaat | Seed 10: ATT = 0.9436, SE = 4.1150, p = 0.82987\n",
      "📊 Diagnostic plots saved for SubSubCat_Topiramaat\n",
      "🏆 Best result for SubSubCat_Topiramaat → Seed 9 | SE = 3.4327\n",
      "\n",
      "🚀 Running OLS for SubSubCat_Tramadol\n",
      "✅ SubSubCat_Tramadol | Seed 1: ATT = -11.1375, SE = 12.5966, p = 0.42655\n",
      "✅ SubSubCat_Tramadol | Seed 2: ATT = -9.2599, SE = 16.9990, p = 0.61489\n",
      "✅ SubSubCat_Tramadol | Seed 3: ATT = -5.9426, SE = 23.8168, p = 0.81525\n",
      "✅ SubSubCat_Tramadol | Seed 4: ATT = -5.6828, SE = 11.9859, p = 0.66013\n",
      "✅ SubSubCat_Tramadol | Seed 5: ATT = -3.7958, SE = 11.8174, p = 0.76414\n",
      "✅ SubSubCat_Tramadol | Seed 6: ATT = -5.7272, SE = 20.7555, p = 0.79626\n",
      "✅ SubSubCat_Tramadol | Seed 7: ATT = -8.8601, SE = 16.0012, p = 0.60927\n",
      "✅ SubSubCat_Tramadol | Seed 8: ATT = 6.5492, SE = 21.5428, p = 0.77628\n",
      "✅ SubSubCat_Tramadol | Seed 9: ATT = 1.3116, SE = 24.9017, p = 0.96052\n",
      "✅ SubSubCat_Tramadol | Seed 10: ATT = -14.6080, SE = 10.0942, p = 0.22140\n",
      "📊 Diagnostic plots saved for SubSubCat_Tramadol\n",
      "🏆 Best result for SubSubCat_Tramadol → Seed 10 | SE = 10.0942\n",
      "\n",
      "🚀 Running OLS for SubSubCat_Zopiclon\n",
      "✅ SubSubCat_Zopiclon | Seed 1: ATT = 4.0592, SE = 5.2135, p = 0.47971\n",
      "✅ SubSubCat_Zopiclon | Seed 2: ATT = 0.8541, SE = 6.6152, p = 0.90350\n",
      "✅ SubSubCat_Zopiclon | Seed 3: ATT = 1.9284, SE = 4.3328, p = 0.67930\n",
      "✅ SubSubCat_Zopiclon | Seed 4: ATT = 4.7258, SE = 4.6736, p = 0.36914\n",
      "✅ SubSubCat_Zopiclon | Seed 5: ATT = 1.0669, SE = 7.1277, p = 0.88826\n",
      "✅ SubSubCat_Zopiclon | Seed 6: ATT = 0.4158, SE = 5.5879, p = 0.94426\n",
      "✅ SubSubCat_Zopiclon | Seed 7: ATT = 0.3130, SE = 7.3719, p = 0.96817\n",
      "✅ SubSubCat_Zopiclon | Seed 8: ATT = 0.7715, SE = 7.1641, p = 0.91943\n",
      "✅ SubSubCat_Zopiclon | Seed 9: ATT = 2.0508, SE = 6.1400, p = 0.75515\n",
      "✅ SubSubCat_Zopiclon | Seed 10: ATT = 2.7212, SE = 5.0498, p = 0.61856\n",
      "📊 Diagnostic plots saved for SubSubCat_Zopiclon\n",
      "🏆 Best result for SubSubCat_Zopiclon → Seed 3 | SE = 4.3328\n",
      "\n",
      "🚀 Running OLS for SubSubCat_Loprazolam\n",
      "✅ SubSubCat_Loprazolam | Seed 1: ATT = -0.0009, SE = 8.1432, p = 0.99991\n",
      "✅ SubSubCat_Loprazolam | Seed 2: ATT = 6.0942, SE = 16.2111, p = 0.72606\n",
      "✅ SubSubCat_Loprazolam | Seed 3: ATT = 3.6991, SE = 9.8987, p = 0.72760\n",
      "✅ SubSubCat_Loprazolam | Seed 4: ATT = 1.2735, SE = 12.2267, p = 0.92206\n",
      "✅ SubSubCat_Loprazolam | Seed 5: ATT = -0.5023, SE = 8.5103, p = 0.95576\n",
      "✅ SubSubCat_Loprazolam | Seed 6: ATT = 3.3270, SE = 8.0728, p = 0.70138\n",
      "✅ SubSubCat_Loprazolam | Seed 7: ATT = 2.5721, SE = 11.9984, p = 0.84074\n",
      "✅ SubSubCat_Loprazolam | Seed 8: ATT = -2.8343, SE = 8.1194, p = 0.74464\n",
      "✅ SubSubCat_Loprazolam | Seed 9: ATT = 2.8530, SE = 12.5662, p = 0.83152\n",
      "✅ SubSubCat_Loprazolam | Seed 10: ATT = -0.4093, SE = 9.4573, p = 0.96755\n",
      "📊 Diagnostic plots saved for SubSubCat_Loprazolam\n",
      "🏆 Best result for SubSubCat_Loprazolam → Seed 6 | SE = 8.0728\n",
      "\n",
      "🚀 Running OLS for SubSubCat_Alprazolam\n",
      "✅ SubSubCat_Alprazolam | Seed 1: ATT = 3.8579, SE = 10.5987, p = 0.73429\n",
      "✅ SubSubCat_Alprazolam | Seed 2: ATT = 0.6101, SE = 13.1195, p = 0.96514\n",
      "✅ SubSubCat_Alprazolam | Seed 3: ATT = 7.6013, SE = 17.7849, p = 0.69109\n",
      "✅ SubSubCat_Alprazolam | Seed 4: ATT = 6.9019, SE = 18.4253, p = 0.72698\n",
      "✅ SubSubCat_Alprazolam | Seed 5: ATT = -12.0906, SE = 13.6685, p = 0.42636\n",
      "✅ SubSubCat_Alprazolam | Seed 6: ATT = -4.1511, SE = 12.1122, p = 0.74906\n",
      "✅ SubSubCat_Alprazolam | Seed 7: ATT = 3.1963, SE = 13.6149, p = 0.82592\n",
      "✅ SubSubCat_Alprazolam | Seed 8: ATT = -10.2218, SE = 26.5471, p = 0.71980\n",
      "✅ SubSubCat_Alprazolam | Seed 9: ATT = -0.2599, SE = 15.6553, p = 0.98755\n",
      "✅ SubSubCat_Alprazolam | Seed 10: ATT = -7.0979, SE = 21.4735, p = 0.75758\n",
      "📊 Diagnostic plots saved for SubSubCat_Alprazolam\n",
      "🏆 Best result for SubSubCat_Alprazolam → Seed 1 | SE = 10.5987\n",
      "\n",
      "🚀 Running OLS for SubSubCat_promethazine\n",
      "✅ SubSubCat_promethazine | Seed 1: ATT = 5.0293, SE = 8.7800, p = 0.59743\n",
      "✅ SubSubCat_promethazine | Seed 2: ATT = 2.6806, SE = 5.8623, p = 0.67121\n",
      "✅ SubSubCat_promethazine | Seed 3: ATT = 7.0045, SE = 4.9035, p = 0.22635\n",
      "✅ SubSubCat_promethazine | Seed 4: ATT = 3.4869, SE = 10.0440, p = 0.74596\n",
      "✅ SubSubCat_promethazine | Seed 5: ATT = 4.1255, SE = 6.5164, p = 0.56106\n",
      "✅ SubSubCat_promethazine | Seed 6: ATT = 7.6003, SE = 5.1878, p = 0.21678\n",
      "✅ SubSubCat_promethazine | Seed 7: ATT = 7.6635, SE = 6.1896, p = 0.28337\n",
      "✅ SubSubCat_promethazine | Seed 8: ATT = 8.1324, SE = 3.9828, p = 0.11070\n",
      "✅ SubSubCat_promethazine | Seed 9: ATT = 4.3011, SE = 12.2295, p = 0.74281\n",
      "✅ SubSubCat_promethazine | Seed 10: ATT = 10.6344, SE = 5.8730, p = 0.14443\n",
      "📊 Diagnostic plots saved for SubSubCat_promethazine\n",
      "🏆 Best result for SubSubCat_promethazine → Seed 8 | SE = 3.9828\n",
      "\n",
      "🚀 Running OLS for SubSubCat_Paroxetine\n",
      "✅ SubSubCat_Paroxetine | Seed 1: ATT = 2.4862, SE = 12.6626, p = 0.85392\n",
      "✅ SubSubCat_Paroxetine | Seed 2: ATT = 6.8310, SE = 12.2735, p = 0.60749\n",
      "✅ SubSubCat_Paroxetine | Seed 3: ATT = 6.3418, SE = 27.0018, p = 0.82585\n",
      "✅ SubSubCat_Paroxetine | Seed 4: ATT = 3.2318, SE = 19.1190, p = 0.87397\n",
      "✅ SubSubCat_Paroxetine | Seed 5: ATT = 2.5910, SE = 13.3471, p = 0.85554\n",
      "✅ SubSubCat_Paroxetine | Seed 6: ATT = 0.9860, SE = 23.8118, p = 0.96895\n",
      "✅ SubSubCat_Paroxetine | Seed 7: ATT = -1.1717, SE = 14.8376, p = 0.94085\n",
      "✅ SubSubCat_Paroxetine | Seed 8: ATT = 25.9845, SE = 35.1381, p = 0.50065\n",
      "✅ SubSubCat_Paroxetine | Seed 9: ATT = 3.4620, SE = 22.9570, p = 0.88743\n",
      "✅ SubSubCat_Paroxetine | Seed 10: ATT = 14.1810, SE = 17.0571, p = 0.45251\n",
      "📊 Diagnostic plots saved for SubSubCat_Paroxetine\n",
      "🏆 Best result for SubSubCat_Paroxetine → Seed 2 | SE = 12.2735\n",
      "\n",
      "🚀 Running OLS for SubSubCat_Bupropion\n",
      "✅ SubSubCat_Bupropion | Seed 1: ATT = -0.5481, SE = 12.0737, p = 0.96597\n",
      "✅ SubSubCat_Bupropion | Seed 2: ATT = -3.4125, SE = 14.9303, p = 0.83042\n",
      "✅ SubSubCat_Bupropion | Seed 3: ATT = 3.3498, SE = 15.9192, p = 0.84362\n",
      "✅ SubSubCat_Bupropion | Seed 4: ATT = 1.4924, SE = 22.1767, p = 0.94958\n",
      "✅ SubSubCat_Bupropion | Seed 5: ATT = 4.5927, SE = 16.4844, p = 0.79435\n",
      "✅ SubSubCat_Bupropion | Seed 6: ATT = -1.0821, SE = 18.8507, p = 0.95698\n",
      "✅ SubSubCat_Bupropion | Seed 7: ATT = -1.8421, SE = 14.0233, p = 0.90183\n",
      "✅ SubSubCat_Bupropion | Seed 8: ATT = -8.3850, SE = 22.5947, p = 0.72938\n",
      "✅ SubSubCat_Bupropion | Seed 9: ATT = 2.5997, SE = 12.9933, p = 0.85118\n",
      "✅ SubSubCat_Bupropion | Seed 10: ATT = -2.9232, SE = 9.0977, p = 0.76406\n",
      "📊 Diagnostic plots saved for SubSubCat_Bupropion\n",
      "🏆 Best result for SubSubCat_Bupropion → Seed 10 | SE = 9.0977\n",
      "\n",
      "🚀 Running OLS for SubSubCat_Methylfenidaat\n",
      "✅ SubSubCat_Methylfenidaat | Seed 1: ATT = -0.1252, SE = 9.3767, p = 0.98999\n",
      "✅ SubSubCat_Methylfenidaat | Seed 2: ATT = 10.1050, SE = 10.8832, p = 0.40571\n",
      "✅ SubSubCat_Methylfenidaat | Seed 3: ATT = 5.2943, SE = 12.7576, p = 0.69944\n",
      "✅ SubSubCat_Methylfenidaat | Seed 4: ATT = 14.6987, SE = 9.3704, p = 0.19181\n",
      "✅ SubSubCat_Methylfenidaat | Seed 5: ATT = 1.7962, SE = 12.8080, p = 0.89525\n",
      "✅ SubSubCat_Methylfenidaat | Seed 6: ATT = 3.4461, SE = 7.1168, p = 0.65354\n",
      "✅ SubSubCat_Methylfenidaat | Seed 7: ATT = 7.5207, SE = 10.3133, p = 0.50627\n",
      "✅ SubSubCat_Methylfenidaat | Seed 8: ATT = 8.4214, SE = 11.5877, p = 0.50763\n",
      "✅ SubSubCat_Methylfenidaat | Seed 9: ATT = 7.2058, SE = 25.5234, p = 0.79170\n",
      "✅ SubSubCat_Methylfenidaat | Seed 10: ATT = 6.1760, SE = 12.6272, p = 0.65038\n",
      "📊 Diagnostic plots saved for SubSubCat_Methylfenidaat\n",
      "🏆 Best result for SubSubCat_Methylfenidaat → Seed 6 | SE = 7.1168\n",
      "\n",
      "🚀 Running OLS for SubSubCat_Olanzapine\n",
      "✅ SubSubCat_Olanzapine | Seed 1: ATT = -2.6835, SE = 5.4292, p = 0.64703\n",
      "✅ SubSubCat_Olanzapine | Seed 2: ATT = -2.6341, SE = 4.8478, p = 0.61575\n",
      "✅ SubSubCat_Olanzapine | Seed 3: ATT = -0.2951, SE = 7.8928, p = 0.97196\n",
      "✅ SubSubCat_Olanzapine | Seed 4: ATT = 0.8298, SE = 5.4612, p = 0.88659\n",
      "✅ SubSubCat_Olanzapine | Seed 5: ATT = -3.4475, SE = 5.2854, p = 0.54982\n",
      "✅ SubSubCat_Olanzapine | Seed 6: ATT = -0.7573, SE = 5.0352, p = 0.88772\n",
      "✅ SubSubCat_Olanzapine | Seed 7: ATT = -1.0206, SE = 4.5849, p = 0.83475\n",
      "✅ SubSubCat_Olanzapine | Seed 8: ATT = 1.1401, SE = 4.2672, p = 0.80255\n",
      "✅ SubSubCat_Olanzapine | Seed 9: ATT = -3.4090, SE = 8.1019, p = 0.69555\n",
      "✅ SubSubCat_Olanzapine | Seed 10: ATT = -1.1030, SE = 3.9729, p = 0.79505\n",
      "📊 Diagnostic plots saved for SubSubCat_Olanzapine\n",
      "🏆 Best result for SubSubCat_Olanzapine → Seed 10 | SE = 3.9729\n",
      "\n",
      "🚀 Running OLS for SubSubCat_Zolpidem\n",
      "✅ SubSubCat_Zolpidem | Seed 1: ATT = 14.5022, SE = 6.0615, p = 0.07496\n",
      "✅ SubSubCat_Zolpidem | Seed 2: ATT = 15.3171, SE = 10.9814, p = 0.23554\n",
      "✅ SubSubCat_Zolpidem | Seed 3: ATT = 10.9959, SE = 5.7613, p = 0.12896\n",
      "✅ SubSubCat_Zolpidem | Seed 4: ATT = 10.3692, SE = 8.7293, p = 0.30060\n",
      "✅ SubSubCat_Zolpidem | Seed 5: ATT = 6.4100, SE = 9.6360, p = 0.54231\n",
      "✅ SubSubCat_Zolpidem | Seed 6: ATT = 8.9954, SE = 10.2423, p = 0.42939\n",
      "✅ SubSubCat_Zolpidem | Seed 7: ATT = 12.1364, SE = 5.9020, p = 0.10891\n",
      "✅ SubSubCat_Zolpidem | Seed 8: ATT = 10.2787, SE = 6.2917, p = 0.17766\n",
      "✅ SubSubCat_Zolpidem | Seed 9: ATT = 8.1273, SE = 5.9784, p = 0.24560\n",
      "✅ SubSubCat_Zolpidem | Seed 10: ATT = 12.1055, SE = 6.9392, p = 0.15601\n",
      "📊 Diagnostic plots saved for SubSubCat_Zolpidem\n",
      "🏆 Best result for SubSubCat_Zolpidem → Seed 3 | SE = 5.7613\n",
      "\n",
      "🎯 All summary files saved.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import KFold\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import t, probplot\n",
    "import xgboost as xgb\n",
    "\n",
    "# -----------------------------\n",
    "# Configuration\n",
    "# -----------------------------\n",
    "seeds = list(range(1, 11))\n",
    "imputations = 5\n",
    "output_folder = \"outputs\"\n",
    "\n",
    "# -----------------------------\n",
    "# Diagnostic Plotting Function\n",
    "# -----------------------------\n",
    "def create_diagnostic_plots(residuals_data, fitted_data, group_name):\n",
    "    \"\"\"Create 4 diagnostic plots for model validation\"\"\"\n",
    "    plots_dir = os.path.join(output_folder, \"plots\")\n",
    "    os.makedirs(plots_dir, exist_ok=True)\n",
    "    \n",
    "    # Flatten the collected data\n",
    "    all_residuals = np.concatenate(residuals_data)\n",
    "    all_fitted = np.concatenate(fitted_data)\n",
    "    \n",
    "    # Create figure with 2x2 subplots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    fig.suptitle(f'Diagnostic Plots - {group_name}', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Residuals vs Fitted\n",
    "    axes[0,0].scatter(all_fitted, all_residuals, alpha=0.6, s=20)\n",
    "    axes[0,0].axhline(y=0, color='red', linestyle='--', alpha=0.8)\n",
    "    axes[0,0].set_xlabel('Fitted Values')\n",
    "    axes[0,0].set_ylabel('Residuals')\n",
    "    axes[0,0].set_title('Residuals vs Fitted')\n",
    "    axes[0,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. QQ Plot\n",
    "    probplot(all_residuals, dist=\"norm\", plot=axes[0,1])\n",
    "    axes[0,1].set_title('Q-Q Plot (Normal)')\n",
    "    axes[0,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Residual Histogram\n",
    "    axes[1,0].hist(all_residuals, bins=30, density=True, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    axes[1,0].set_xlabel('Residuals')\n",
    "    axes[1,0].set_ylabel('Density')\n",
    "    axes[1,0].set_title('Residual Distribution')\n",
    "    axes[1,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Scale-Location Plot\n",
    "    sqrt_abs_residuals = np.sqrt(np.abs(all_residuals))\n",
    "    axes[1,1].scatter(all_fitted, sqrt_abs_residuals, alpha=0.6, s=20)\n",
    "    axes[1,1].set_xlabel('Fitted Values')\n",
    "    axes[1,1].set_ylabel('√|Residuals|')\n",
    "    axes[1,1].set_title('Scale-Location Plot')\n",
    "    axes[1,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save plot\n",
    "    plot_filename = os.path.join(plots_dir, f'{group_name}_unweighted.png')\n",
    "    plt.savefig(plot_filename, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"📊 Diagnostic plots saved for {group_name}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Rubin's Rule\n",
    "# -----------------------------\n",
    "def rubins_pool(estimates, ses):\n",
    "    m = len(estimates)\n",
    "    q_bar = np.mean(estimates)\n",
    "    u_bar = np.mean(np.square(ses))\n",
    "    b_m = np.var(estimates, ddof=1)\n",
    "    total_var = u_bar + ((1 + 1/m) * b_m)\n",
    "    total_se = np.sqrt(total_var)\n",
    "    ci_lower = q_bar - 1.96 * total_se\n",
    "    ci_upper = q_bar + 1.96 * total_se\n",
    "    p_value = 2 * (1 - t.cdf(np.abs(q_bar / total_se), df=m-1))\n",
    "    rounded_p = round(p_value, 5)\n",
    "    formatted_p = \"< 0.00001\" if rounded_p <= 0.00001 else f\"{rounded_p:.5f}\"\n",
    "    return q_bar, total_se, ci_lower, ci_upper, formatted_p\n",
    "\n",
    "# -----------------------------\n",
    "# SMD + Variance Ratio\n",
    "# -----------------------------\n",
    "def calculate_smd_vr(X, T):\n",
    "    treated = X[T == 1]\n",
    "    control = X[T == 0]\n",
    "    smd, vr = [], []\n",
    "    for col in X.columns:\n",
    "        try:\n",
    "            m1, m0 = treated[col].mean(), control[col].mean()\n",
    "            s1 = treated[col].std()\n",
    "            s0 = control[col].std()\n",
    "            pooled_sd = np.sqrt((s1 ** 2 + s0 ** 2) / 2)\n",
    "            smd.append((m1 - m0) / pooled_sd if pooled_sd > 0 else 0)\n",
    "            vr.append(s1**2 / s0**2 if s0**2 > 0 else 0)\n",
    "        except Exception:\n",
    "            smd.append(np.nan)\n",
    "            vr.append(np.nan)\n",
    "    return np.nanmean(smd), np.nanmean(vr)\n",
    "\n",
    "# -----------------------------\n",
    "# OLS Main Loop\n",
    "# -----------------------------\n",
    "def run_dml_with_trimmed_data(final_covariates_map):\n",
    "    att_results = []\n",
    "    balance_results = []\n",
    "\n",
    "    for group, covariates in final_covariates_map.items():\n",
    "        print(f\"\\n🚀 Running OLS for {group}\")\n",
    "        group_dir = os.path.join(output_folder, group)\n",
    "        os.makedirs(group_dir, exist_ok=True)\n",
    "\n",
    "        best_result = None\n",
    "        best_se = float(\"inf\")\n",
    "        \n",
    "        # Initialize lists to collect residuals and fitted values for diagnostic plots\n",
    "        group_residuals = []\n",
    "        group_fitted = []\n",
    "\n",
    "        for seed in seeds:\n",
    "            # Set random seed for this iteration\n",
    "            np.random.seed(seed)\n",
    "            \n",
    "            att_list, se_list, r2_list, rmse_list, smd_list, vr_list = [], [], [], [], [], []\n",
    "\n",
    "            for imp in range(1, imputations + 1):\n",
    "                file_path = os.path.join(group_dir, f\"trimmed_data_imp{imp}.pkl\")\n",
    "                if not os.path.exists(file_path):\n",
    "                    continue\n",
    "\n",
    "                df = pd.read_pickle(file_path)\n",
    "                if group not in df.columns or \"iptw\" not in df.columns:\n",
    "                    continue\n",
    "\n",
    "                # Add bootstrap sampling with seed-based randomization\n",
    "                n_samples = len(df)\n",
    "                bootstrap_idx = np.random.choice(n_samples, size=n_samples, replace=True)\n",
    "                df_bootstrap = df.iloc[bootstrap_idx].reset_index(drop=True)\n",
    "\n",
    "                X = df_bootstrap[covariates].copy()\n",
    "                T = df_bootstrap[group]\n",
    "                Y = df_bootstrap[\"caps5_change_baseline\"]\n",
    "                #W = df_bootstrap[\"iptw\"]\n",
    "\n",
    "                try:\n",
    "                    # Create design matrix with treatment variable and covariates\n",
    "                    X_ols = pd.concat([T, X], axis=1)\n",
    "                    X_ols = sm.add_constant(X_ols)\n",
    "                    \n",
    "                    # Fit OLS with robust standard errors (unweighted)\n",
    "                    ols_model = sm.OLS(Y, X_ols).fit(cov_type='HC1')\n",
    "                    \n",
    "                    # Extract treatment effect (coefficient of treatment variable)\n",
    "                    att = ols_model.params[group]  # Treatment coefficient\n",
    "                    se = ols_model.bse[group]  # Robust standard error for treatment\n",
    "                    \n",
    "                    att_list.append(att)\n",
    "                    se_list.append(se)\n",
    "\n",
    "                    # Calculate model fit statistics\n",
    "                    Y_pred = ols_model.fittedvalues\n",
    "                    residuals = ols_model.resid\n",
    "                    rmse = mean_squared_error(Y, Y_pred, squared=False)\n",
    "                    r2 = ols_model.rsquared\n",
    "                    r2_list.append(r2)\n",
    "                    rmse_list.append(rmse)\n",
    "                    \n",
    "                    # Collect residuals and fitted values for diagnostic plots\n",
    "                    group_residuals.append(residuals.values)\n",
    "                    group_fitted.append(Y_pred.values)\n",
    "\n",
    "                    smd, vr = calculate_smd_vr(X, T)\n",
    "                    smd_list.append(smd)\n",
    "                    vr_list.append(vr)\n",
    "                except Exception as e:\n",
    "                    print(f\"⚠️ Error in {group}, seed {seed}, imp {imp}: {e}\")\n",
    "\n",
    "            if att_list:\n",
    "                att, se, ci_l, ci_u, p_val = rubins_pool(att_list, se_list)\n",
    "                avg_r2 = np.mean(r2_list)\n",
    "                avg_rmse = np.mean(rmse_list)\n",
    "                avg_smd = np.mean(smd_list)\n",
    "                avg_vr = np.mean(vr_list)\n",
    "\n",
    "                balance_results.append({\n",
    "                    \"group\": group, \"seed\": seed, \"smd\": avg_smd, \"vr\": avg_vr\n",
    "                })\n",
    "\n",
    "                if se < best_se:\n",
    "                    best_se = se\n",
    "                    best_result = {\n",
    "                        \"group\": group, \"seed\": seed, \"att\": att, \"se\": se,\n",
    "                        \"ci_lower\": ci_l, \"ci_upper\": ci_u, \"p_value\": p_val,\n",
    "                        \"r2\": avg_r2, \"rmse\": avg_rmse\n",
    "                    }\n",
    "\n",
    "                print(f\"✅ {group} | Seed {seed}: ATT = {att:.4f}, SE = {se:.4f}, p = {p_val}\")\n",
    "            else:\n",
    "                print(f\"⚠️ No valid results for {group} | Seed {seed}\")\n",
    "\n",
    "        # Create diagnostic plots for this group\n",
    "        if group_residuals and group_fitted:\n",
    "            create_diagnostic_plots(group_residuals, group_fitted, group)\n",
    "\n",
    "        if best_result:\n",
    "            att_results.append(best_result)\n",
    "            print(f\"🏆 Best result for {group} → Seed {best_result['seed']} | SE = {best_result['se']:.4f}\")\n",
    "\n",
    "    # Save final output\n",
    "    pd.DataFrame(att_results).to_excel(\"ols_rubin_summary_subsubcats_unweighted.xlsx\", index=False)\n",
    "    pd.DataFrame(balance_results).to_excel(\"smd_vr_summary_subsubcats_unweighted.xlsx\", index=False)\n",
    "    print(\"\\n🎯 All summary files saved.\")\n",
    "\n",
    "run_dml_with_trimmed_data(final_covariates_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "23e02efd-519d-4712-b990-4d017532db01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Final_ATT_Summary_SubSubCat saved\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import sem, ttest_ind\n",
    "\n",
    "# ----------------------------------\n",
    "# File paths\n",
    "# ----------------------------------\n",
    "output_base = \"outputs\"\n",
    "att_file = \"ols_rubin_summary_subsubcats.xlsx\"\n",
    "trimmed_file = \"trimmed_data_imp1.pkl\"\n",
    "auc_file = \"auc_scores.xlsx\"  # NEW\n",
    "\n",
    "# ----------------------------------\n",
    "# Load ATT Summary\n",
    "# ----------------------------------\n",
    "if os.path.exists(att_file):\n",
    "    att_df = pd.read_excel(att_file)\n",
    "else:\n",
    "    raise FileNotFoundError(\"❌ ATT summary file not found: ols_summary_subsubcats.xlsx\")\n",
    "\n",
    "summary_rows = []\n",
    "\n",
    "# ----------------------------------\n",
    "# Loop over medication groups\n",
    "# ----------------------------------\n",
    "groups = [g for g in medication_groups if os.path.isdir(os.path.join(output_base, g))]\n",
    "\n",
    "for med in groups:\n",
    "    try:\n",
    "        group_path = os.path.join(output_base, med)\n",
    "\n",
    "        # Load trimmed data\n",
    "        df = pd.read_pickle(os.path.join(group_path, trimmed_file))\n",
    "\n",
    "        # Detect treatment column\n",
    "        treatment_cols = [col for col in df.columns if col.upper() == med.upper()]\n",
    "        if not treatment_cols:\n",
    "            print(f\"⚠️ Treatment column {med} not found in trimmed data. Skipping.\")\n",
    "            continue\n",
    "        treatment_var = treatment_cols[0]\n",
    "\n",
    "        # Extract treatment and outcome\n",
    "        T = df[treatment_var]\n",
    "        Y = df[\"caps5_change_baseline\"]\n",
    "\n",
    "        # Treated and control stats\n",
    "        treated = Y[T == 1]\n",
    "        control = Y[T == 0]\n",
    "\n",
    "        mean_treat = treated.mean()\n",
    "        se_treat = sem(treated) if len(treated) > 1 else np.nan\n",
    "\n",
    "        mean_ctrl = control.mean()\n",
    "        se_ctrl = sem(control) if len(control) > 1 else np.nan\n",
    "\n",
    "        # Cohen's d (unadjusted)\n",
    "        pooled_sd = np.sqrt(((treated.std() ** 2) + (control.std() ** 2)) / 2)\n",
    "        cohen_d = (mean_treat - mean_ctrl) / pooled_sd if pooled_sd > 0 else np.nan\n",
    "\n",
    "        # E-value (unadjusted)\n",
    "        delta = mean_treat - mean_ctrl\n",
    "        E = delta / abs(mean_ctrl) * 100 if mean_ctrl != 0 else np.nan\n",
    "\n",
    "        # Unadjusted p-value\n",
    "        try:\n",
    "            t_stat, p_val = ttest_ind(treated, control, equal_var=False, nan_policy=\"omit\")\n",
    "            rounded_p = round(p_val, 5)\n",
    "            formatted_p = \"< 0.00001\" if rounded_p < 0.00001 else rounded_p\n",
    "        except Exception:\n",
    "            formatted_p = np.nan\n",
    "\n",
    "        # AUC from new auc_scores.xlsx file\n",
    "        auc_val = np.nan\n",
    "        auc_path = os.path.join(group_path, auc_file)\n",
    "        if os.path.exists(auc_path):\n",
    "            auc_df = pd.read_excel(auc_path)\n",
    "            if \"AUC\" in auc_df.columns:\n",
    "                auc_val = auc_df[\"AUC\"].dropna().mean()\n",
    "\n",
    "        # Adjusted stats from Rubin summary\n",
    "        att_row = att_df[att_df[\"group\"].str.strip().str.upper() == med.strip().upper()]\n",
    "        if not att_row.empty:\n",
    "            att = att_row.iloc[0][\"att\"]\n",
    "            att_se = att_row.iloc[0][\"se\"]\n",
    "            att_p_val = att_row.iloc[0][\"p_value\"]\n",
    "            r2 = att_row.iloc[0][\"r2\"]\n",
    "            rmse = att_row.iloc[0][\"rmse\"]\n",
    "\n",
    "            try:\n",
    "                rounded_att_p = round(float(att_p_val), 5)\n",
    "                formatted_att_p = \"< 0.00001\" if rounded_att_p < 0.00001 else rounded_att_p\n",
    "            except:\n",
    "                formatted_att_p = att_p_val\n",
    "        else:\n",
    "            att, att_se, formatted_att_p, r2, rmse = np.nan, np.nan, np.nan, np.nan, np.nan\n",
    "\n",
    "        # Append full row\n",
    "        summary_rows.append({\n",
    "            'Medication Group': med,\n",
    "            'Mean Treated': mean_treat,\n",
    "            'SE Treated': se_treat,\n",
    "            'Mean Control': mean_ctrl,\n",
    "            'SE Control': se_ctrl,\n",
    "            'Cohen d': cohen_d,\n",
    "            'E (Unadjusted)': E,\n",
    "            'n Treated': len(treated),\n",
    "            'n Control': len(control),\n",
    "            #'Unadjusted p-value': formatted_p,\n",
    "            'ATT Estimate': att,\n",
    "            'ATT SE (Robust)': att_se,\n",
    "            'ATT p-value': formatted_att_p,\n",
    "            'R²': r2,\n",
    "            'RMSE': rmse,\n",
    "            'AUC': auc_val\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error in {med}: {e}\")\n",
    "\n",
    "# ----------------------------------\n",
    "# Save final summary\n",
    "# ----------------------------------\n",
    "summary_df = pd.DataFrame(summary_rows)\n",
    "summary_df = summary_df.sort_values(\"Medication Group\")\n",
    "summary_df.to_excel(\"Final_ATT_Summary_SubSubCat.xlsx\", index=False)\n",
    "print(\"✅ Final_ATT_Summary_SubSubCat saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "fb3090fc-bcf4-4134-ba0e-4aebd0c93537",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ ols_att_barplot_subsubcat saved\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "# ✅ Load the final summary table\n",
    "final_df = pd.read_excel(\"Final_ATT_Summary_SubSubCat.xlsx\")\n",
    "\n",
    "# ✅ Parse DML p-values (handle \"< 0.00001\")\n",
    "def parse_pval(p):\n",
    "    try:\n",
    "        if isinstance(p, str) and \"<\" in p:\n",
    "            return 0.000001\n",
    "        return float(p)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "final_df['ATT p-value'] = final_df['ATT p-value'].apply(parse_pval)\n",
    "\n",
    "# ✅ Plot settings\n",
    "width = 0.35\n",
    "\n",
    "# ✅ Plotting function for a single medication group\n",
    "def plot_single_group(row):\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    bars1 = ax.bar(-width/2, row['Mean Control'], width, \n",
    "                   yerr=row['SE Control'], label='Control', hatch='//', color='gray', capsize=5)\n",
    "    bars2 = ax.bar(+width/2, row['Mean Treated'], width, \n",
    "                   yerr=row['SE Treated'], label='Treated', color='steelblue', capsize=5)\n",
    "\n",
    "    label = (\n",
    "        f\"ATT = {row['ATT Estimate']:.2f}\\n\"\n",
    "        f\"d = {row['Cohen d']:.2f}, p = {row['ATT p-value']:.3f}\\n\"\n",
    "        f\"nT = {row['n Treated']}, nC = {row['n Control']}\\n\"\n",
    "        f\"E = {row['E (Unadjusted)']:.1f}%\"\n",
    "    )\n",
    "    max_y = max(row['Mean Control'], row['Mean Treated']) + 1.5\n",
    "    ax.text(0, max_y, label, ha='center', va='bottom', fontsize=9, color='#FFD700')\n",
    "\n",
    "    ax.axhline(0, color='black', linewidth=0.8)\n",
    "    ax.set_xticks([-width/2, +width/2])\n",
    "    ax.set_xticklabels(['Control', 'Treated'])\n",
    "    ax.set_title(f\"Group: {row['Medication Group']}\", fontsize=12, weight='bold')\n",
    "    ax.set_ylabel(\"CAPS5 Change Score\")\n",
    "    ax.set_ylim(bottom=0, top=max_y + 2)\n",
    "    ax.legend()\n",
    "    fig.tight_layout()\n",
    "    return fig\n",
    "\n",
    "# ✅ Generate and save all plots into a multi-page PDF\n",
    "with PdfPages(\"ols_att_barplot_subsubcat.pdf\") as pdf:\n",
    "    for idx, row in final_df.iterrows():\n",
    "        fig = plot_single_group(row)\n",
    "        pdf.savefig(fig, dpi=300, bbox_inches='tight')\n",
    "        plt.close(fig)\n",
    "print(\"✅ ols_att_barplot_subsubcat saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "1ca0cd05-2cb6-4516-b20d-55a74f23e2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Love plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "bf7bf994-5ff3-4c95-8be8-2903b37e2f15",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Processing SUBSUBCAT_Alprazolam...\n",
      "📊 Exported numeric summary to: outputs\\SUBSUBCAT_Alprazolam\\covariate_balance_table_SUBSUBCAT_Alprazolam.xlsx\n",
      "✅ Saved love plot: outputs\\SUBSUBCAT_Alprazolam\\love_plot_SUBSUBCAT_Alprazolam.pdf\n",
      "📏 Max weighted SMD for SUBSUBCAT_Alprazolam: 0.736\n",
      "\n",
      "🔍 Processing SUBSUBCAT_Amitriptyline...\n",
      "📊 Exported numeric summary to: outputs\\SUBSUBCAT_Amitriptyline\\covariate_balance_table_SUBSUBCAT_Amitriptyline.xlsx\n",
      "✅ Saved love plot: outputs\\SUBSUBCAT_Amitriptyline\\love_plot_SUBSUBCAT_Amitriptyline.pdf\n",
      "📏 Max weighted SMD for SUBSUBCAT_Amitriptyline: 0.666\n",
      "\n",
      "🔍 Processing SUBSUBCAT_Bupropion...\n",
      "📊 Exported numeric summary to: outputs\\SUBSUBCAT_Bupropion\\covariate_balance_table_SUBSUBCAT_Bupropion.xlsx\n",
      "✅ Saved love plot: outputs\\SUBSUBCAT_Bupropion\\love_plot_SUBSUBCAT_Bupropion.pdf\n",
      "📏 Max weighted SMD for SUBSUBCAT_Bupropion: 0.719\n",
      "\n",
      "🔍 Processing SUBSUBCAT_Citalopram...\n",
      "📊 Exported numeric summary to: outputs\\SUBSUBCAT_Citalopram\\covariate_balance_table_SUBSUBCAT_Citalopram.xlsx\n",
      "✅ Saved love plot: outputs\\SUBSUBCAT_Citalopram\\love_plot_SUBSUBCAT_Citalopram.pdf\n",
      "📏 Max weighted SMD for SUBSUBCAT_Citalopram: 0.201\n",
      "\n",
      "🔍 Processing SUBSUBCAT_Diazepam...\n",
      "📊 Exported numeric summary to: outputs\\SUBSUBCAT_Diazepam\\covariate_balance_table_SUBSUBCAT_Diazepam.xlsx\n",
      "✅ Saved love plot: outputs\\SUBSUBCAT_Diazepam\\love_plot_SUBSUBCAT_Diazepam.pdf\n",
      "📏 Max weighted SMD for SUBSUBCAT_Diazepam: 0.531\n",
      "\n",
      "🔍 Processing SUBSUBCAT_Escitalopram...\n",
      "📊 Exported numeric summary to: outputs\\SUBSUBCAT_Escitalopram\\covariate_balance_table_SUBSUBCAT_Escitalopram.xlsx\n",
      "✅ Saved love plot: outputs\\SUBSUBCAT_Escitalopram\\love_plot_SUBSUBCAT_Escitalopram.pdf\n",
      "📏 Max weighted SMD for SUBSUBCAT_Escitalopram: 0.416\n",
      "\n",
      "🔍 Processing SUBSUBCAT_Fluoxetine...\n",
      "📊 Exported numeric summary to: outputs\\SUBSUBCAT_Fluoxetine\\covariate_balance_table_SUBSUBCAT_Fluoxetine.xlsx\n",
      "✅ Saved love plot: outputs\\SUBSUBCAT_Fluoxetine\\love_plot_SUBSUBCAT_Fluoxetine.pdf\n",
      "📏 Max weighted SMD for SUBSUBCAT_Fluoxetine: 0.529\n",
      "\n",
      "🔍 Processing SUBSUBCAT_Loprazolam...\n",
      "📊 Exported numeric summary to: outputs\\SUBSUBCAT_Loprazolam\\covariate_balance_table_SUBSUBCAT_Loprazolam.xlsx\n",
      "✅ Saved love plot: outputs\\SUBSUBCAT_Loprazolam\\love_plot_SUBSUBCAT_Loprazolam.pdf\n",
      "📏 Max weighted SMD for SUBSUBCAT_Loprazolam: 0.728\n",
      "\n",
      "🔍 Processing SUBSUBCAT_Lorazepam...\n",
      "📊 Exported numeric summary to: outputs\\SUBSUBCAT_Lorazepam\\covariate_balance_table_SUBSUBCAT_Lorazepam.xlsx\n",
      "✅ Saved love plot: outputs\\SUBSUBCAT_Lorazepam\\love_plot_SUBSUBCAT_Lorazepam.pdf\n",
      "📏 Max weighted SMD for SUBSUBCAT_Lorazepam: 0.229\n",
      "\n",
      "🔍 Processing SUBSUBCAT_Methylfenidaat...\n",
      "📊 Exported numeric summary to: outputs\\SUBSUBCAT_Methylfenidaat\\covariate_balance_table_SUBSUBCAT_Methylfenidaat.xlsx\n",
      "✅ Saved love plot: outputs\\SUBSUBCAT_Methylfenidaat\\love_plot_SUBSUBCAT_Methylfenidaat.pdf\n",
      "📏 Max weighted SMD for SUBSUBCAT_Methylfenidaat: 0.515\n",
      "\n",
      "🔍 Processing SUBSUBCAT_Mirtazapine...\n",
      "📊 Exported numeric summary to: outputs\\SUBSUBCAT_Mirtazapine\\covariate_balance_table_SUBSUBCAT_Mirtazapine.xlsx\n",
      "✅ Saved love plot: outputs\\SUBSUBCAT_Mirtazapine\\love_plot_SUBSUBCAT_Mirtazapine.pdf\n",
      "📏 Max weighted SMD for SUBSUBCAT_Mirtazapine: 0.329\n",
      "\n",
      "🔍 Processing SUBSUBCAT_Olanzapine...\n",
      "📊 Exported numeric summary to: outputs\\SUBSUBCAT_Olanzapine\\covariate_balance_table_SUBSUBCAT_Olanzapine.xlsx\n",
      "✅ Saved love plot: outputs\\SUBSUBCAT_Olanzapine\\love_plot_SUBSUBCAT_Olanzapine.pdf\n",
      "📏 Max weighted SMD for SUBSUBCAT_Olanzapine: 0.464\n",
      "\n",
      "🔍 Processing SUBSUBCAT_Oxazepam...\n",
      "📊 Exported numeric summary to: outputs\\SUBSUBCAT_Oxazepam\\covariate_balance_table_SUBSUBCAT_Oxazepam.xlsx\n",
      "✅ Saved love plot: outputs\\SUBSUBCAT_Oxazepam\\love_plot_SUBSUBCAT_Oxazepam.pdf\n",
      "📏 Max weighted SMD for SUBSUBCAT_Oxazepam: 0.158\n",
      "\n",
      "🔍 Processing SUBSUBCAT_Paracetamol...\n",
      "📊 Exported numeric summary to: outputs\\SUBSUBCAT_Paracetamol\\covariate_balance_table_SUBSUBCAT_Paracetamol.xlsx\n",
      "✅ Saved love plot: outputs\\SUBSUBCAT_Paracetamol\\love_plot_SUBSUBCAT_Paracetamol.pdf\n",
      "📏 Max weighted SMD for SUBSUBCAT_Paracetamol: 0.607\n",
      "\n",
      "🔍 Processing SUBSUBCAT_Paroxetine...\n",
      "📊 Exported numeric summary to: outputs\\SUBSUBCAT_Paroxetine\\covariate_balance_table_SUBSUBCAT_Paroxetine.xlsx\n",
      "✅ Saved love plot: outputs\\SUBSUBCAT_Paroxetine\\love_plot_SUBSUBCAT_Paroxetine.pdf\n",
      "📏 Max weighted SMD for SUBSUBCAT_Paroxetine: 0.821\n",
      "\n",
      "🔍 Processing SUBSUBCAT_Promethazine...\n",
      "📊 Exported numeric summary to: outputs\\SUBSUBCAT_Promethazine\\covariate_balance_table_SUBSUBCAT_Promethazine.xlsx\n",
      "✅ Saved love plot: outputs\\SUBSUBCAT_Promethazine\\love_plot_SUBSUBCAT_Promethazine.pdf\n",
      "📏 Max weighted SMD for SUBSUBCAT_Promethazine: 0.388\n",
      "\n",
      "🔍 Processing SUBSUBCAT_Quetiapine...\n",
      "📊 Exported numeric summary to: outputs\\SUBSUBCAT_Quetiapine\\covariate_balance_table_SUBSUBCAT_Quetiapine.xlsx\n",
      "✅ Saved love plot: outputs\\SUBSUBCAT_Quetiapine\\love_plot_SUBSUBCAT_Quetiapine.pdf\n",
      "📏 Max weighted SMD for SUBSUBCAT_Quetiapine: 0.270\n",
      "\n",
      "🔍 Processing SUBSUBCAT_Sertraline...\n",
      "📊 Exported numeric summary to: outputs\\SUBSUBCAT_Sertraline\\covariate_balance_table_SUBSUBCAT_Sertraline.xlsx\n",
      "✅ Saved love plot: outputs\\SUBSUBCAT_Sertraline\\love_plot_SUBSUBCAT_Sertraline.pdf\n",
      "📏 Max weighted SMD for SUBSUBCAT_Sertraline: 0.401\n",
      "\n",
      "🔍 Processing SUBSUBCAT_Temazepam...\n",
      "📊 Exported numeric summary to: outputs\\SUBSUBCAT_Temazepam\\covariate_balance_table_SUBSUBCAT_Temazepam.xlsx\n",
      "✅ Saved love plot: outputs\\SUBSUBCAT_Temazepam\\love_plot_SUBSUBCAT_Temazepam.pdf\n",
      "📏 Max weighted SMD for SUBSUBCAT_Temazepam: 0.220\n",
      "\n",
      "🔍 Processing SUBSUBCAT_Topiramaat...\n",
      "📊 Exported numeric summary to: outputs\\SUBSUBCAT_Topiramaat\\covariate_balance_table_SUBSUBCAT_Topiramaat.xlsx\n",
      "✅ Saved love plot: outputs\\SUBSUBCAT_Topiramaat\\love_plot_SUBSUBCAT_Topiramaat.pdf\n",
      "📏 Max weighted SMD for SUBSUBCAT_Topiramaat: 0.452\n",
      "\n",
      "🔍 Processing SUBSUBCAT_Tramadol...\n",
      "📊 Exported numeric summary to: outputs\\SUBSUBCAT_Tramadol\\covariate_balance_table_SUBSUBCAT_Tramadol.xlsx\n",
      "✅ Saved love plot: outputs\\SUBSUBCAT_Tramadol\\love_plot_SUBSUBCAT_Tramadol.pdf\n",
      "📏 Max weighted SMD for SUBSUBCAT_Tramadol: 1.050\n",
      "\n",
      "🔍 Processing SUBSUBCAT_Venlafaxine...\n",
      "📊 Exported numeric summary to: outputs\\SUBSUBCAT_Venlafaxine\\covariate_balance_table_SUBSUBCAT_Venlafaxine.xlsx\n",
      "✅ Saved love plot: outputs\\SUBSUBCAT_Venlafaxine\\love_plot_SUBSUBCAT_Venlafaxine.pdf\n",
      "📏 Max weighted SMD for SUBSUBCAT_Venlafaxine: 0.528\n",
      "\n",
      "🔍 Processing SUBSUBCAT_Zolpidem...\n",
      "📊 Exported numeric summary to: outputs\\SUBSUBCAT_Zolpidem\\covariate_balance_table_SUBSUBCAT_Zolpidem.xlsx\n",
      "✅ Saved love plot: outputs\\SUBSUBCAT_Zolpidem\\love_plot_SUBSUBCAT_Zolpidem.pdf\n",
      "📏 Max weighted SMD for SUBSUBCAT_Zolpidem: 0.379\n",
      "\n",
      "🔍 Processing SUBSUBCAT_Zopiclon...\n",
      "📊 Exported numeric summary to: outputs\\SUBSUBCAT_Zopiclon\\covariate_balance_table_SUBSUBCAT_Zopiclon.xlsx\n",
      "✅ Saved love plot: outputs\\SUBSUBCAT_Zopiclon\\love_plot_SUBSUBCAT_Zopiclon.pdf\n",
      "📏 Max weighted SMD for SUBSUBCAT_Zopiclon: 0.558\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "# ----------------------------------------\n",
    "# Functions to calculate balance\n",
    "# ----------------------------------------\n",
    "def calculate_smd(x1, x2, w1=None, w2=None):\n",
    "    def weighted_mean(x, w): return np.average(x, weights=w)\n",
    "    def weighted_var(x, w):\n",
    "        m = np.average(x, weights=w)\n",
    "        return np.average((x - m) ** 2, weights=w)\n",
    "    \n",
    "    m1 = weighted_mean(x1, w1) if w1 is not None else np.mean(x1)\n",
    "    m2 = weighted_mean(x2, w2) if w2 is not None else np.mean(x2)\n",
    "    v1 = weighted_var(x1, w1) if w1 is not None else np.var(x1, ddof=1)\n",
    "    v2 = weighted_var(x2, w2) if w2 is not None else np.var(x2, ddof=1)\n",
    "    \n",
    "    pooled_sd = np.sqrt((v1 + v2) / 2)\n",
    "    return np.abs(m1 - m2) / pooled_sd if pooled_sd > 0 else 0\n",
    "\n",
    "def variance_ratio(x1, x2, w1=None, w2=None):\n",
    "    def weighted_var(x, w):\n",
    "        m = np.average(x, weights=w)\n",
    "        return np.average((x - m) ** 2, weights=w)\n",
    "    \n",
    "    v1 = weighted_var(x1, w1) if w1 is not None else np.var(x1, ddof=1)\n",
    "    v2 = weighted_var(x2, w2) if w2 is not None else np.var(x2, ddof=1)\n",
    "    \n",
    "    return max(v1 / v2, v2 / v1) if v1 > 0 and v2 > 0 else 1\n",
    "\n",
    "# ----------------------------------------\n",
    "# Setup\n",
    "# ----------------------------------------\n",
    "output_base = \"outputs\"\n",
    "groups = [g for g in os.listdir(output_base) if os.path.isdir(os.path.join(output_base, g))]\n",
    "\n",
    "# Create a case-insensitive mapping\n",
    "final_covariates_map_lower = {k.lower(): v for k, v in final_covariates_map.items()}\n",
    "\n",
    "# ----------------------------------------\n",
    "# Main Loop\n",
    "# ----------------------------------------\n",
    "for group in groups:\n",
    "    if group.lower() not in final_covariates_map_lower:\n",
    "        continue\n",
    "\n",
    "    print(f\"\\n🔍 Processing {group}...\")\n",
    "\n",
    "    try:\n",
    "        group_path = os.path.join(output_base, group)\n",
    "        covariates = final_covariates_map_lower[group.lower()]\n",
    "        \n",
    "        column_name = None\n",
    "        for col in pd.read_pickle(os.path.join(group_path, \"trimmed_data_imp1.pkl\")).columns:\n",
    "            if col.lower() == group.lower():\n",
    "                column_name = col\n",
    "                break\n",
    "        if column_name is None:\n",
    "            print(f\"⚠️ Column not found for {group}, skipping.\")\n",
    "            continue\n",
    "\n",
    "        smd_unw_all, smd_w_all = [], []\n",
    "        vr_unw_all, vr_w_all = [], []\n",
    "\n",
    "        for i in range(1, 6):\n",
    "            df_path = os.path.join(group_path, f\"trimmed_data_imp{i}.pkl\")\n",
    "            iptw_path = os.path.join(group_path, \"iptw_weights.xlsx\")\n",
    "\n",
    "            if not os.path.exists(df_path) or not os.path.exists(iptw_path):\n",
    "                print(f\"⚠️ Missing data for {group} imp{i}, skipping.\")\n",
    "                continue\n",
    "\n",
    "            df = pd.read_pickle(df_path)\n",
    "            iptw_df = pd.read_excel(iptw_path, index_col=0)\n",
    "            T = df[column_name]\n",
    "            W = iptw_df.loc[df.index, \"iptw_mean\"]\n",
    "\n",
    "            smd_unw_i, smd_w_i, vr_unw_i, vr_w_i = [], [], [], []\n",
    "\n",
    "            for cov in covariates:\n",
    "                x1, x0 = df.loc[T == 1, cov], df.loc[T == 0, cov]\n",
    "                w1, w0 = W[T == 1], W[T == 0]\n",
    "\n",
    "                su = calculate_smd(x1, x0)\n",
    "                sw = calculate_smd(x1, x0, w1, w0)\n",
    "\n",
    "                vu = variance_ratio(x1, x0) if cov in ['treatmentdurationdays', 'CAPS5score_baseline', 'age'] else np.nan\n",
    "                vw = variance_ratio(x1, x0, w1, w0) if cov in ['treatmentdurationdays', 'CAPS5score_baseline', 'age'] else np.nan\n",
    "\n",
    "                smd_unw_i.append(su)\n",
    "                smd_w_i.append(sw)\n",
    "                vr_unw_i.append(vu)\n",
    "                vr_w_i.append(vw)\n",
    "\n",
    "            smd_unw_all.append(smd_unw_i)\n",
    "            smd_w_all.append(smd_w_i)\n",
    "            vr_unw_all.append(vr_unw_i)\n",
    "            vr_w_all.append(vr_w_i)\n",
    "\n",
    "        smd_unw = np.mean(smd_unw_all, axis=0)\n",
    "        smd_w = np.mean(smd_w_all, axis=0)\n",
    "        vr_unw = np.nanmean(vr_unw_all, axis=0)\n",
    "        vr_w = np.nanmean(vr_w_all, axis=0)\n",
    "\n",
    "        severity = []\n",
    "        for sw in smd_w:\n",
    "            if sw <= 0.1:\n",
    "                severity.append(\"Good\")\n",
    "            elif sw <= 0.2:\n",
    "                severity.append(\"Moderate\")\n",
    "            else:\n",
    "                severity.append(\"Poor\")\n",
    "\n",
    "        covariate_names = covariates\n",
    "        numeric_df = pd.DataFrame({\n",
    "            \"Covariate\": covariate_names,\n",
    "            \"SMD_Unweighted\": smd_unw,\n",
    "            \"SMD_Weighted\": smd_w,\n",
    "            \"Imbalance_Severity\": severity,\n",
    "            \"VR_Unweighted\": vr_unw,\n",
    "            \"VR_Weighted\": vr_w\n",
    "        })\n",
    "\n",
    "        numeric_path = os.path.join(group_path, f\"covariate_balance_table_{group}.xlsx\")\n",
    "        numeric_df.to_excel(numeric_path, index=False)\n",
    "        print(f\"📊 Exported numeric summary to: {numeric_path}\")\n",
    "\n",
    "        # -------------------------\n",
    "        # Plot\n",
    "        # -------------------------\n",
    "        labels = covariates\n",
    "        y_pos = np.arange(len(labels))\n",
    "\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(18, len(labels) * 0.45))\n",
    "\n",
    "        axes[0].scatter(smd_unw, y_pos, color='red', label=\"Unweighted\")\n",
    "        axes[0].scatter(smd_w, y_pos, color='blue', label=\"Weighted\")\n",
    "        axes[0].axvline(0.1, color='gray', linestyle='--', label=\"Threshold 0.1\")\n",
    "        axes[0].axvline(0.2, color='black', linestyle='--', label=\"Threshold 0.2\")\n",
    "        axes[0].set_xlim(0, max(max(smd_unw), max(smd_w), 0.25) + 0.05)\n",
    "        axes[0].set_yticks(y_pos)\n",
    "        axes[0].set_yticklabels(labels)\n",
    "        axes[0].invert_yaxis()\n",
    "        axes[0].set_title(\"Standardized Mean Differences (SMD)\")\n",
    "        axes[0].legend(loc=\"upper right\")\n",
    "        axes[0].grid(True)\n",
    "\n",
    "        vr_mask = [cov in ['treatmentdurationdays', 'CAPS5score_baseline', 'age'] for cov in covariates]\n",
    "        filtered_y = [i for i, b in enumerate(vr_mask) if b]\n",
    "        filtered_labels = [labels[i] for i in filtered_y]\n",
    "        filtered_vr_unw = [vr_unw[i] for i in filtered_y]\n",
    "        filtered_vr_w = [vr_w[i] for i in filtered_y]\n",
    "\n",
    "        axes[1].scatter(filtered_vr_unw, filtered_y, color='blue', marker='o', label=\"Unweighted\")\n",
    "        axes[1].scatter(filtered_vr_w, filtered_y, color='red', marker='x', label=\"Weighted\")\n",
    "        axes[1].axvline(2, color='gray', linestyle='--')\n",
    "        axes[1].axvline(0.5, color='gray', linestyle='--')\n",
    "        axes[1].set_xlim(0, max(filtered_vr_unw + filtered_vr_w + [2.5]) + 0.5)\n",
    "        axes[1].set_yticks(filtered_y)\n",
    "        axes[1].set_yticklabels(filtered_labels)\n",
    "        axes[1].invert_yaxis()\n",
    "        axes[1].set_title(\"Variance Ratio (VR)\")\n",
    "        axes[1].legend()\n",
    "        axes[1].grid(True)\n",
    "\n",
    "        fig.suptitle(f\"Covariate Balance for {group.replace('CAT_', '')}\", fontsize=14, weight='bold')\n",
    "        fig.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "        plot_path = os.path.join(group_path, f\"love_plot_{group}.pdf\")\n",
    "        fig.savefig(plot_path, dpi=300)\n",
    "        plt.close()\n",
    "        print(f\"✅ Saved love plot: {plot_path}\")\n",
    "        print(f\"📏 Max weighted SMD for {group}: {np.max(smd_w):.3f}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error in {group}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "351cdcc2-c480-437b-a023-c81a95efbb47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "1f2a0c4e-78d3-42d2-b3f4-7aa21c575faa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== Creating Heatmap for SubSubCat_Oxazepam ==========\n",
      "✅ Heatmap saved: outputs\\SubSubCat_Oxazepam\\heatmap_smd_SubSubCat_Oxazepam.png\n",
      "\n",
      "========== Creating Heatmap for SubSubCat_Diazepam ==========\n",
      "✅ Heatmap saved: outputs\\SubSubCat_Diazepam\\heatmap_smd_SubSubCat_Diazepam.png\n",
      "\n",
      "========== Creating Heatmap for SubSubCat_Paracetamol ==========\n",
      "✅ Heatmap saved: outputs\\SubSubCat_Paracetamol\\heatmap_smd_SubSubCat_Paracetamol.png\n",
      "\n",
      "========== Creating Heatmap for SubSubCat_Lorazepam ==========\n",
      "✅ Heatmap saved: outputs\\SubSubCat_Lorazepam\\heatmap_smd_SubSubCat_Lorazepam.png\n",
      "\n",
      "========== Creating Heatmap for SubSubCat_Mirtazapine ==========\n",
      "✅ Heatmap saved: outputs\\SubSubCat_Mirtazapine\\heatmap_smd_SubSubCat_Mirtazapine.png\n",
      "\n",
      "========== Creating Heatmap for SubSubCat_Escitalopram ==========\n",
      "✅ Heatmap saved: outputs\\SubSubCat_Escitalopram\\heatmap_smd_SubSubCat_Escitalopram.png\n",
      "\n",
      "========== Creating Heatmap for SubSubCat_Sertraline ==========\n",
      "✅ Heatmap saved: outputs\\SubSubCat_Sertraline\\heatmap_smd_SubSubCat_Sertraline.png\n",
      "\n",
      "========== Creating Heatmap for SubSubCat_Temazepam ==========\n",
      "✅ Heatmap saved: outputs\\SubSubCat_Temazepam\\heatmap_smd_SubSubCat_Temazepam.png\n",
      "\n",
      "========== Creating Heatmap for SubSubCat_Citalopram ==========\n",
      "✅ Heatmap saved: outputs\\SubSubCat_Citalopram\\heatmap_smd_SubSubCat_Citalopram.png\n",
      "\n",
      "========== Creating Heatmap for SubSubCat_Quetiapine ==========\n",
      "✅ Heatmap saved: outputs\\SubSubCat_Quetiapine\\heatmap_smd_SubSubCat_Quetiapine.png\n",
      "\n",
      "========== Creating Heatmap for SubSubCat_Amitriptyline ==========\n",
      "✅ Heatmap saved: outputs\\SubSubCat_Amitriptyline\\heatmap_smd_SubSubCat_Amitriptyline.png\n",
      "\n",
      "========== Creating Heatmap for SubSubCat_Venlafaxine ==========\n",
      "✅ Heatmap saved: outputs\\SubSubCat_Venlafaxine\\heatmap_smd_SubSubCat_Venlafaxine.png\n",
      "\n",
      "========== Creating Heatmap for SubSubCat_Fluoxetine ==========\n",
      "✅ Heatmap saved: outputs\\SubSubCat_Fluoxetine\\heatmap_smd_SubSubCat_Fluoxetine.png\n",
      "\n",
      "========== Creating Heatmap for SubSubCat_Topiramaat ==========\n",
      "✅ Heatmap saved: outputs\\SubSubCat_Topiramaat\\heatmap_smd_SubSubCat_Topiramaat.png\n",
      "\n",
      "========== Creating Heatmap for SubSubCat_Tramadol ==========\n",
      "✅ Heatmap saved: outputs\\SubSubCat_Tramadol\\heatmap_smd_SubSubCat_Tramadol.png\n",
      "\n",
      "========== Creating Heatmap for SubSubCat_Zopiclon ==========\n",
      "✅ Heatmap saved: outputs\\SubSubCat_Zopiclon\\heatmap_smd_SubSubCat_Zopiclon.png\n",
      "\n",
      "========== Creating Heatmap for SubSubCat_Loprazolam ==========\n",
      "✅ Heatmap saved: outputs\\SubSubCat_Loprazolam\\heatmap_smd_SubSubCat_Loprazolam.png\n",
      "\n",
      "========== Creating Heatmap for SubSubCat_Alprazolam ==========\n",
      "✅ Heatmap saved: outputs\\SubSubCat_Alprazolam\\heatmap_smd_SubSubCat_Alprazolam.png\n",
      "\n",
      "========== Creating Heatmap for SubSubCat_promethazine ==========\n",
      "✅ Heatmap saved: outputs\\SubSubCat_promethazine\\heatmap_smd_SubSubCat_promethazine.png\n",
      "\n",
      "========== Creating Heatmap for SubSubCat_Paroxetine ==========\n",
      "✅ Heatmap saved: outputs\\SubSubCat_Paroxetine\\heatmap_smd_SubSubCat_Paroxetine.png\n",
      "\n",
      "========== Creating Heatmap for SubSubCat_Bupropion ==========\n",
      "✅ Heatmap saved: outputs\\SubSubCat_Bupropion\\heatmap_smd_SubSubCat_Bupropion.png\n",
      "\n",
      "========== Creating Heatmap for SubSubCat_Methylfenidaat ==========\n",
      "✅ Heatmap saved: outputs\\SubSubCat_Methylfenidaat\\heatmap_smd_SubSubCat_Methylfenidaat.png\n",
      "\n",
      "========== Creating Heatmap for SubSubCat_Olanzapine ==========\n",
      "✅ Heatmap saved: outputs\\SubSubCat_Olanzapine\\heatmap_smd_SubSubCat_Olanzapine.png\n",
      "\n",
      "========== Creating Heatmap for SubSubCat_Zolpidem ==========\n",
      "✅ Heatmap saved: outputs\\SubSubCat_Zolpidem\\heatmap_smd_SubSubCat_Zolpidem.png\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "#-----------------------------\n",
    "# Generate heatmaps\n",
    "# -------------------------------\n",
    "for treatment_var in medication_groups:\n",
    "    print(f\"\\n========== Creating Heatmap for {treatment_var} ==========\")\n",
    "\n",
    "    try:\n",
    "        output_folder = os.path.join('outputs', treatment_var)\n",
    "        balance_path = os.path.join(output_folder, f'covariate_balance_table_{treatment_var}.xlsx')\n",
    "\n",
    "        if not os.path.exists(balance_path):\n",
    "            print(f\"❌ Balance file not found: {balance_path}\")\n",
    "            continue\n",
    "\n",
    "        balance_df = pd.read_excel(balance_path)\n",
    "\n",
    "        # ✅ Use finalized covariates + 'Propensity Score'\n",
    "        covariates = final_covariates_map[treatment_var] + ['Propensity Score']\n",
    "        balance_df = balance_df[balance_df['Covariate'].isin(covariates)]\n",
    "\n",
    "        # ✅ Check for CAPS5score_baseline\n",
    "        highlight_caps = 'CAPS5score_baseline' in balance_df['Covariate'].values\n",
    "\n",
    "        # ✅ Format for heatmap\n",
    "        heatmap_df = balance_df[['Covariate', 'SMD_Unweighted', 'SMD_Weighted']].copy()\n",
    "        heatmap_df.columns = ['Covariate', 'Unweighted', 'Weighted']\n",
    "        heatmap_df = heatmap_df.set_index('Covariate')\n",
    "        heatmap_df = heatmap_df.sort_values(by='Unweighted', ascending=False)\n",
    "\n",
    "        # ✅ Plot\n",
    "        plt.figure(figsize=(12, max(10, len(heatmap_df) * 0.35)))\n",
    "        ax = sns.heatmap(\n",
    "            heatmap_df,\n",
    "            cmap=\"coolwarm\",\n",
    "            annot=True,\n",
    "            fmt=\".2f\",\n",
    "            linewidths=0.6,\n",
    "            linecolor='gray',\n",
    "            cbar_kws={\"label\": \"Standardized Mean Difference\"}\n",
    "        )\n",
    "\n",
    "        plt.title(f\"Covariate Balance Heatmap (Rubin IPTW)\\n{treatment_var}\", fontsize=15, weight='bold')\n",
    "        plt.xlabel(\"Condition\")\n",
    "        plt.ylabel(\"Covariate\")\n",
    "\n",
    "        # ✅ Bold CAPS5score_baseline if present\n",
    "        if highlight_caps:\n",
    "            ylabels = [label.get_text() for label in ax.get_yticklabels()]\n",
    "            ax.set_yticklabels([\n",
    "                f\"{label} ←\" if label == 'CAPS5score_baseline' else label for label in ylabels\n",
    "            ])\n",
    "\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # ✅ Save image\n",
    "        save_path = os.path.join(output_folder, f'heatmap_smd_{treatment_var}.png')\n",
    "        plt.savefig(save_path, dpi=300)\n",
    "        plt.close()\n",
    "\n",
    "        print(f\"✅ Heatmap saved: {save_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error processing {treatment_var}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91c1872-86ad-4dd6-9015-795fe70a59d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
